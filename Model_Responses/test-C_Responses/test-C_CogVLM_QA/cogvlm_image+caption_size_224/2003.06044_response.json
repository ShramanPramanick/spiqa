{"927": {"question": "What previous methods is the proposed method compared against?", "response": {"6-Table5-1.png": ["The proposed method is compared against previous methods such as CNN, LSTM, BERT, LSTM+BLSTM, LSTM+Attention, LSTM+BLSTM+Attention, LSTM+LC Attention, and LSTM+LC Attention.</s>", "The proposed method is compared against previous methods such as CNN, LSTM, BERT, LSTM+BLSTM, LSTM+Attention, LSTM+BLSTM+Attention, LSTM+LC Attention, and LSTM+LC Attention.</s>"], "6-Table4-1.png": ["The proposed method is compared against various previous approaches such as BLSTM+Attention+BLSTM (2019), Hierarchical BLSTM-CRF (2018), CRF-ASN (2018), Hierarchical CNN (window 4, 2017), mLSTM-RNN (2018), DRLM-Conditional (2016), LSTM-Softmax (2016), RCN (2013), CNN (2016), CRF (2010), and reimplemented and proposed approaches like CNN, LSTM, BERT, LSTM+Attention, LSTM+Local Contextual Attention, and Human annotator.</s>", "The proposed method is compared against various previous methods such as BLSTM+Attention+BLSTM (2019), Hierarchical BLSTM-CRF (2018), CRF-ASN (2018), Hierarchical CNN (window 4, 2017), mLSTM-RNN (2018), DRLM-Conditional (2016), LSTM-Softmax (2016), RCNN (2013), CNN (2016), CRF (2010), and Human annotator.</s>"]}, "referred_figures_names": ["6-Table5-1.png", "6-Table4-1.png"], "answer": "BLSTM+Attention+BLSTM\nHierarchical BLSTM-CRF\nCRF-ASN\nHierarchical CNN (window 4)\nmLSTM-RNN\nDRLM-Conditional\nLSTM-Softmax\nRCNN\nCNN\nCRF\nLSTM\nBERT"}}