{"1340": {"question": "Which are the sequence model architectures this method can be transferred across?", "response": {"9-Table8-1.png": ["The method can be transferred across the following sequence model architectures: LSTM, CNN, and Transformer.</s>", "The method can be transferred across the following sequence model architectures: LSTM, CNN, and Transformer.</s>"]}, "referred_figures_names": ["9-Table8-1.png"], "answer": "The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models"}, "1341": {"question": " What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?", "response": {"8-Table3-1.png": ["The proposed method shows an improvement of approximately 10% over the newest state-of-the-art methods, as it achieves an average of 106.55 sentences per second, which is higher than the 96.76 of the proposed (CNN) and 104.24 of the proposed (Transformer).</s>", "To calculate the percentage improvement, we subtract the speed of the newest state-of-the-art methods from the proposed method's speed, divide by the speed of the newest state-of-the-art methods, and then multiply by 100. So, ((106.55 - 80.75) / 80.75) * 100 = 25.31%. Therefore, the proposed method shows a 25.31% improvement in inference speed over the newest state-of-the-art methods.</s>"]}, "referred_figures_names": ["8-Table3-1.png"], "answer": "Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)"}}