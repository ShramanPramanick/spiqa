{"204": {"question": "Which baselines did they compare against?", "response": {"6-Table1-1.png": ["They compared against the following baselines: RNN-Socher (2015), Ad-MC-RNN (Dog et al., 2014), TE-RNN (Qin et al., 2015), TREC-LSTM (Fix et al., 2015), Tree-LSTM (Fix, Socher, and Manning, 2015), DC-Tree-LSTM (Qin and Huang, 2017), TF-LSTM (Huang, Ding, and Zhang, 2017), BG-CNN (Teng and Zhang, 2017), Gumbel Tree-LSTM (Xion, and Lee, 208), SATA Tree-LSTM (Ours), and other neural models like CNN (Kim et al., 2014), Ad-Sent (Zhao et al., 2015), LSTM-CNN (Zhou et al., 2016), bpt.ml-STM (Bford, Rosenberg, and Seldner, 2017), BCNN+Char (Covey and McCane, 2017), and BCN+ELMo (Peters et al., 2018).</s>", "They compared their models against several baselines including RNN-Socher (2015), Ad-MC-RNN (Dog et al., 2014), TE-RNN (Qin et al., 2015), TREC-LSTM (Fix et al., 2015), Tree-LSTM (Fix, Socher, and Manning, 2015), DC-Tree-LSTM (Qin and Huang, 2017), TF-LSTM (Huang, Ding, and Zhang, 2017), BG-CNN (Teng and Zhang, 2017), Gumbel Tree-LSTM (Xion, and Lee, 2018), and SATA Tree-LSTM (Ours).</s>"], "6-Table2-1.png": ["They compared against the following baselines: 'Latent tree models', 'Tree-based CNN', 'SPINN-PI', 'Gumbel tree-LSTM', and '3000 SATA Tree-LSTM (Ours)'.</s>", "They compared against the following baselines: 'Latent tree models', 'Tree-based CNN', 'SPINN-PI', 'Gumbel tree-LSTM', and '3000 SATA Tree-LSTM (Ours)'.</s>"]}, "referred_figures_names": ["6-Table1-1.png", "6-Table2-1.png"], "answer": "Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks"}, "205": {"question": "Which baselines did they compare against?", "response": {"6-Table1-1.png": ["They compared against the following baselines: RNN-Socher (2015), Ad-MC-RNN (Dog et al., 2014), TE-RNN (Qin et al., 2015), TREC-LSTM (Fix et al., 2015), Tree-LSTM (Fix, Socher, and Manning, 2015), DC-Tree-LSTM (Qin and Huang, 2017), TF-LSTM (Huang, Ding, and Zhang, 2017), BG-CNN (Teng and Zhang, 2017), Gumbel Tree-LSTM (Xion, and Lee, 208), SATA Tree-LSTM (Ours), and other neural models like CNN (Kim et al., 2014), Ad-Sent (Zhao et al., 2015), LSTM-CNN (Zhou et al., 2016), bpt.ml-STM (Bford, Rosenberg, and Seldner, 2017), BCNN+Char (Covey and McCane, 2017), and BCN+ELMo (Peters et al., 2018).</s>", "They compared their models against several baselines including RNN-Socher (2015), Ad-MC-RNN (Dog et al., 2014), TE-RNN (Qin et al., 2015), TREC-LSTM (Fix et al., 2015), Tree-LSTM (Fix, Socher, and Manning, 2015), DC-Tree-LSTM (Qin and Huang, 2017), TF-LSTM (Huang, Ding, and Zhang, 2017), BG-CNN (Teng and Zhang, 2017), Gumbel Tree-LSTM (Xion, and Lee, 2018), and SATA Tree-LSTM (Ours).</s>"], "6-Table2-1.png": ["They compared against the following baselines: 'Latent tree models', 'Tree-based CNN', 'SPINN-PI', 'Gumbel tree-LSTM', and '3000 SATA Tree-LSTM (Ours)'.</s>", "They compared against the following baselines: 'Latent tree models', 'Tree-based CNN', 'SPINN-PI', 'Gumbel tree-LSTM', and '3000 SATA Tree-LSTM (Ours)'.</s>"]}, "referred_figures_names": ["6-Table1-1.png", "6-Table2-1.png"], "answer": "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."}}