{"483": {"question": "Do they conduct any human evaluation?", "response": {"2-Figure1-1.png": ["No.</s>", "Yes, they conduct human evaluation as mentioned in the caption.</s>"]}, "referred_figures_names": ["2-Figure1-1.png"], "answer": "Yes"}, "485": {"question": "How do they enrich the positional embedding with length information", "response": {"4-Figure3-1.png": ["They enrich the positional embedding with length information by computing a relative length embedding for the decoder input. This is done by dividing the input sequence length by the desired output sequence length and then adding the result to the positional embeddings.</s>", "They enrich the positional embedding with length information by computing a relative length embedding for each position in the sequence. This is done by dividing the absolute length of the sequence by the desired target string length and then multiplying by 100 to get a percentage. This percentage is then added to the positional embedding.</s>"]}, "referred_figures_names": ["4-Figure3-1.png"], "answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative)."}, "486": {"question": "How do they condition the output to a given target-source class?", "response": {"2-Figure2-1.png": ["They condition the output to a given target-source class using a softmax function.</s>", "They condition the output to a given target-source class by using a softmax function.</s>"]}, "referred_figures_names": ["2-Figure2-1.png"], "answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group."}}