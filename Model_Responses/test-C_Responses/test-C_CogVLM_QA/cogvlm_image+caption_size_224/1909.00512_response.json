{"31": {"question": "What experiments are proposed to test that upper layers produce context-specific embeddings?", "response": {"5-Figure1-1.png": ["The experiments proposed to test that upper layers produce context-specific embeddings are: 1) Testing on a large corpus of English text, 2) Testing on a corpus of text that includes a variety of languages, and 3) Testing on a corpus of text that includes a variety of topics.</s>", "The experiments proposed to test that upper layers produce context-specific embeddings involve training BERT, ELMo, and GPT-2 on the same dataset but with different initial layers. Specifically, they will train BERT, ELMo, and GPT-2 with the same initial layers but with the exception of the input layer. This will help in understanding if the embeddings produced by these models in higher layers are context-specific or not.</s>"], "6-Figure2-1.png": ["The experiments proposed to test that upper layers produce context-specific embeddings are: 1) Uniformly randomly sampled words, 2) Words from the same context, and 3) Words from different contexts.</s>", "The experiments proposed to test that upper layers produce context-specific embeddings are: 1) Uniformly randomly sampled words, 2) Words with a small set of contexts, and 3) Words with a large set of contexts.</s>"], "7-Figure3-1.png": ["The experiments proposed to test that upper layers produce context-specific embeddings are: 1) Uniformly randomly sampled sentences, 2) Sentences with a fixed positional encoding, and 3) Sentences with a fixed positional encoding and a fixed masking rate.</s>", "The experiments proposed to test that upper layers produce context-specific embeddings are: 1) Uniformly randomly sampled sentences, 2) Sentences with a fixed positional encoding, and 3) Sentences with a fixed masked positional encoding.</s>"]}, "referred_figures_names": ["5-Figure1-1.png", "6-Figure2-1.png", "7-Figure3-1.png"], "answer": "They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  \nThey plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.\nThey plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another."}, "32": {"question": "How do they calculate a static embedding for each word?", "response": {"8-Table1-1.png": ["They calculate a static embedding for each word using the first principal component of a word\u2019s contextualized representations in a given layer.</s>", "They calculate a static embedding for each word using the first principal component of a word\u2019s contextualized representations in a given layer.</s>"]}, "referred_figures_names": ["8-Table1-1.png"], "answer": "They use the first principal component of a word's contextualized representation in a given layer as its static embedding."}}