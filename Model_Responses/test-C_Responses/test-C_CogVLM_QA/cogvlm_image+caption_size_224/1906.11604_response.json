{"1392": {"question": "How are sentence embeddings incorporated into the speech recognition system?", "response": {"4-Figure1-1.png": ["Sentence embeddings are incorporated into the speech recognition system by first being passed through a BERT (Bidirectional Encoder Representations from Transformers) model to generate context embeddings. These context embeddings are then combined with the previous word embeddings to produce the final embeddings for the entire sequence of words in the sentence.</s>", "Sentence embeddings are incorporated into the speech recognition system by first being passed through a BERT (Bidirectional Encoder Representations from Transformers) model to generate context embeddings. These context embeddings are then combined with the previous word embeddings to produce a final representation for the current word in the sequence.</s>"], "5-Figure2-1.png": ["Sentence embeddings are incorporated into the speech recognition system by first being passed through a gated embedding mechanism. This mechanism takes in the contextual embeddings, previous word embeddings, and current speech embeddings. The output of this mechanism is then passed to the decoder.</s>", "Sentence embeddings are incorporated into the speech recognition system by first being passed through a gated embedding mechanism. This mechanism takes in the contextual embeddings, previous word embeddings, and current speech embeddings. The output of this mechanism is then passed to the decoder, which further processes the information to generate the final output.</s>"]}, "referred_figures_names": ["4-Figure1-1.png", "5-Figure2-1.png"], "answer": "BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer."}}