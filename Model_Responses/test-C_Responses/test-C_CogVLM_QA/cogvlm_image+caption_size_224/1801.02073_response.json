{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "response": {"3-Table2-1.png": ["Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset.</s>", "Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset.</s>"]}, "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "response": {"3-Figure1-1.png": ["They found 6 question types in the datasets analyzed.</s>", "They find six question types in the datasets analyzed: What, How, Who, When, Where, and Why.</s>"]}, "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "response": {"2-Table1-1.png": ["They analyze contextual similarities across datasets by calculating the macro average in percentage of overlapping words between question-answer pairs normalized by the lengths of the questions and answers.</s>", "They analyze contextual similarities across datasets by calculating the macro average in percentage of overlapping words between question-answer pairs, normalized by the lengths of the questions and answers. This is represented by the \u2126q/a column in the table.</s>"]}, "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}