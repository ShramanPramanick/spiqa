{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "response": {"4-Figure2-1.png": ["The HMM models use features such as 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', 'period', 'question mark', 'exclamation mark', 'capital letter', 'lower case letter', 'digit', 'space', 'comma', '</s>", "The HMM models use features like 'space', 'comma', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', 'lowercase', 'number', 'question mark', 'exclamation mark', 'comma', 'space', 'period', 'capital', '</s>"], "4-Figure3-1.png": ["The HMM models use features like 'N', 'N+1', 'N+2', 'N+3', 'N+4', 'N+5', 'N+6', 'N+7', 'N+8', 'N+9', 'N+10', 'N+11', 'N+12', 'N+13', 'N+14', 'N+15', 'N+16', 'N+17', 'N+18', 'N+19', 'N+20', 'N+21', 'N+22', 'N+23', 'N+24', 'N+25', 'N+26', 'N+27', 'N+28', 'N+29', 'N+30', 'N+31', 'N+32', 'N+33', 'N+34', 'N+35', 'N+36', 'N+37', 'N+38', 'N+39', 'N+40', 'N+41', 'N+42', 'N+43', 'N+44', 'N+45', 'N+46', 'N+47', 'N+48', 'N+49', 'N+50', 'N+51', 'N+52', 'N+53', 'N+54', 'N+55', 'N+56', 'N+57', 'N+58', 'N+59', 'N+60', 'N+61', 'N+62', 'N+63', 'N+64', 'N+65', 'N+66', 'N+67', 'N+68', 'N+69', 'N+', 'N++', 'N+++', 'N++', 'N+++', 'N++++', 'N++++', 'N+++', 'N+++', 'N+++++', 'N+++++', 'N++++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N++++++', 'N+++++', 'N++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N+++++', 'N++++', 'N+++++', 'N+++++', 'N+++++', 'N++++++', 'N+++++', 'N+++++', 'N+++++", "The HMM models use features like 'N', 'N+1', 'N+2', 'N+3', 'N+4', 'N+5', 'N+6', 'N+7', 'N+8', 'N+9', 'N+10', 'N+11', 'N+12', 'N+13', 'N+14', 'N+15', 'N+16', 'N+17', 'N+18', 'N+19', 'N+20', 'N+21', 'N+22', 'N+23', 'N+24', 'N+25', 'N+26', 'N+27', 'N+28', 'N+29', 'N+30', 'N+31', 'N+32', 'N+333', 'N+34', 'N+35', 'N+36', 'N+37', 'N+38', 'N+39', 'N+40', 'N+41', 'N+42', 'N+43', 'N+44', 'N+45', 'N+46', 'N+47', 'N+48', 'N+49', 'N+50', 'N+51', 'N+52', 'N+53', 'N+54', 'N+55', 'N+56', 'N+57', 'N+58', 'N+59', 'N+60', 'N+61', 'N+62', 'N+63', 'N+64', 'N+65', 'N+66', 'N+67', 'N+68', 'N+69', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+7', 'N', 'N+77', '</s>"]}, "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "response": {"4-Figure2-1.png": ["The HMMs learn some complementary features in the text related to spaces and comments, which the LSTMs don't.</s>", "The HMMs learn some complementary features in the text related to spaces and comments, which the LSTMs don't.</s>"], "4-Figure3-1.png": ["The HMMs learn the context of the sequence, which is the probability of the sequence given the preceding characters. This context information is not captured by the LSTMs.</s>", "The HMMs learn the context-dependent probabilities of the preceding characters, which the LSTMs don't. This is evident from the decision tree where the HMMs have specific nodes for certain character sequences, indicating contextual information, while the LSTMs have more generic nodes like 'N_a' or 'N_b'.</s>"]}, "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "response": {"3-Table1-1.png": ["The gap in performance between the HMMs and the LSTMs appears to be quite significant, with LSTMs generally having higher predictive loglikelihood values across all validation sets.</s>", "The gap in performance between the HMMs and the LSTMs appears to be quite significant. For instance, in the 'Single-patient Physician data', the LSTMs consistently have higher loglikelihood values than the HMMs across all states.</s>"]}, "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}