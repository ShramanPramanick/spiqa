{"585": {"question": "What evaluations did the authors use on their system?", "response": {"8-Table1-1.png": ["The authors used BLEU scores to evaluate their system. BLEU scores are a metric commonly used to measure the quality of machine-translated text.</s>", "The authors used the BLEU (Bilingual Evaluation Understudy) score to evaluate their system. The BLEU score is a case-insensitive metric used to measure the quality of machine-translated text by comparing it with reference translations.</s>"], "10-Table6-1.png": ["The authors used the following evaluations on their system: BLEU, METEOR, and ROUGE-L.</s>", "The authors used three evaluations on their system: BLEU, METEOR, and ROUGE-L.</s>"], "9-Table3-1.png": ["The authors used evaluations such as BLEU, NIST, and METEOR to evaluate their system.</s>", "The authors used evaluations such as RNNSearch, +Ct, and Reference to evaluate their system.</s>"]}, "referred_figures_names": ["8-Table1-1.png", "10-Table6-1.png", "9-Table3-1.png"], "answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."}}