{"31": {"question": "What experiments are proposed to test that upper layers produce context-specific embeddings?", "response": {"6-Figure2-1.png": ["Yes", "The experiments proposed to test that upper layers produce context-specific embeddings are measuring the average cosine similarity between representations of the same word in different contexts, which is called the word's self-similarity. The results show that as the layer increases, the self-similarity decreases, suggesting that contextualized word representations are more context-specific in higher layers."], "7-Figure3-1.png": ["No", "The experiments proposed to test that upper layers produce context-specific embeddings involve calculating the average cosine similarity between each word representation in a sentence and their mean. This statistic reflects how context-specificity manifests in the representation space, and as seen above, it manifests very differently for ELMo, BERT, and GPT-2."], "5-Figure1-1.png": ["No", "The experiments proposed to test that upper layers produce context-specific embeddings are measuring the cosine similarity between uniformly randomly sampled words in almost all layers of BERT, ELMo, and GPT-2. The results show that representations in higher layers are generally more anisotropic than those in lower ones."]}, "referred_figures_names": ["6-Figure2-1.png", "7-Figure3-1.png", "5-Figure1-1.png"], "answer": "They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  \nThey plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.\nThey plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another."}, "32": {"question": "How do they calculate a static embedding for each word?", "response": {"8-Table1-1.png": ["Yes", "The static embeddings are calculated using the first principal component of a word's contextualized representations in a given layer."]}, "referred_figures_names": ["8-Table1-1.png"], "answer": "They use the first principal component of a word's contextualized representation in a given layer as its static embedding."}}