{"247": {"question": "How do the authors measure performance?", "response": {"7-Table2-1.png": ["Yes", "The authors measure performance by comparing the accuracies of different methods for various benchmarks on two classifier architectures."]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "Accuracy across six datasets"}, "248": {"question": "Does the new objective perform better than the original objective bert is trained on?", "response": {"7-Table2-1.png": ["Yes", "Yes, the new objective (CBERT) performs better than the original BERT on two classifier architectures over six datasets."]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}, "249": {"question": "Are other pretrained language models also evaluated for contextual augmentation? ", "response": {"7-Table2-1.png": ["No", "Yes, other pretrained language models like RoBERTa and BERT are also evaluated for contextual augmentation in the table."]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "No"}, "250": {"question": "Do the authors report performance of conditional bert on tasks without data augmentation?", "response": {"7-Table2-1.png": ["No", "Yes, the authors report performance of conditional BERT on tasks without data augmentation."]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}}