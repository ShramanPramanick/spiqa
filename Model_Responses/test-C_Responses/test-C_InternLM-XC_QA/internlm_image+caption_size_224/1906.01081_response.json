{"220": {"question": "How many people participated in their evaluation study of table-to-text models?", "response": {"5-Table1-1.png": ["No", "The image does not provide information about the number of people who participated in their evaluation study of table-to-text models."]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "about 500"}, "222": {"question": "By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?", "response": {"7-Figure3-1.png": ["No", "The figure does not provide information about the correlation of PARENT with human judgements in comparison to other text generation metrics."], "8-Table4-1.png": ["No", "The PARENT text generation metrics correlate more with human judgements than other metrics, as indicated by the higher average Pearson correlation across 500 bootstrap samples."], "6-Table2-1.png": ["Yes", "PARENT correlates more with human judgements than other text generation metrics by a significant amount, as indicated by the lower correlation values in the table."]}, "referred_figures_names": ["7-Figure3-1.png", "8-Table4-1.png", "6-Table2-1.png"], "answer": "Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge."}, "223": {"question": "By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?", "response": {"6-Table2-1.png": ["Yes", "PARENT correlates more with human judgements than other text generation metrics by a significant amount, as indicated by the lower correlation values in the table."]}, "referred_figures_names": ["6-Table2-1.png"], "answer": "Their average correlation tops the best other model by 0.155 on WikiBio."}}