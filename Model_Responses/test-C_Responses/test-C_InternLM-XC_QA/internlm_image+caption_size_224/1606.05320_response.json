{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "response": {"4-Figure3-1.png": ["No", "The HMM models use features such as the preceding characters on the Linux data. These features are not very interpretable, as they rely on patterns and sequences in the data rather than explicit rules or explanations."], "4-Figure2-1.png": ["Yes", "The HMM models use a combination of HMM states and LSTM state dimensions to learn features from the text. The image shows that these components are able to capture complementary information related to spaces and comments in the data. However, the specific details of the learned features are not explicitly provided in the image or caption, so it is difficult to determine how interpretable those features are."]}, "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "response": {"4-Figure3-1.png": ["No", "The HMMs learn the probabilities of the hidden states given the observed data, while LSTMs also learn the temporal dependencies between the hidden states."], "4-Figure2-1.png": ["No", "The HMMs learn information related to spaces and comments, while the LSTMs don't."]}, "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "response": {"3-Table1-1.png": ["No", "The image does not provide enough information to determine the gap in performance between HMMs and LSTMs."]}, "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}