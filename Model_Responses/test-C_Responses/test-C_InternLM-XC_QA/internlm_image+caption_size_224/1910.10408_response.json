{"483": {"question": "Do they conduct any human evaluation?", "response": {"2-Figure1-1.png": ["No", "It is not clear from the image whether they conducted a human evaluation or not."]}, "referred_figures_names": ["2-Figure1-1.png"], "answer": "Yes"}, "485": {"question": "How do they enrich the positional embedding with length information", "response": {"4-Figure3-1.png": ["Yes", "In the image, they enrich the positional embedding with length information by computing a relative length embedding according to the desired target string length. This is done using the Transformer architecture with a decoder input that includes the (relative) length embedding."]}, "referred_figures_names": ["4-Figure3-1.png"], "answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative)."}, "486": {"question": "How do they condition the output to a given target-source class?", "response": {"2-Figure2-1.png": ["Yes", "The image does not provide enough information to answer the question."]}, "referred_figures_names": ["2-Figure2-1.png"], "answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group."}}