{"801": {"question": "how small of a dataset did they train on?", "response": {"4-Table1-1.png": ["No", "The training set is the smallest dataset, with a size of 16150."]}, "referred_figures_names": ["4-Table1-1.png"], "answer": "23085 hours of data"}, "802": {"question": "what was their character error rate?", "response": {"6-Table4-1.png": ["Yes", "The Shenma distilled model had a CER of 7.87, the Amap model further trained with Amap dataset had a CER of 6.81, and the Shenma model trained with sMBR on Amap dataset had a CER of 6.26."], "6-Table3-1.png": ["Yes", "The character error rate (CER) for the 9-layers LSTM model is 2.49, and for the 2-layers regular-trained and distilled LSTM models, it is 3.06 and 2.63 respectively."], "6-Table2-1.png": ["Yes", "The character error rate (CER) for the 6-layer model is 3.72%, for the 7-layer model it's 3.68%, for the 8-layer model it's 3.60%, and for the 9-layer model it's 3.82%."]}, "referred_figures_names": ["6-Table4-1.png", "6-Table3-1.png", "6-Table2-1.png"], "answer": "2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning."}, "803": {"question": "what was their character error rate?", "response": {"6-Table3-1.png": ["Yes", "The character error rate (CER) for the 9-layers LSTM model is 2.49, and for the 2-layers regular-trained and distilled LSTM models, it is 3.06 and 2.63 respectively."], "6-Table2-1.png": ["Yes", "The character error rate (CER) for the 6-layer model is 3.72%, for the 7-layer model it's 3.68%, for the 8-layer model it's 3.60%, and for the 9-layer model it's 3.82%."]}, "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Their best model achieved a 2.49% Character Error Rate."}, "804": {"question": "which lstm models did they compare with?", "response": {"6-Table3-1.png": ["Yes", "The LSTM models compared in the table are 9-layers LSTM, 2-layers regular-trained LSTM, and 2-layers distilled LSTM."], "6-Table2-1.png": ["No", "The table compares LSTM models with different layers, trained using regular Xavier Initialization, layer-wise training with CE criterion, and CE + sMBR criteria. The teacher of the 9-layer model is an 8-layer sMBR model, while the others' teacher is a CE model."]}, "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."}}