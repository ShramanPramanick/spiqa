{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "response": {"3-Table2-1.png": ["No", "Yes, they use their indexing-based method to create a sample of a QA Wikipedia dataset."]}, "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "response": {"3-Figure1-1.png": ["Yes", "There are seven question types in the datasets analyzed."]}, "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "response": {"2-Table1-1.png": ["Yes", "The table compares the four corpora for answer selection, including WIKIQA, SELQA, SQUAD, and TriviaQA. They analyze contextual similarities across datasets by calculating various metrics such as the number of questions, answer candidates, tokens, token types, average question length, average answer candidate length, macro average overlapping words between question-answer pairs, and F1 score. These metrics help in understanding the differences and similarities among the datasets, which can be useful for developing effective models for answer selection tasks."]}, "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}