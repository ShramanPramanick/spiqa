{"585": {"question": "What evaluations did the authors use on their system?", "response": {"10-Table6-1.png": ["Yes", "The authors used cosine similarity as an evaluation metric to measure the coherence of their system's responses."], "8-Table1-1.png": ["Yes", "The authors used the NIST Chinese-English translation tasks to evaluate their system. They reported the BLEU scores for different models, including Moses, Groundhog, RNK3search, +Cd, and +Cd,Ct. The average BLEU score (Avg) was also calculated across all test sets."], "9-Table3-1.png": ["Yes", "The authors used the following evaluations on their system: RN3-search, +C, and Reference."]}, "referred_figures_names": ["10-Table6-1.png", "8-Table1-1.png", "9-Table3-1.png"], "answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."}}