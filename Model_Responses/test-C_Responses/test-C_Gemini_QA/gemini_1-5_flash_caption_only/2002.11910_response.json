{"180": {"question": "What are previous state of the art results?", "referred_figures_indices": [1], "response": "```json\n{'Answer': 'Previous state-of-the-art results are shown in Table 1, which lists the performance of two prior models. The current study outperforms these prior models in terms of precision, recall, and F1 scores for both named entity and nominal mention.'}\n```", "all_figures_names": ["2-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "Overall F1 score:\n- He and Sun (2017) 58.23\n- Peng and Dredze (2017) 58.99\n- Xu et al. (2018) 59.11"}, "181": {"question": "What are previous state of the art results?", "referred_figures_indices": [1], "response": "```json\n{'Answer': 'Two previous models, along with the results of this study are shown in Table 1. The results include precision, recall, and F1 scores for both named entity and nominal mention. The study outperforms both previous models for both tasks and overall performance.'}\n```", "all_figures_names": ["2-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%"}}