{"654": {"question": "How they extract \"structured answer-relevant relation\"?", "referred_figures_indices": [3], "response": "```json\n{'Answer': 'Table 2 describes the comparison between sentences and answer-relevant relations, and explains that overlapped words are the non-stop tokens that appear in both the source (sentence/relation) and the target question. The copy ratio represents the proportion of source tokens used in the question.'}\n```", "all_figures_names": ["7-Table5-1.png", "4-Figure3-1.png", "3-Table2-1.png", "1-Figure1-1.png", "3-Figure2-1.png", "5-Table3-1.png", "2-Table1-1.png", "6-Table4-1.png"], "referred_figures_names": ["1-Figure1-1.png"], "answer": "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."}, "656": {"question": "How big are significant improvements?", "referred_figures_indices": [1], "response": "{'Answer': 'The captions provided do not give information on how big significant improvements are. '} \n", "all_figures_names": ["3-Table2-1.png", "6-Table4-1.png", "4-Figure3-1.png", "8-Figure5-1.png", "7-Figure4-1.png", "3-Figure2-1.png", "5-Table3-1.png", "1-Figure1-1.png"], "referred_figures_names": ["6-Table4-1.png"], "answer": "Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"}}