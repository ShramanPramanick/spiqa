{"951": {"question": "what are the evaluation metrics?", "referred_figures_indices": [4], "response": "```json\n{'Answer': 'The evaluation metrics are based on text genre, multi-corpus, official results, 10-fold cross validation, and English results. The features of the best previous in-domain results include local, linguistic, global, gazetteers, word representation features, rules, ensemble, public, and external resources.'}\n```", "all_figures_names": ["21-Table17-1.png", "22-Table19-1.png", "16-Table7-1.png", "17-Table11-1.png", "15-Table5-1.png", "18-Table13-1.png", "6-Table2-1.png", "18-Table12-1.png"], "referred_figures_names": ["15-Table5-1.png"], "answer": "Precision, Recall, F1"}, "952": {"question": "which datasets were used in evaluation?", "referred_figures_indices": [0], "response": "```json\n{'Answer': 'The datasets used for evaluation include: MUC7, SONAR-1, Ancora 2.0, Basque Egunkaria, CoNLL 2003 English, and MEANTIME English multi-corpus.'}\n```", "all_figures_names": ["5-Table1-1.png", "9-Table3-1.png", "22-Table19-1.png", "18-Table12-1.png", "18-Table13-1.png", "17-Table11-1.png", "15-Table5-1.png", "11-Figure1-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0"}}