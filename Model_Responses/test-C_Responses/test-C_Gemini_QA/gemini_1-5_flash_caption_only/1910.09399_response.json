{"159": {"question": "What is typical GAN architecture for each text-to-image synhesis group?", "referred_figures_indices": [2], "response": "```json\n{'Answer': 'The typical GAN architectures for text-to-image synthesis include: \\n\\n**(a) Conditional GAN (cGAN)**:  This architecture uses labels (in this case, text descriptions) to condition both the generator and discriminator. The discriminator's final output is similar to a generic GAN, indicating whether a sample is real or fake. \\n\\n**(b) Manifold Interpolation Matching-aware Discriminator GAN (GAN-INT-CLS)**: Text input is fed to both the generator and discriminator as preprocessed embeddings. These embeddings are concatenated with other inputs before being passed to both networks. The discriminator's final output resembles a generic GAN. \\n\\n**(c) Auxiliary Classifier GAN (AC-GAN)**: An auxiliary classifier layer is added to predict the image's class, ensuring output images come from different classes and promoting diversity in synthesis. \\n\\n**(d) Text Conditioned Auxiliary Classifier GAN (TACGAN)**:  Similar to GAN-INT-CLS, but the output includes both a discriminator and a classifier, which can be used for classification. \\n\\n**(e) Text Conditioned Semantic Classifier GAN (Text-SeGAN)**: This architecture uses a regression layer to estimate the semantic relevance between the image and the text input, ensuring that generated images match the text semantically and are not limited to specific classes.'}\n```", "all_figures_names": ["8-Figure6-1.png", "3-Figure2-1.png", "12-Figure9-1.png", "9-Figure7-1.png", "21-Figure10-1.png", "7-Figure5-1.png", "20-Table2-1.png", "18-Table1-1.png"], "referred_figures_names": ["12-Figure9-1.png"], "answer": "Semantic Enhancement GANs: DC-GANs, MC-GAN\nResolution Enhancement GANs: StackGANs, AttnGAN, HDGAN\nDiversity Enhancement GANs: AC-GAN, TAC-GAN etc.\nMotion Enhancement GAGs: T2S, T2V, StoryGAN"}}