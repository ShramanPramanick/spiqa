{"457": {"question": "What do they mean by answer styles?", "referred_figures_indices": [2], "response": "```json\n{'Answer': 'Answer styles refer to different ways of generating answers, such as Natural Language Generation (NLG) and Question & Answer (Q&A). These styles affect how the model combines words from the vocabulary, question, and multiple passages to form an answer.'}\n```", "all_figures_names": ["4-Figure3-1.png", "6-Table3-1.png", "5-Table1-1.png", "6-Table2-1.png", "1-Figure1-1.png", "8-Table5-1.png", "7-Figure5-1.png", "7-Figure4-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "referred_figures_indices": [6], "response": "```json\n{'Answer': 'No, the captions do not explicitly state whether there is exactly one \"answer style\" per dataset. They focus on different aspects, including model architecture, answer style distributions, ablation tests, and comparison to competing models. Therefore, the answer to this question cannot be determined from the provided captions.'}\n```", "all_figures_names": ["2-Figure2-1.png", "7-Figure5-1.png", "6-Table3-1.png", "6-Table4-1.png", "4-Figure3-1.png", "8-Table5-1.png", "5-Table1-1.png", "1-Figure1-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "referred_figures_indices": [2, 1], "response": "```json\n{'Answer': 'Masque is compared against models from Seo et al. (2017), Tay et al. (2018), Bauer et al. (2018), Indurthi et al. (2018), Hu et al. (2018), Yan et al. (2019), Shao (unpublished), Li (unpublished), Qian (unpublished), and Wu et al. (2018).'}\n```", "all_figures_names": ["2-Figure2-1.png", "8-Table5-1.png", "6-Table2-1.png", "6-Table4-1.png", "7-Figure4-1.png", "1-Figure1-1.png", "4-Figure3-1.png", "6-Table3-1.png"], "referred_figures_names": ["6-Table2-1.png", "8-Table5-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "referred_figures_indices": [3], "response": "```json\n{'Answer': 'Table 5 provides the performance of various models on the NarrativeQA test set.'}\n```", "all_figures_names": ["4-Figure3-1.png", "1-Figure1-1.png", "6-Table3-1.png", "8-Table5-1.png", "7-Figure4-1.png", "7-Figure5-1.png", "2-Figure2-1.png", "6-Table2-1.png"], "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "referred_figures_indices": [4], "response": "```json\n{'Answer': 'An \"answer style\" refers to a specific way in which an answer is generated or presented, such as Natural Language Generation (NLG) or Question & Answer (Q&A). This can influence the structure and content of the answer, as illustrated by Figure 1, which shows how the model uses different distributions to generate words based on the chosen answer style.'}\n```", "all_figures_names": ["6-Table3-1.png", "6-Table2-1.png", "6-Table4-1.png", "7-Figure5-1.png", "5-Table1-1.png", "7-Figure4-1.png", "1-Figure1-1.png", "8-Table5-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}