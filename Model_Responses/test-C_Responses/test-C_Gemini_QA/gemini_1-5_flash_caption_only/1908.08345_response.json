{"836": {"question": "What rouge score do they achieve?", "referred_figures_indices": [2, 7, 6, 1], "response": "```json\n{'Answer': 'The rouge score is not mentioned in the captions. The captions discuss the architecture of BERTSUM, the comparison of summarization datasets, model perplexity, QA-based evaluation, and the proportion of novel n-grams in model generated summaries. They also show ROUGE Recall results on NYT test set, ROUGE F1 results on XSum test set and CNN/DailyMail test set.'}\n```", "all_figures_names": ["3-Figure1-1.png", "7-Table3-1.png", "5-Table1-1.png", "8-Table5-1.png", "9-Table6-1.png", "9-Figure3-1.png", "7-Table4-1.png", "6-Table2-1.png"], "referred_figures_names": ["5-Table1-1.png", "6-Table2-1.png", "7-Table4-1.png", "7-Table3-1.png"], "answer": "Best results on unigram:\nCNN/Daily Mail: Rogue F1 43.85\nNYT: Rogue Recall 49.02\nXSum: Rogue F1 38.81"}, "837": {"question": "What rouge score do they achieve?", "referred_figures_indices": [0, 2], "response": "```json\n{'Answer': 'The captions do not provide specific ROUGE scores. They mention ROUGE F1 scores in tables 2 and 4, but don't state the actual values.'}\n```", "all_figures_names": ["6-Table2-1.png", "7-Table3-1.png", "7-Table4-1.png", "3-Figure1-1.png", "8-Figure2-1.png", "9-Figure3-1.png", "5-Table1-1.png", "9-Table6-1.png"], "referred_figures_names": ["6-Table2-1.png", "7-Table4-1.png"], "answer": "Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55"}}