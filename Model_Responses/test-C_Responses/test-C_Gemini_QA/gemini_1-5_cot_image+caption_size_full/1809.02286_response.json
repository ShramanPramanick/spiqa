{"204": {"question": "Which baselines did they compare against?", "referred_figures_indices": [1, 2], "response": "```json\n{\n 'Image': 2,\n 'Rationale': 'Image 2 is a table that compares their proposed SATA Tree-LSTM with other tree-structured models and other neural models.'\n}\n```\nThe answer is : The baselines they compared against include other tree-structured models like 100D Latent Syntax Tree-LSTM, 300D Tree-based CNN, 300D SPINN-PI, and 300D Gumbel Tree-LSTM. They also compared against other neural models such as 300D NSE, 300D Reinforced Self-Attention Network, 600D Residual stacked encoders, and 600D BiLSTM with generalized pooling. \n", "all_figures_names": ["7-Figure4-1.png", "6-Table1-1.png", "6-Table2-1.png", "7-Figure3-1.png", "3-Figure1-1.png", "4-Figure2-1.png"], "referred_figures_names": ["6-Table1-1.png", "6-Table2-1.png"], "answer": "Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks"}, "205": {"question": "Which baselines did they compare against?", "referred_figures_indices": [1, 2], "response": "```json\n{'Image': 1, 'Rationale': 'This image contains a table that compares the performance of their model, SATA Tree-LSTM, with various other models on different tasks.'}\n```\nThe answer is : The baselines they compared against include various tree-structured models such as RNTN, AdaMC-RNN, TB-RNN, Tree-LSTM, AdaTree-LSTM-CM, DC-Tree-LSTM, TE-Tree-LSTM, BiConTree, Gumbel Tree-LSTM, TreeNet. They also compared against other neural models like CNN, AdaSent, LSTM-CNN, byte-mlst, and BCN with different variations. \n", "all_figures_names": ["7-Figure3-1.png", "6-Table1-1.png", "6-Table2-1.png", "4-Figure2-1.png", "3-Figure1-1.png", "7-Figure4-1.png"], "referred_figures_names": ["6-Table1-1.png", "6-Table2-1.png"], "answer": "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."}}