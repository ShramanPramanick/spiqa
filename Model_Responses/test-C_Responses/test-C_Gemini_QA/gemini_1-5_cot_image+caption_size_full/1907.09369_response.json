{"601": {"question": "Do they report results only on English data?", "referred_figures_indices": [6], "response": "{'Image': 5, 'Rationale': 'This table describes the statistics of the data used for training, which might mention what language the data is in.'}\nThe answer is : The caption does not mention what language the data is in. ", "all_figures_names": ["4-Table5-1.png", "3-Figure1-1.png", "5-Table6-1.png", "4-Table4-1.png", "5-Table7-1.png", "3-Table3-1.png", "2-Table2-1.png", "2-Table1-1.png"], "referred_figures_names": ["2-Table2-1.png"], "answer": "Yes"}, "603": {"question": "What are the hyperparameters of the bi-GRU?", "referred_figures_indices": [6, 2], "response": "```json\n{'Image': 6, 'Rationale': 'The image depicts the Bi-GRU architecture and lists hyperparameters such as dropout rate and number of dense units.'}\n```\nThe answer is : The image doesn't explicitly state all the hyperparameters of the bi-GRU model. However, it shows a dropout rate of 0.5 after the concatenation layer and 70 units in the dense layer. Other potential hyperparameters not shown in the image could include the learning rate, batch size, optimizer used, number of GRU units, and the activation function used in the GRU layers. \n", "all_figures_names": ["5-Table6-1.png", "3-Table3-1.png", "4-Table5-1.png", "5-Table7-1.png", "2-Table2-1.png", "4-Table4-1.png", "3-Figure1-1.png", "2-Table1-1.png"], "referred_figures_names": ["3-Figure1-1.png", "4-Table5-1.png"], "answer": "They use the embedding layer with a size 35 and embedding dimension of 300. They use a dense layer with 70 units and a dropout layer with a rate of 50%."}}