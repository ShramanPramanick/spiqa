{"801": {"question": "how small of a dataset did they train on?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'Image 1 breaks down the sizes of the training, validation, and test sets used for each dataset.'}\nThe answer is : The training set sizes were 16,150 and 6,935 for the Shenma Voice Search and Amap datasets, respectively. \n", "all_figures_names": ["6-Table4-1.png", "4-Table1-1.png", "6-Table3-1.png", "6-Table2-1.png", "5-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "23085 hours of data"}, "802": {"question": "what was their character error rate?", "referred_figures_indices": [0, 4, 2], "response": "```json\n{'Image': 0, 'Rationale': 'This table contains character error rate (CER) values for different models.'}\n```\nThe answer is : The table provides the CER for various models but does not specify for whom \"their\" refers to. Therefore, I cannot answer the question. \n", "all_figures_names": ["6-Table3-1.png", "5-Figure1-1.png", "6-Table2-1.png", "4-Table1-1.png", "6-Table4-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table4-1.png", "6-Table2-1.png"], "answer": "2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning."}, "803": {"question": "what was their character error rate?", "referred_figures_indices": [3, 2], "response": "{'Image': 3, 'Rationale': 'Image 3 contains a table listing the CER for various LSTM models.'}\nThe answer is : The 9-layers LSTM has a CER of 2.49%, while the 2-layer regular-trained and distilled LSTMs have CERs of 3.06% and 2.63% respectively. \n", "all_figures_names": ["5-Figure1-1.png", "6-Table4-1.png", "6-Table2-1.png", "6-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Their best model achieved a 2.49% Character Error Rate."}, "804": {"question": "which lstm models did they compare with?", "referred_figures_indices": [0, 4], "response": "{'Image': 0, 'Rationale': 'This table explicitly lists various LSTM architectures and their corresponding performance metrics, directly relating to the question.'}\nThe answer is : The compared LSTM models are a 9-layers LSTM, a 2-layers regular-trained LSTM, and a 2-layers distilled LSTM. \n", "all_figures_names": ["6-Table3-1.png", "6-Table4-1.png", "5-Figure1-1.png", "4-Table1-1.png", "6-Table2-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."}}