{"583": {"question": "Does this method help in sentiment classification task improvement?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Rationale': 'This table compares the performance of different methods on the sentiment classification task.'}\nThe answer is : Adding shallow syntactic features improves the performance of ELMo-transformer on the sentiment classification task. \n", "all_figures_names": ["1-Figure1-1.png", "2-Table1-1.png", "8-Table6-1.png", "5-Table2-1.png", "8-Table5-1.png", "5-Table4-1.png", "5-Table3-1.png", "3-Figure2-1.png"], "referred_figures_names": ["5-Table2-1.png"], "answer": "Yes"}, "584": {"question": "Does this method help in sentiment classification task improvement?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Rationale': 'Image 7 has results for sentiment classification with the method in question.'}\nThe answer is : Yes, shallow syntactic contextualization (i.e., mSynC) shows improvement over the baseline and ELMo-transformer models for the sentiment classification task. \n", "all_figures_names": ["8-Table6-1.png", "2-Table1-1.png", "5-Table4-1.png", "5-Table3-1.png", "1-Figure1-1.png", "8-Table5-1.png", "3-Figure2-1.png", "5-Table2-1.png"], "referred_figures_names": ["5-Table2-1.png"], "answer": "No"}, "586": {"question": "For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo\u2019s embedding?", "referred_figures_indices": [2], "response": "```json\n{\n 'Image': 2,\n 'Rationale': 'Image 2 is a table that has the performance of both ELMo and mSynC on several linguistic probes, one of them being NER. The question asks for the number of probes for which shallow-syntax-aware contextual embedding (mSynC) performs better than ELMo.'\n}\n```\nThe answer is : 6 \n", "all_figures_names": ["8-Table5-1.png", "5-Table4-1.png", "5-Table3-1.png", "1-Figure1-1.png", "5-Table2-1.png", "8-Table6-1.png", "2-Table1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["5-Table3-1.png"], "answer": "3"}, "587": {"question": "What are the black-box probes used?", "referred_figures_indices": [4, 3], "response": "{'Image': 3, 'Rationale': 'Image 3 lists probing tasks which include Named Entity Recognition.'}\nThe answer is : The black-box probes used are CCG Supertagging, PTB part-of-speech tagging, EWT part-of-speech tagging, Chunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function and Event Factuality Detection. \n", "all_figures_names": ["8-Table5-1.png", "5-Table2-1.png", "5-Table4-1.png", "8-Table6-1.png", "5-Table3-1.png", "3-Figure2-1.png", "2-Table1-1.png", "1-Figure1-1.png"], "referred_figures_names": ["5-Table3-1.png", "8-Table6-1.png"], "answer": "CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,\nChunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection"}}