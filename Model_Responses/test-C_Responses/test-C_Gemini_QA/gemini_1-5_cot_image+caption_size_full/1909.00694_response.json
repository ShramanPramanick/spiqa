{"2": {"question": "What are the results?", "referred_figures_indices": [4, 1], "response": "{'Image': 4, 'Rationale': 'Image 4 shows the result of various models, like BiGRU, BERT with different training datasets.'}\nThe answer is : The best result achieved is 0.933 accuracy by BERT with ACP dataset training. \n", "all_figures_names": ["4-Table2-1.png", "5-Table4-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["5-Table3-1.png", "5-Table4-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'Image 4 shows examples of how discourse relations, such as Cause and Concession, can be used to propagate polarity from one event to another.'} \nThe answer is : Discourse relations, like CAUSE and CONCESSION, can propagate polarity between events. For example, if a latter event has positive polarity and is linked to a former event by a CAUSE relation, the former event is likely to also have positive polarity. Conversely, a CONCESSION relation might suggest the former event to have an opposite polarity to the latter event. \n", "all_figures_names": ["5-Table3-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table5-1.png", "2-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [1], "response": "```json\n{\n 'Image': 1,\n 'Rationale': 'Image 1 shows how discourse relations such as CAUSE and CONCESSION are used to determine whether the polarity of an event should be the same or the reverse of a related event.'\n}\n```\n\nThe answer is : Discourse relations like CAUSE and CONCESSION connect pairs of events.  The polarity of one event, along with the relation, can be used to determine the polarity of the other event. For CAUSE relations, the polarity of the related events tends to be the same, while for CONCESSION relations, the polarity of the events tends to be reversed. \n", "all_figures_names": ["4-Table2-1.png", "2-Figure1-1.png", "5-Table4-1.png", "4-Table1-1.png", "5-Table3-1.png", "5-Table5-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [5, 3], "response": "{'Image': 5, 'Rationale': 'The table gives numbers of pairs for different types of data. This table is related to the size of the data.'}\nThe answer is : The table shows 1,000,000 automatically labeled pairs, 5,000,000 CAUSE pairs, and 5,000,000 CONCESSION pairs in the Japanese data. \n", "all_figures_names": ["5-Table5-1.png", "2-Figure1-1.png", "5-Table3-1.png", "4-Table2-1.png", "5-Table4-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png", "4-Table2-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [4], "response": "{'Image': 0, 'Rationale': 'Image 0 lists the number of pairs for different Japanese datasets: AL (1,000,000), CA (5,000,000), and CO (5,000,000).'}\nThe answer is : The total number of pairs in the Japanese data is 11,000,000 (1 million + 5 million + 5 million). \n", "all_figures_names": ["4-Table1-1.png", "5-Table3-1.png", "5-Table5-1.png", "2-Figure1-1.png", "4-Table2-1.png", "5-Table4-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Rationale': 'This table compares performance of proposed approach on smaller labeled data (ACP (6K)) with the basic approach.'}\nThe answer is : Using the proposed approach improves the accuracy from 0.876 to 0.886 when using BERT and from 0.830 to 0.879 when using BiGRU, given the smaller labeled dataset ACP (6K). \n", "all_figures_names": ["5-Table5-1.png", "4-Table1-1.png", "5-Table3-1.png", "2-Figure1-1.png", "4-Table2-1.png", "5-Table4-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [2], "response": "```json\n{'Image': 1, 'Rationale': 'This image describes the different types of labeled data pairs used in their model training, which is key to understanding their use of raw data.'}\n```\nThe answer is : The model leverages a large dataset of \"Automatically Labeled Pairs\" (AL) consisting of 1,000,000 pairs. These pairs likely consist of raw text data with automatically assigned labels, enabling the model to learn from a vast amount of minimally processed data. \n", "all_figures_names": ["5-Table4-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table5-1.png", "4-Table2-1.png", "5-Table3-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [5], "response": "{'Image': 1, 'Rationale': 'This image mentions that the seed lexicon is used to identify positive and negative words in automatically labelled event pairs (AL)'}\nThe answer is : Although the size of the seed lexicon is not mentioned, Image 1 states that a seed lexicon is used to automatically label the polarity of 1,000,000 event pairs. \n", "all_figures_names": ["5-Table3-1.png", "2-Figure1-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table5-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}