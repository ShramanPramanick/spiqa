{"654": {"question": "How they extract \"structured answer-relevant relation\"?", "referred_figures_indices": [6], "response": "{'Image': 3, 'Rationale': 'Image 3 shows an example of structured answer-relevant relation.'} \nThe answer is : Structured answer-relevant relation is extracted by finding the relation in the sentence that contains the answer. For example, in the sentence \"Beyonc\u00e9 received critical acclaim and commercial success, selling one million digital copies worldwide in six days\", the structured answer-relevant relation for the question \"How many digital copies of her fifth album did Beyonc\u00e9 sell in six days?\" is \"(Beyonc\u00e9; received commercial success selling; one million digital copies worldwide; in six days)\". \n", "all_figures_names": ["8-Figure5-1.png", "7-Table5-1.png", "3-Figure2-1.png", "7-Figure4-1.png", "6-Table4-1.png", "4-Figure3-1.png", "1-Figure1-1.png", "5-Table3-1.png"], "referred_figures_names": ["1-Figure1-1.png"], "answer": "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."}, "656": {"question": "How big are significant improvements?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'Image 1 shows the experimental results of the model and compares it to other models. Improvements can be determined numerically.'}\nThe answer is : The improvements range from around 1 to 3 points for most models. ", "all_figures_names": ["3-Table2-1.png", "6-Table4-1.png", "5-Table3-1.png", "1-Figure1-1.png", "8-Figure5-1.png", "7-Table5-1.png", "4-Figure3-1.png", "2-Table1-1.png"], "referred_figures_names": ["6-Table4-1.png"], "answer": "Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"}}