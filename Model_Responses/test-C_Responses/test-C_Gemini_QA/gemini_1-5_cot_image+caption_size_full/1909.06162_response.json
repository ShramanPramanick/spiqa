{"909": {"question": "What is best performing model among author's submissions, what performance it had?", "referred_figures_indices": [3], "response": "```json\n{'Image': 3, 'Rationale': 'This is the only image that shows model performance and we are looking for the author\\'s submission performance.'}\n```\nThe answer is : The MIC-CIS model had a score of 0.6231 F1 / 0.5736 Precision / 0.6819 Recall for the SLC task.", "all_figures_names": ["4-Table3-1.png", "3-Figure1-1.png", "2-Table1-1.png", "3-Table2-1.png", "4-Table4-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."}, "910": {"question": "What extracted features were most influencial on performance?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'This image shows different features used for SLC, along with their scores. So it can answer what features were most impactful.'}\nThe answer is : Based on Table 3: SLC scores on Dev (internal), the most influential features for SLC performance were using BERT with Fine-tune (\u03c4 \u2265 35), achieving an F1 score of 0.650 on Dev (external). \n", "all_figures_names": ["3-Table2-1.png", "4-Table4-1.png", "4-Table3-1.png", "2-Table1-1.png", "3-Figure1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "Linguistic"}, "912": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [0], "response": "```json\n{'Image': 1, 'Rationale': 'This image showcases the performance of an ensemble model versus individual models for the FLC task.'}\n```\nThe answer is : Using an ensemble of model (II) and (IV) increased performance from 0.129 to 0.164 F1 points for the FLC task. This represents a 27% relative improvement. \n", "all_figures_names": ["4-Table3-1.png", "4-Table4-1.png", "3-Table2-1.png", "2-Table1-1.png", "3-Figure1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "The best ensemble topped the best single model by 0.029 in F1 score on dev (external)."}, "913": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [3, 1, 2], "response": "{'Image': 2, 'Rationale': 'Image 2 shows the performance of different models for SLC, including ensembles.'}\nThe answer is : Based on Table 3, ensembles boosted performance. The highest F1 score for Dev (external) was achieved by an ensemble of models r3, r5, r6, and r12, resulting in an F1 score of .678, an increase from .650 for model r12 alone. \n", "all_figures_names": ["3-Figure1-1.png", "3-Table2-1.png", "4-Table3-1.png", "4-Table4-1.png", "2-Table1-1.png"], "referred_figures_names": ["4-Table4-1.png", "3-Table2-1.png", "4-Table3-1.png"], "answer": "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"}, "914": {"question": "Which basic neural architecture perform best by itself?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Rationale': 'Image 3 shows the performance of several classifiers by themselves and in ensembles, which is helpful to answer the question.'}\nThe answer is : Based on Table 3, BERT with fine-tuning when the relax boundary is greater than or equal to 35 (r12) performs best by itself, achieving an F1 score of 0.65 on the development set. \n", "all_figures_names": ["3-Table2-1.png", "3-Figure1-1.png", "4-Table4-1.png", "4-Table3-1.png", "2-Table1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "BERT"}, "915": {"question": "What participating systems had better results than ones authors submitted?", "referred_figures_indices": [4], "response": "```json\n{'Image': 4, 'Rationale': 'Image 4 shows the F1 scores of the participating systems compared to the author's submission (MIC-CIS).'}\n```\nThe answer is :  *newspeak* for FLC task and *ProperGender,  YMJA* for SLC task had better results than the author's submission (MIC-CIS). \n", "all_figures_names": ["4-Table4-1.png", "3-Figure1-1.png", "2-Table1-1.png", "4-Table3-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."}, "916": {"question": "What is specific to multi-granularity and multi-tasking neural arhiteture design?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'The image describes a multi-granularity, multi-tasking architecture for propaganda detection.'}\nThe answer is : Multi-granularity and multi-tasking neural architecture design is specific in jointly performing multiple related tasks at different levels of granularity within a single architecture, enabling knowledge sharing and improving overall performance. For instance, in propaganda detection, one might have tasks for identifying propaganda at the sentence level (coarse-grained) and at the phrase level (fine-grained), with shared components in the architecture allowing these tasks to benefit from each other. \n", "all_figures_names": ["3-Table2-1.png", "3-Figure1-1.png", "2-Table1-1.png", "4-Table3-1.png", "4-Table4-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "An output layer for each task"}}