{"1133": {"question": "Do they do quantitative quality analysis of learned embeddings?", "referred_figures_indices": [1], "response": "{'Image': 2, 'Rationale': 'This table lists scores for \"Functional Dissimilarity\" which refers to the quality of the embeddings.'}\nThe answer is : Yes, they measure the quality of learned embeddings quantitatively using Functional Dissimilarity Scores, as shown in Table 5. Lower scores indicate better embedding quality. \n", "all_figures_names": ["6-Table6-1.png", "4-Table3-1.png", "6-Table5-1.png", "4-Table2-1.png", "6-Table4-1.png", "3-Figure1-1.png", "2-Table1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "Yes"}, "1134": {"question": "Do they evaluate on downstream tasks?", "referred_figures_indices": [2], "response": "{'Image': 5, 'Rationale': 'This table compares the models with respect to syntactic nearest neighbor accuracy. This will tell us whether downstream tasks are considered'}\nThe answer is : The paper does evaluate on downstream tasks, specifically, they report accuracy for syntactic nearest neighbor search on various languages.", "all_figures_names": ["3-Figure1-1.png", "2-Table1-1.png", "6-Table6-1.png", "6-Table5-1.png", "4-Table2-1.png", "6-Table4-1.png", "4-Table3-1.png"], "referred_figures_names": ["6-Table6-1.png"], "answer": "Yes"}}