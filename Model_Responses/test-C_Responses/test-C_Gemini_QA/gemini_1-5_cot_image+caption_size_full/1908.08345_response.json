{"836": {"question": "What rouge score do they achieve?", "referred_figures_indices": [7, 2, 3, 0], "response": "{'Image': 7, 'Rationale': 'Image 7 tabulates ROUGE F1 scores for various models, including BERT.'}\nThe answer is : The ROUGE score achieved varies depending on the specific BERT model and dataset. For instance, on the CNN/DailyMail dataset, BERTSUMEXT achieves a ROUGE-1 F1 score of 43.25, ROUGE-2 F1 score of 20.24, and a ROUGE-L F1 score of 39.36. \n", "all_figures_names": ["5-Table1-1.png", "9-Figure3-1.png", "7-Table3-1.png", "7-Table4-1.png", "9-Table6-1.png", "8-Table5-1.png", "3-Figure1-1.png", "6-Table2-1.png"], "referred_figures_names": ["6-Table2-1.png", "7-Table3-1.png", "7-Table4-1.png", "5-Table1-1.png"], "answer": "Best results on unigram:\nCNN/Daily Mail: Rogue F1 43.85\nNYT: Rogue Recall 49.02\nXSum: Rogue F1 38.81"}, "837": {"question": "What rouge score do they achieve?", "referred_figures_indices": [7, 1], "response": "{'Image': 1, 'Rationale': 'This image contains a table of ROUGE scores for different summarization models.'}\nThe answer is : The provided text and captions do not mention a specific ROUGE score achieved. \n", "all_figures_names": ["9-Figure3-1.png", "6-Table2-1.png", "3-Figure1-1.png", "8-Figure2-1.png", "9-Table7-1.png", "7-Table3-1.png", "9-Table6-1.png", "7-Table4-1.png"], "referred_figures_names": ["7-Table4-1.png", "6-Table2-1.png"], "answer": "Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55"}}