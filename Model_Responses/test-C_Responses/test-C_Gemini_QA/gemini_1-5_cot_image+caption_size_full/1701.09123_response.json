{"951": {"question": "what are the evaluation metrics?", "referred_figures_indices": [1], "response": "{'Image': 0, 'Rationale': 'This image shows the evaluation metrics such as Precision, Recall, and F1.'}\nThe answer is : The evaluation metrics are Precision, Recall and F1. \n", "all_figures_names": ["17-Table11-1.png", "15-Table5-1.png", "6-Table2-1.png", "5-Table1-1.png", "19-Table15-1.png", "20-Table16-1.png", "22-Table19-1.png", "15-Table6-1.png"], "referred_figures_names": ["15-Table5-1.png"], "answer": "Precision, Recall, F1"}, "952": {"question": "which datasets were used in evaluation?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Rationale': 'This table explicitly shows the datasets used for evaluation for various languages.'}\nThe answer is : The datasets used for evaluation vary depending on the language: \n* English: CoNLL 2003, MUC7, Wikigold, and MEANTIME\n* German: GermEval 2014\n* Spanish: CoNLL 2002, Ancora 2.0, and MEANTIME\n* Dutch: CoNLL 2002 and SONAR-1\n* Basque: Egunkaria 1999-2003. \n", "all_figures_names": ["17-Table11-1.png", "22-Table18-1.png", "15-Table6-1.png", "22-Table19-1.png", "9-Table3-1.png", "5-Table1-1.png", "16-Table9-1.png", "19-Table14-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0"}}