{"585": {"question": "What evaluations did the authors use on their system?", "referred_figures_indices": [3, 1, 7], "response": "```json\n{'Image': 1, 'Rationale': 'Image 1 shows a table of experiment results with different models and their BLEU scores on the NIST Chinese-English translation tasks.'}\n```\nThe answer is : The authors used BLEU scores on the NIST Chinese-English translation tasks to evaluate their system. \n", "all_figures_names": ["4-Figure1-1.png", "8-Table1-1.png", "9-Table4-1.png", "9-Table3-1.png", "8-Table2-1.png", "5-Figure2-1.png", "9-Table5-1.png", "10-Table6-1.png"], "referred_figures_names": ["9-Table3-1.png", "8-Table1-1.png", "10-Table6-1.png"], "answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."}}