{"753": {"question": "Is the new model evaluated on the tasks that BERT and ELMo are evaluated on?", "referred_figures_indices": [4], "response": "```json\n{'Image': 4, 'Rationale': 'Image 4 shows the performance of models (including BERT and ELMo) on the GLUE benchmark with and without STILTs.'}\n```\nThe answer is : Yes, the new model is evaluated on the GLUE benchmark, which includes tasks like RTE that BERT and ELMo are evaluated on. \n", "all_figures_names": ["8-Table3-1.png", "5-Table2-1.png", "6-Figure1-1.png", "12-Table4-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "Yes"}, "754": {"question": "Does the additional training on supervised tasks hurt performance in some tasks?", "referred_figures_indices": [3], "response": "{'Image': 4, 'Rationale': 'Image 4 shows the distribution of task scores with and without intermediate fine-tuning on MNLI across different tasks and training sizes. We can look for instances where performance degrades with additional MNLI training.'}\nThe answer is : Yes, in some cases, additional training on supervised tasks like MNLI can hurt performance on certain downstream tasks, especially when fine-tuning with limited data.  Image 4 demonstrates this, showing that intermediate fine-tuning on MNLI leads to a wider distribution of scores and even worse performance in some cases (e.g., RTE with limited training data). \n", "all_figures_names": ["5-Table2-1.png", "12-Table4-1.png", "8-Table3-1.png", "4-Table1-1.png", "6-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "Yes"}}