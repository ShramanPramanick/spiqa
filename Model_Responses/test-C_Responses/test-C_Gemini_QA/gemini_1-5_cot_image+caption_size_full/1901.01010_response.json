{"883": {"question": "Do the methods that work best on academic papers also work best on Wikipedia?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Rationale': 'Image 3 shows a table comparing performance on academic papers and wikipedia.'}\nThe answer is : No, the best performing method on academic papers (77.1/77.3 on Peer Review) is not the best performing on Wikipedia (JOINT at 59.4).", "all_figures_names": ["1-Figure1-1.png", "4-Table2-1.png", "3-Figure2-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "No"}, "884": {"question": "What is their system's absolute accuracy?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Rationale': 'Image 0 is a table of experimental results, which is likely to have the absolute accuracy.'}\nThe answer is : 59.4\u00b10.47%", "all_figures_names": ["4-Table1-1.png", "1-Figure1-1.png", "3-Figure2-1.png", "4-Table2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"}, "887": {"question": "Which languages do they use?", "referred_figures_indices": [1], "response": "{'Image': 3, 'Rationale': 'Image 3 shows experimental results for classifying the quality of Wikipedia articles, implying that the question refers to the languages used in the Wikipedia articles.'}\nThe answer is : The image and captions do not provide information about which languages are used. \n", "all_figures_names": ["3-Figure2-1.png", "4-Table2-1.png", "1-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "English"}, "888": {"question": "How large is their data set?", "referred_figures_indices": [3], "response": "```json\n{\n 'Image': 0,\n 'Rationale': 'This image is a confusion matrix that summarizes the performance of a model on the dataset, which could provide insights into the size of the dataset.'\n}\n```The answer is : While the confusion matrix provides information about the distribution of classes within the dataset, it doesn't explicitly state the actual size of the dataset. Therefore, I cannot answer your question based on the images provided. \n", "all_figures_names": ["4-Table2-1.png", "1-Figure1-1.png", "3-Figure2-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "a sample of  29,794 wikipedia articles and 2,794 arXiv papers "}}