{"856": {"question": "What baseline did they compare Entity-GCN to?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'This image lists the accuracy of different models on WIKIHOP, allowing for a direct comparison with Entity-GCN.'}\nThe answer is : Table 2 shows that Entity-GCN was compared to several models including: Human Performance, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, and MHQA-GRN. \n", "all_figures_names": ["1-Figure1-1.png", "5-Table1-1.png", "8-Table4-1.png", "13-Figure3-1.png", "6-Table2-1.png", "7-Table3-1.png", "11-Table5-1.png", "12-Table6-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}, "858": {"question": "Did they use a relation extraction method to construct the edges in the graph?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'Image 4 showcases a graph with supporting documents organized in ellipses. The caption explains different edges connecting nodes, which helps to understand how edges are constructed in their model.'}\nThe answer is : The caption explicitly states that nodes are connected based on co-occurrence, exact match, or coreference. Thus, they do not employ any specific relation extraction method to form the edges. \n", "all_figures_names": ["8-Table4-1.png", "7-Table3-1.png", "6-Table2-1.png", "13-Figure3-1.png", "3-Figure2-1.png", "5-Table1-1.png", "12-Table6-1.png", "1-Figure1-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "No"}, "859": {"question": "How did they get relations between mentions?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Rationale': 'Image 7 describes how mentions are connected, which helps to understand their relationships.'}\nThe answer is : The relations between mentions are determined by co-occurrence in the same document, exact match of mentions, or coreference relationships. These relationships are represented as edges in a graph where nodes represent mentions of candidate or query entities. \n", "all_figures_names": ["1-Figure1-1.png", "6-Table2-1.png", "11-Table5-1.png", "5-Table1-1.png", "8-Table4-1.png", "12-Table6-1.png", "13-Figure3-1.png", "3-Figure2-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."}, "862": {"question": "What performance does the Entity-GCN get on WIKIHOP?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Rationale': 'This image contains the exact performance numbers of different models on the WIKIHOP dataset, including Entity-GCN.'}\nThe answer is : The Entity-GCN achieves an accuracy of 66.4 on the WIKIHOP dataset when using a single model and 71.2 when using an ensemble of 5 models. \n", "all_figures_names": ["6-Table2-1.png", "5-Table1-1.png", "8-Table4-1.png", "7-Table3-1.png", "3-Figure2-1.png", "1-Figure1-1.png", "11-Table5-1.png", "13-Figure3-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"}}