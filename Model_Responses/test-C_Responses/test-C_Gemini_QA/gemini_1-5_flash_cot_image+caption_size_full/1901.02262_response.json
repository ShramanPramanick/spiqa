{"457": {"question": "What do they mean by answer styles?", "referred_figures_indices": [3], "response": "{'Image': 6, 'Rationale': 'This table shows results for models using different answer styles'}\nThe answer is : The authors are referring to the different styles of answers that the model can generate, such as NLG style or Q&A style.", "all_figures_names": ["7-Figure5-1.png", "6-Table4-1.png", "6-Table2-1.png", "5-Table1-1.png", "2-Figure2-1.png", "8-Table5-1.png", "6-Table3-1.png", "7-Figure4-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "referred_figures_indices": [5], "response": "{'Image': 3, 'Rationale': 'The image shows two answer styles'}\nThe answer is : No, there are two answer styles, NLG and Q&A.", "all_figures_names": ["8-Table5-1.png", "6-Table3-1.png", "4-Figure3-1.png", "1-Figure1-1.png", "7-Figure4-1.png", "5-Table1-1.png", "6-Table2-1.png", "7-Figure5-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "referred_figures_indices": [6, 4], "response": "{'Image': 4, 'Rationale': 'It shows performance of other models and Masque is compared against them'}\nThe answer is : The baselines that Masque is compared against are BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, and Human Performance.", "all_figures_names": ["7-Figure4-1.png", "1-Figure1-1.png", "6-Table3-1.png", "5-Table1-1.png", "6-Table2-1.png", "4-Figure3-1.png", "8-Table5-1.png", "7-Figure5-1.png"], "referred_figures_names": ["8-Table5-1.png", "6-Table2-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Rationale': 'This image contains a table showing the performance of various models on NarrativeQA'}\nThe answer is : The table shows that the performance of Masque on the NarrativeQA test set is 54.11 for B-1, 30.43 for B-4, 26.13 for M, and 59.87 for R-L.", "all_figures_names": ["8-Table5-1.png", "7-Figure5-1.png", "7-Figure4-1.png", "5-Table1-1.png", "6-Table4-1.png", "4-Figure3-1.png", "6-Table2-1.png", "2-Figure2-1.png"], "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "referred_figures_indices": [2], "response": "{'Image': 5, 'Rationale': 'This table shows the performance of the model with different answer styles'}\nThe answer is : \"Answer style\" refers to different ways that a model can answer a question. For example, a model might answer a question by providing a single sentence, multiple sentences, or a list of facts.", "all_figures_names": ["7-Figure5-1.png", "7-Figure4-1.png", "5-Table1-1.png", "4-Figure3-1.png", "8-Table5-1.png", "6-Table3-1.png", "6-Table2-1.png", "6-Table4-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}