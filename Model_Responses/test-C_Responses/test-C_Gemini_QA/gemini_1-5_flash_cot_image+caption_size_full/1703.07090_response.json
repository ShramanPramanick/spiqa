{"801": {"question": "how small of a dataset did they train on?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'Table showing dataset sizes for two different voice search models'}\nThe answer is : The Shenma model was trained on 16150 data points and the Amap model was trained on 6935 data points.", "all_figures_names": ["6-Table2-1.png", "5-Figure1-1.png", "6-Table4-1.png", "6-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "23085 hours of data"}, "802": {"question": "what was their character error rate?", "referred_figures_indices": [1, 2, 4], "response": "{'Image': 1, 'Rationale': 'Image 1 is the only one that contains CER.'}\nThe answer is :  The character error rate was 6.26.", "all_figures_names": ["4-Table1-1.png", "6-Table4-1.png", "6-Table3-1.png", "5-Figure1-1.png", "6-Table2-1.png"], "referred_figures_names": ["6-Table4-1.png", "6-Table3-1.png", "6-Table2-1.png"], "answer": "2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning."}, "803": {"question": "what was their character error rate?", "referred_figures_indices": [1, 2], "response": "{'Image': 1, 'Rationale': 'Contains CER column'}\nThe answer is : 2.49", "all_figures_names": ["4-Table1-1.png", "6-Table3-1.png", "6-Table2-1.png", "5-Figure1-1.png", "6-Table4-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Their best model achieved a 2.49% Character Error Rate."}, "804": {"question": "which lstm models did they compare with?", "referred_figures_indices": [3, 1], "response": "{'Image': 3, 'Rationale': 'Image 3 describes the models that were used and provides CER information'}\nThe answer is : They compared with a 9-layer LSTM, a 2-layer regular-trained LSTM, and a 2-layer distilled LSTM.", "all_figures_names": ["4-Table1-1.png", "6-Table2-1.png", "6-Table4-1.png", "6-Table3-1.png", "5-Figure1-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."}}