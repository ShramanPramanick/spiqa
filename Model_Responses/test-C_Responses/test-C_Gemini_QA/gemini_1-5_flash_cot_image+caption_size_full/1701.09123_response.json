{"951": {"question": "what are the evaluation metrics?", "referred_figures_indices": [4], "response": "{'Image': 0, 'Rationale': 'It has a table with F1 score which is a common evaluation metric'}\nThe answer is : The evaluation metrics are precision, recall, and F1 score.", "all_figures_names": ["15-Table6-1.png", "5-Table1-1.png", "22-Table18-1.png", "21-Table17-1.png", "15-Table5-1.png", "6-Table2-1.png", "20-Table16-1.png", "13-Table4-1.png"], "referred_figures_names": ["15-Table5-1.png"], "answer": "Precision, Recall, F1"}, "952": {"question": "which datasets were used in evaluation?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Rationale': 'This image lists the datasets used for training, development and evaluation.'}\nThe answer is : The datasets used for training, development and evaluation are listed in Table 1. MUC7, Wikigold, MEANTIME, SONAR-1 and Ancora 2.0 are used for out-of-domain evaluation.  CoNLL 2003, GermEval 2014, CoNLL 2002, CoNLL 2002, and Egunkaria are used for in-domain evaluation.", "all_figures_names": ["20-Table16-1.png", "16-Table8-1.png", "16-Table7-1.png", "13-Table4-1.png", "22-Table19-1.png", "5-Table1-1.png", "19-Table14-1.png", "22-Table18-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0"}}