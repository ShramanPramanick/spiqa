{"654": {"question": "How they extract \"structured answer-relevant relation\"?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'Image 4 shows the structured answer relevant relation for a sentence and a question.'}\nThe answer is : It represents a relationship between a portion of a sentence and an answer to a question. The example shows \"The daily mean temperature in January\" as a portion of the sentence and \"32.6 \u00b0F (0.3 \u00b0C)\" as the answer to the question.", "all_figures_names": ["8-Figure5-1.png", "2-Table1-1.png", "5-Table3-1.png", "7-Table5-1.png", "1-Figure1-1.png", "3-Table2-1.png", "4-Figure3-1.png", "6-Table4-1.png"], "referred_figures_names": ["1-Figure1-1.png"], "answer": "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."}, "656": {"question": "How big are significant improvements?", "referred_figures_indices": [0], "response": "{'Image': 2, 'Rationale': 'This image shows percentage improvements for \"our model\" over \"hybrid model.\"'}\nThe answer is : The improvements range from 3.96% to 4.30%.", "all_figures_names": ["6-Table4-1.png", "1-Figure1-1.png", "7-Table5-1.png", "8-Figure5-1.png", "7-Figure4-1.png", "5-Table3-1.png", "3-Figure2-1.png", "3-Table2-1.png"], "referred_figures_names": ["6-Table4-1.png"], "answer": "Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"}}