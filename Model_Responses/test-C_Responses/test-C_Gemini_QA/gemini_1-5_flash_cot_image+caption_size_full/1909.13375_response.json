{"204": {"question": "What is difference in peformance between proposed model and state-of-the art on other question types?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'This table shows the performance of the proposed model compared to other models on different types of questions'}\nThe answer is : The proposed model outperforms other models in terms of Exact Match (EM) and F1 scores for all, multi-span, and single-span questions. For example, on the development set, the proposed model achieved an EM score of 76.91 and an F1 score of 79.92 for all question types, while the NBERT+ model achieved an EM score of 61.76 and an F1 score of 65.48.", "all_figures_names": ["6-Table1-1.png", "6-Table2-1.png", "6-Table3-1.png", "7-Table4-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1."}, "205": {"question": "What is the performance of proposed model on entire DROP dataset?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Rationale': 'Table 0 shows the performance of various models on DROP datasets.'}\nThe answer is :  The performance of the proposed model on the entire DROP dataset is shown in the last row of the table. It achieves an EM score of 77.63 and an F1 score of 80.73.", "all_figures_names": ["6-Table3-1.png", "6-Table2-1.png", "7-Table4-1.png", "6-Table1-1.png"], "referred_figures_names": ["6-Table3-1.png"], "answer": "The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev"}}