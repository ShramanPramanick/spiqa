{"31": {"question": "What experiments are proposed to test that upper layers produce context-specific embeddings?", "referred_figures_indices": [2, 0, 1], "response": "{'Image': 1, 'Rationale': 'Image 1 shows how self-similarity, a measure of context-specificity, varies between layers.'}\nThe answer is : The experiment measures the average self-similarity of uniformly randomly sampled words after adjusting for anisotropy, which is a measure of how much context-specificity manifests in the representation space. In all three models, ELMo, BERT, and GPT-2, the higher the layer, the lower the self-similarity, suggesting that contextualized word representations are more context-specific in higher layers.", "all_figures_names": ["5-Figure1-1.png", "6-Figure2-1.png", "7-Figure3-1.png", "8-Table1-1.png", "8-Figure4-1.png"], "referred_figures_names": ["7-Figure3-1.png", "5-Figure1-1.png", "6-Figure2-1.png"], "answer": "They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  \nThey plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.\nThey plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another."}, "32": {"question": "How do they calculate a static embedding for each word?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Rationale': 'Image 0 shows a table which lists different static embedding methods along with performance results for each method'}\nThe answer is : The first principal component of a word\u2019s contextualized representations in a given layer is used to create static embeddings for contextualizing models such as ELMo, BERT, and GPT-2.  The embedding can be thought of as a single vector which represents the word.  The researchers found that static embeddings created using ELMo and BERT's contextualized representations generally outperformed embeddings based on GloVe and FastText vectors.", "all_figures_names": ["8-Table1-1.png", "7-Figure3-1.png", "8-Figure4-1.png", "5-Figure1-1.png", "6-Figure2-1.png"], "referred_figures_names": ["8-Table1-1.png"], "answer": "They use the first principal component of a word's contextualized representation in a given layer as its static embedding."}}