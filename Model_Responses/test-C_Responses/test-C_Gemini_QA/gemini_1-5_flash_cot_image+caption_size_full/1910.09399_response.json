{"159": {"question": "What is typical GAN architecture for each text-to-image synhesis group?", "referred_figures_indices": [7], "response": "{'Image': 4, 'Rationale': 'Image 4 shows the architecture of each GAN'}\nThe answer is : The architectures of these GANs are as follows:\\\n(a) Conditional GAN (cGAN) (Mirza and Osindero, 2014a) use labels to condition the input to the generator and the discriminator.\\\n(b) Manifold interpolation matching-aware discriminator GAN (GAN-INT-CLS) (Reed et al., 2016b) feeds text input to both generator and discriminator (texts are preprocessed as embedding features, using function \u03d5(), and concatenated with other input, before feeding to both generator and discriminator).\\\n(c) Auxiliary classifier GAN (AC-GAN) (Odena et al., 2017b) uses an auxiliary classifier layer to predict the class of the image to ensure that the output consists of images from different classes, resulting in diversified synthesis images.\\\n(d) text conditioned auxiliary classifier GAN (TACGAN) (Dash et al., 2017a) share similar design as GAN-INT-CLS, whereas the output include both a discriminator and a classifier (which can be used for classification).\\\n(e) text conditioned semantic classifier GAN (Text-SeGAN) (Cha et al., 2019a) uses a regression layer to estimate the semantic relevance between the image, so the generated images are not limited to certain classes and are semantically matching to the text input.", "all_figures_names": ["22-Figure12-1.png", "2-Figure1-1.png", "21-Figure10-1.png", "21-Figure11-1.png", "9-Figure7-1.png", "10-Figure8-1.png", "3-Figure3-1.png", "12-Figure9-1.png"], "referred_figures_names": ["12-Figure9-1.png"], "answer": "Semantic Enhancement GANs: DC-GANs, MC-GAN\nResolution Enhancement GANs: StackGANs, AttnGAN, HDGAN\nDiversity Enhancement GANs: AC-GAN, TAC-GAN etc.\nMotion Enhancement GAGs: T2S, T2V, StoryGAN"}}