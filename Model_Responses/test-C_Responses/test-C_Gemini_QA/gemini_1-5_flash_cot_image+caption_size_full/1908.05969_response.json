{"1340": {"question": "Which are the sequence model architectures this method can be transferred across?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'The table shows the F1-score for different sequence models for the given method'}\nThe answer is : The sequence model architectures this method can be transferred across are LSTM, CNN, and Transformer.", "all_figures_names": ["8-Table3-1.png", "9-Table7-1.png", "9-Table8-1.png", "7-Table2-1.png", "6-Table1-1.png", "7-Figure1-1.png", "9-Table5-1.png", "9-Table6-1.png"], "referred_figures_names": ["9-Table8-1.png"], "answer": "The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models"}, "1341": {"question": " What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'Table shows inference speed of different models'}\nThe answer is : The proposed method with an LSTM sequence modeling layer achieves an average inference speed of 73.72 sentences per second, while the state-of-the-art LR-CNN model achieves an average speed of 26.73 sentences per second. Thus, the improvement is 73.72-26.73 = 46.99 sentences per second. The percentage improvement is 46.99/26.73*100 = 176.11%.", "all_figures_names": ["8-Table4-1.png", "9-Table6-1.png", "7-Table2-1.png", "9-Table7-1.png", "8-Table3-1.png", "9-Table5-1.png", "6-Table1-1.png", "9-Table8-1.png"], "referred_figures_names": ["8-Table3-1.png"], "answer": "Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)"}}