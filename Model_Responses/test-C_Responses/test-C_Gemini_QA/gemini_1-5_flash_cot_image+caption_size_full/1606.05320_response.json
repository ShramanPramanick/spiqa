{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "referred_figures_indices": [0, 1], "response": "{'Image': 1, 'Rationale': 'It explains what features are learned by the HMM part'}\nThe answer is : The HMM model used here has 10 states and learns some complementary features in the text related to spaces and comments. It is not clearly stated how interpretable these features are.", "all_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png", "3-Table1-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "referred_figures_indices": [1, 3], "response": "{'Image': 2, 'Rationale': 'Image 2 provides a schematic comparison of how HMM and LSTM are used in a hybrid architecture'}\nThe answer is : The answer is that HMMs learn the state probabilities of a system based on the observed data. LSTMs, on the other hand, learn the temporal dependencies in the data. Therefore, HMMs provide a better way to model the state of a system, while LSTMs provide a better way to model the changes in the state of the system over time.", "all_figures_names": ["3-Table1-1.png", "4-Figure3-1.png", "2-Figure1-1.png", "4-Figure2-1.png"], "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Rationale': 'Contains a table comparing the LL scores of various models, including HMMs and LSTMs.'}\nThe answer is : The table does not include performance metrics to directly compare the gap between HMMs and LSTMs. It shows the LL score for various configurations and models, but it does not explicitly highlight a performance gap.  However, one could infer the gap from the LL score difference between the models.", "all_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png", "2-Figure1-1.png", "3-Table1-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}