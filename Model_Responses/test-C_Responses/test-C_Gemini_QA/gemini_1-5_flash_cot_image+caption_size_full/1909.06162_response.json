{"909": {"question": "What is best performing model among author's submissions, what performance it had?", "referred_figures_indices": [0], "response": "{'Image': 2, 'Rationale': 'The table shows F1 scores for different models and features'}\nThe answer is : The best performing model is r12 - BERT + Fine-tune (\u03c4 = 35) within Fold1 with F1 score of 650/647/654.", "all_figures_names": ["3-Table2-1.png", "3-Figure1-1.png", "4-Table3-1.png", "2-Table1-1.png", "4-Table4-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."}, "910": {"question": "What extracted features were most influencial on performance?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Rationale': 'This image shows the scores for different features, allowing for comparison'}\nThe answer is : The extracted features that were most influential on performance were the combined features of  BERT + Fine-tune (\u03c4 = 35), BERT + Fine-tune (\u03c4 = 50), and BERT + Fine-tune (\u03c4 = 40). This combination achieved the highest F1 score on the Dev (external) set.", "all_figures_names": ["4-Table3-1.png", "4-Table4-1.png", "2-Table1-1.png", "3-Figure1-1.png", "3-Table2-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "Linguistic"}, "912": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [4], "response": "{'Image': 2, 'Rationale': 'Shows the performance improvement with ensembles'}\nThe answer is : Yes, ensemble schemes helped in boosting performance by 11% on the dev (external) set.", "all_figures_names": ["2-Table1-1.png", "3-Table2-1.png", "4-Table4-1.png", "3-Figure1-1.png", "4-Table3-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "The best ensemble topped the best single model by 0.029 in F1 score on dev (external)."}, "913": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [1, 4, 3], "response": "{'Image': 3, 'Rationale': 'Image 3 shows scores on different ensembles.'}\nThe answer is : Ensemble schemes helped boost performance. In this study, the best ensemble scheme (ensemble of (II and IV) from each Fold1-3, i.e., |M| = 6) achieved F1 score of 0.164, which is higher than all other models, including individual models (II and IV) that make up the ensemble.", "all_figures_names": ["2-Table1-1.png", "3-Table2-1.png", "3-Figure1-1.png", "4-Table4-1.png", "4-Table3-1.png"], "referred_figures_names": ["3-Table2-1.png", "4-Table3-1.png", "4-Table4-1.png"], "answer": "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"}, "914": {"question": "Which basic neural architecture perform best by itself?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'Shows a table of different models and their F1 scores'}\nThe answer is : The best model is r28, BERT + Fine-tune (\u03c4 \u2265 35) + Ensemble + relax-voting  + postprocess, with an F1 score of .751.", "all_figures_names": ["3-Table2-1.png", "4-Table4-1.png", "2-Table1-1.png", "3-Figure1-1.png", "4-Table3-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "BERT"}, "915": {"question": "What participating systems had better results than ones authors submitted?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'This image shows a table that lists the top 5 participating systems, making it helpful to compare their scores'}\nThe answer is : The MIC-CIS system had a better F1 score than the other systems in the table.", "all_figures_names": ["4-Table3-1.png", "3-Table2-1.png", "4-Table4-1.png", "2-Table1-1.png", "3-Figure1-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."}, "916": {"question": "What is specific to multi-granularity and multi-tasking neural arhiteture design?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Rationale': 'The image shows the architecture of a multi-tasking neural network'}\nThe answer is : The specific features of multi-granularity and multi-tasking neural architecture design are the use of multiple layers, each performing a specific task, and the ability to jointly perform multiple tasks. This allows for the model to learn more complex relationships between data and to make more accurate predictions. For example, the FLC layer in the image jointly performs the PFD and FLC tasks.", "all_figures_names": ["4-Table3-1.png", "2-Table1-1.png", "3-Table2-1.png", "3-Figure1-1.png", "4-Table4-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "An output layer for each task"}}