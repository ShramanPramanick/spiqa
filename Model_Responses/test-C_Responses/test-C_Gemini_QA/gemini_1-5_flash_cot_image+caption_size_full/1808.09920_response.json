{"856": {"question": "What baseline did they compare Entity-GCN to?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Rationale': 'This image lists various models and their accuracy. Entity-GCN is compared to these models.'}\nThe answer is : The authors compare Entity-GCN to other models like Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver/Jenga, MHQA-GRN, and Entity-GCN without coreference.", "all_figures_names": ["7-Table3-1.png", "13-Figure3-1.png", "1-Figure1-1.png", "5-Table1-1.png", "11-Table5-1.png", "6-Table2-1.png", "8-Table4-1.png", "3-Figure2-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}, "858": {"question": "Did they use a relation extraction method to construct the edges in the graph?", "referred_figures_indices": [5], "response": "{'Image': 4, 'Rationale': 'This image shows the performance of the model with various components removed. It specifically says that the model performs worse when the R-GCN component is removed'}\nThe answer is : The answer is not clear from this image, though it does appear that a relation extraction method was used as the paper notes that the model performs worse without the relation extraction component (R-GCN). However, more information is required to know for certain.", "all_figures_names": ["6-Table2-1.png", "1-Figure1-1.png", "12-Table6-1.png", "11-Table5-1.png", "7-Table3-1.png", "3-Figure2-1.png", "8-Table4-1.png", "13-Figure3-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "No"}, "859": {"question": "How did they get relations between mentions?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'This image shows how the relations are represented in a graph, with different colors indicating different entities.'}\nThe answer is :  The relations between mentions are represented in a graph, where nodes represent mentions of candidate entities or query entities. Solid edges indicate co-occurrence in the same document, dashed edges indicate exact matches, and bold-red lines indicate coreference.", "all_figures_names": ["13-Figure3-1.png", "12-Table6-1.png", "3-Figure2-1.png", "8-Table4-1.png", "7-Table3-1.png", "1-Figure1-1.png", "11-Table5-1.png", "5-Table1-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."}, "862": {"question": "What performance does the Entity-GCN get on WIKIHOP?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'This image shows the performance of Entity-GCN compared to other models on the WIKIHOP dataset.'}\nThe answer is : The Entity-GCN model achieves an accuracy of 67.6 on the WIKIHOP closed test set and 64.8 on the public validation set.", "all_figures_names": ["8-Table4-1.png", "6-Table2-1.png", "11-Table5-1.png", "5-Table1-1.png", "1-Figure1-1.png", "12-Table6-1.png", "13-Figure3-1.png", "3-Figure2-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"}}