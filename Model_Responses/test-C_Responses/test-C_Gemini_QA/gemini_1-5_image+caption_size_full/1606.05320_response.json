{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "referred_figures_indices": [0, 1], "response": "{'Answer': 'The visualizations of the HMM states reveal that they learn features from the data like spaces and comments. These features are somewhat interpretable, giving insights into the kind of patterns captured by the HMM.'}", "all_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png", "3-Table1-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "referred_figures_indices": [2, 1], "response": "{'Answer': 'HMMs learn about spaces and comments in the code which the LSTMs don\\'t seem to focus on.'}", "all_figures_names": ["2-Figure1-1.png", "4-Figure2-1.png", "4-Figure3-1.png", "3-Table1-1.png"], "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "referred_figures_indices": [3], "response": "{'Answer': 'The provided context does not have information on the performance gap between HMMs and LSTMs, therefore, I cannot answer your question.'}\n", "all_figures_names": ["2-Figure1-1.png", "4-Figure3-1.png", "4-Figure2-1.png", "3-Table1-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}