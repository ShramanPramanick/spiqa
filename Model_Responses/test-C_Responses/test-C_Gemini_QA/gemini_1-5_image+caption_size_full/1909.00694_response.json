{"2": {"question": "What are the results?", "referred_figures_indices": [3, 4], "response": "{'Answer': 'The results are that the ACP dataset has the best performance with BERT of 0.933 and when trained only with AL data it is 0.886.'}", "all_figures_names": ["4-Table2-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table3-1.png", "5-Table4-1.png", "5-Table5-1.png"], "referred_figures_names": ["5-Table3-1.png", "5-Table4-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [1], "response": "{'Answer': 'Polarity in this method is propagated based on the discourse relations between the former and latter events.  The same polarity as the latter event is used if the discourse relation is \"CAUSE\" and the reversed polarity for the relation \"CONCESSION\".'}", "all_figures_names": ["4-Table2-1.png", "2-Figure1-1.png", "5-Table4-1.png", "5-Table5-1.png", "4-Table1-1.png", "5-Table3-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [3], "response": "{'Answer': 'Relations are used to propagate polarity by leveraging the relationship between two events, specifically cause and concession. In a cause relation, the polarity of the latter event influences the polarity of the former event. If the relation is concession, the polarity of the former event is the reverse of the latter event.'}", "all_figures_names": ["5-Table3-1.png", "4-Table2-1.png", "5-Table5-1.png", "2-Figure1-1.png", "5-Table4-1.png", "4-Table1-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [1, 2], "response": "{'Answer': 'The training set of the Japanese dataset ACP consists of 299,834 positive and 300,164 negative events.'}", "all_figures_names": ["5-Table5-1.png", "4-Table2-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table3-1.png", "5-Table4-1.png"], "referred_figures_names": ["4-Table2-1.png", "4-Table1-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [4], "response": "{'Answer': 'The Japanese dataset has 1,000,000 automatically labeled pairs.'}", "all_figures_names": ["4-Table1-1.png", "2-Figure1-1.png", "5-Table4-1.png", "5-Table5-1.png", "4-Table2-1.png", "5-Table3-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [2], "response": "{'Answer': 'When trained on a small labeled dataset (ACP (6K)), the proposed approach of enhancing it with AL (Automatically Labeled Pairs) shows an improvement of 1% (from 0.876 to 0.886) in accuracy when using BERT as the encoder. Using BiGRU as the encoder, adding AL+CA+CO to the ACP (6K) dataset increases the accuracy by 0.049 (from 0.830 to 0.879).'}\n", "all_figures_names": ["2-Figure1-1.png", "4-Table2-1.png", "5-Table4-1.png", "4-Table1-1.png", "5-Table5-1.png", "5-Table3-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [2], "response": "{'Answer': 'The model uses the seed lexicon in the automatically labelled pairs to learn the polarity of an event. This polarity is then propagated to other events in the same sentence depending on the discourse relation, thereby learning from mostly raw data.'}", "all_figures_names": ["4-Table2-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table3-1.png", "5-Table4-1.png", "5-Table5-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [1], "response": "{'Answer': \"The provided information does not specify the exact size of the seed lexicon used for training. Therefore, I am unable to answer your question.\"}", "all_figures_names": ["5-Table4-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table3-1.png", "5-Table5-1.png", "4-Table2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}