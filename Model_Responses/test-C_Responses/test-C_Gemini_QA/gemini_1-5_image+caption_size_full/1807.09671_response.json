{"962": {"question": "Do they build one model per topic or on all topics?", "referred_figures_indices": [3, 1], "response": "{'Answer': \"While the provided document doesn't explicitly state whether they build one model for all topics or one model per topic, we can infer they're evaluating model performance on various datasets covering different topics (e.g., student responses, reviews from domains like cameras, movies, peers). This suggests they likely trained a single model on a diverse dataset encompassing multiple topics, rather than building separate models for each.\"}", "all_figures_names": ["15-Table7-1.png", "20-Figure2-1.png", "17-Table9-1.png", "9-Table1-1.png", "11-Table3-1.png", "13-Table5-1.png", "10-Table2-1.png", "12-Table4-1.png"], "referred_figures_names": ["9-Table1-1.png", "20-Figure2-1.png"], "answer": "One model per topic."}, "963": {"question": "Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?", "referred_figures_indices": [5, 7], "response": "{'Answer': 'The text describes evaluation of low-rank approximation in the context of summarization, but it does not directly address quantitative or qualitative verification of lexical item grouping based on the approximation output.'}", "all_figures_names": ["16-Table8-1.png", "13-Table5-1.png", "23-Table13-1.png", "9-Table1-1.png", "27-Table15-1.png", "10-Table2-1.png", "22-Table11-1.png", "15-Table7-1.png"], "referred_figures_names": ["10-Table2-1.png", "15-Table7-1.png"], "answer": "They evaluate quantitatively."}}