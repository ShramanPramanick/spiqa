{"883": {"question": "Do the methods that work best on academic papers also work best on Wikipedia?", "referred_figures_indices": [1], "response": "{'Answer': 'The Inception model performs best on academic papers but the Joint model performs best on Wikipedia.'}", "all_figures_names": ["4-Table2-1.png", "4-Table1-1.png", "3-Figure2-1.png", "1-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "No"}, "884": {"question": "What is their system's absolute accuracy?", "referred_figures_indices": [0], "response": "{'Answer': 'The Joint model achieves an absolute accuracy of 59.4% on the Wikipedia dataset.'}", "all_figures_names": ["4-Table1-1.png", "4-Table2-1.png", "1-Figure1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"}, "887": {"question": "Which languages do they use?", "referred_figures_indices": [2], "response": "{'Answer': \"This document doesn't specify which languages the datasets are in, but they are about predicting the quality of Wikipedia articles.\"}", "all_figures_names": ["3-Figure2-1.png", "1-Figure1-1.png", "4-Table2-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "English"}, "888": {"question": "How large is their data set?", "referred_figures_indices": [2], "response": "{'Answer': 'The confusion matrix shown in the first image sums to 1001, so their Wikipedia dataset has 1001 articles.'}", "all_figures_names": ["4-Table2-1.png", "1-Figure1-1.png", "4-Table1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "a sample of  29,794 wikipedia articles and 2,794 arXiv papers "}}