{"48": {"question": "Do they report results only on English datasets?", "referred_figures_indices": [4, 6], "response": "{'Answer': 'The captions refer to Twitter, Chatbot NLU Corpus datasets, which are English datasets. Therefore, it can be concluded that they report results only on English datasets.'}", "all_figures_names": ["15-Figure3-1.png", "17-Figure4-1.png", "9-Table3-1.png", "8-Table1-1.png", "9-Table2-1.png", "11-Figure2-1.png", "10-Table4-1.png", "12-Table5-1.png"], "referred_figures_names": ["9-Table2-1.png", "10-Table4-1.png"], "answer": "Yes"}, "51": {"question": "Do they test their approach on a dataset without incomplete data?", "referred_figures_indices": [0], "response": "{'Answer': 'Yes, they test Stacked DeBERT on the Kaggle\u2019s Sentiment140 Corpus, which includes a corrected version of the dataset (without incomplete data) in addition to the original dataset with naturally incorrect tweets.'}", "all_figures_names": ["11-Figure2-1.png", "5-Figure1-1.png", "9-Table3-1.png", "14-Table6-1.png", "17-Figure5-1.png", "17-Figure4-1.png", "16-Table7-1.png", "8-Table1-1.png"], "referred_figures_names": ["11-Figure2-1.png"], "answer": "No"}, "52": {"question": "Do they test their approach on a dataset without incomplete data?", "referred_figures_indices": [3], "response": "{'Answer': 'Yes, they test their approach on the original version of the Kaggle Sentiment140 Corpus, which they label as \"(Inc)\" in Table 6, in addition to a corrected version of the dataset, labeled as \"(Corr)\".'}", "all_figures_names": ["9-Table3-1.png", "11-Figure2-1.png", "14-Table6-1.png", "8-Table1-1.png", "12-Table5-1.png", "9-Table2-1.png", "16-Table7-1.png", "5-Figure1-1.png"], "referred_figures_names": ["8-Table1-1.png"], "answer": "No"}, "53": {"question": "Should their approach be applied only when dealing with incomplete data?", "referred_figures_indices": [6], "response": "{'Answer': 'No, their approach achieves high F1-scores on both incomplete and complete data, as shown in Tables 6 and 7.'}", "all_figures_names": ["16-Table7-1.png", "11-Figure2-1.png", "9-Table2-1.png", "10-Table4-1.png", "12-Table5-1.png", "8-Table1-1.png", "14-Table6-1.png", "9-Table3-1.png"], "referred_figures_names": ["14-Table6-1.png"], "answer": "No"}, "54": {"question": "Should their approach be applied only when dealing with incomplete data?", "referred_figures_indices": [2], "response": "{'Answer': 'Although their approach shows promising results with incomplete data, it is not limited to such cases and can be applied to complete data as well.'}", "all_figures_names": ["8-Table1-1.png", "9-Table2-1.png", "5-Figure1-1.png", "11-Figure2-1.png", "12-Table5-1.png", "9-Table3-1.png", "16-Table7-1.png", "17-Figure5-1.png"], "referred_figures_names": ["5-Figure1-1.png"], "answer": "No"}, "55": {"question": "By how much do they outperform other models in the sentiment in intent classification tasks?", "referred_figures_indices": [5, 6], "response": "{'Answer': 'In the intent classification task, the Stacked DeBERT model achieves an F1-score of 99.06 in the complete data, 97.17 in the gtts-witai data, and 96.23 in the macsay-witai data, outperforming BERT by 0.95%, 4.94% and 2.89% respectively. In the sentiment classification task, the model reaches an F1-score of 80.00, compared to BERT\\'s 74.00, a 6% improvement over BERT.'}", "all_figures_names": ["5-Figure1-1.png", "17-Figure5-1.png", "17-Figure4-1.png", "12-Table5-1.png", "15-Figure3-1.png", "16-Table7-1.png", "14-Table6-1.png", "10-Table4-1.png"], "referred_figures_names": ["16-Table7-1.png", "14-Table6-1.png"], "answer": "In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average"}}