{"654": {"question": "How they extract \"structured answer-relevant relation\"?", "referred_figures_indices": [0], "response": "{'Answer': 'The model extracts \"structured answer-relevant relation\" by identifying the most relevant part of the sentence based on the question. For example, if the question is \"What is New York City's daily January mean temperature in degrees Celsius?\", the model would extract \"(The daily mean temperature in January; is; 32.6 \u25e6F (0.3 \u25e6C))\" as the answer-relevant relation. This approach helps the model to focus on the most important information in the sentence and generate more accurate and relevant questions.'}", "all_figures_names": ["1-Figure1-1.png", "3-Table2-1.png", "5-Table3-1.png", "4-Figure3-1.png", "7-Figure4-1.png", "7-Table5-1.png", "8-Figure5-1.png", "3-Figure2-1.png"], "referred_figures_names": ["1-Figure1-1.png"], "answer": "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."}, "656": {"question": "How big are significant improvements?", "referred_figures_indices": [1], "response": "{'Answer': 'Significant improvements are indicated by increases in scores in Table 4, where our model has higher scores than the baselines. ' }", "all_figures_names": ["1-Figure1-1.png", "6-Table4-1.png", "4-Figure3-1.png", "8-Figure5-1.png", "7-Figure4-1.png", "5-Table3-1.png", "3-Figure2-1.png", "3-Table2-1.png"], "referred_figures_names": ["6-Table4-1.png"], "answer": "Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"}}