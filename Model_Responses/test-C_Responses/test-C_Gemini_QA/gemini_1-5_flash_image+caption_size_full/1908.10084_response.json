{"434": {"question": "What other sentence embeddings methods are evaluated?", "referred_figures_indices": [4, 5], "response": "{'Answer': 'The table shows the evaluation of different sentence embedding methods including InferSent-GloVe, Universal Sentence Encoder, SBERT-NLI-base and SBERT-NLI-large.'}", "all_figures_names": ["7-Table6-1.png", "8-Table7-1.png", "7-Table5-1.png", "3-Figure1-1.png", "4-Table1-1.png", "6-Table3-1.png", "6-Table4-1.png", "5-Table2-1.png"], "referred_figures_names": ["4-Table1-1.png", "6-Table3-1.png"], "answer": "GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent"}, "435": {"question": "What other sentence embeddings methods are evaluated?", "referred_figures_indices": [2], "response": "{'Answer': 'The paper also evaluates the performance of InferSent-GloVe, Universal Sentence Encoder, and several BERT models (BERT-NLI-base, BERT-NLI-large, RoBERTa-NLI-base, RoBERTa-NLI-large) on the STS benchmark dataset.'}", "all_figures_names": ["6-Table4-1.png", "7-Table6-1.png", "7-Table5-1.png", "3-Figure1-1.png", "4-Table1-1.png", "3-Figure2-1.png", "5-Table2-1.png", "6-Table3-1.png"], "referred_figures_names": ["7-Table5-1.png"], "answer": "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."}}