{"2": {"question": "What are the results?", "referred_figures_indices": [0, 4], "response": "{'Answer': 'The results show that BERT is better than BiGRU in all cases, achieving the highest accuracy of 0.933 on the ACP test set.'}", "all_figures_names": ["5-Table4-1.png", "5-Table5-1.png", "2-Figure1-1.png", "4-Table1-1.png", "5-Table3-1.png", "4-Table2-1.png"], "referred_figures_names": ["5-Table4-1.png", "5-Table3-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [1], "response": "{'Answer': 'Relations are used to propagate polarity through a process called \"propagation.\" In this process, the polarity of a latter event, which is determined based on a seed lexicon, is propagated to the former event. The polarity propagation depends on the type of discourse relation between the two events: CAUSE and CONCESSION. In CAUSE relations, the same polarity as the latter event is used, while in CONCESSION relations, the reversed polarity is used.'}", "all_figures_names": ["4-Table2-1.png", "2-Figure1-1.png", "5-Table5-1.png", "4-Table1-1.png", "5-Table4-1.png", "5-Table3-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [3], "response": "{'Answer': 'Relations, such as CAUSE and CONCESSION, are used to propagate polarity by either encouraging the events to have the same polarity (CAUSE) or encouraging them to have the reversed polarity (CONCESSION).'}", "all_figures_names": ["4-Table2-1.png", "5-Table3-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table4-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [3, 5], "response": "{'Answer': 'The Japanese data is 1,000,000 Automatically Labeled Pairs (AL), 5,000,000 CAUSE Pairs (CA) and 5,000,000 CONCESSION Pairs (CO).'}", "all_figures_names": ["5-Table3-1.png", "2-Figure1-1.png", "5-Table5-1.png", "4-Table1-1.png", "5-Table4-1.png", "4-Table2-1.png"], "referred_figures_names": ["4-Table1-1.png", "4-Table2-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [4], "response": "{'Answer': 'The Japanese data is composed of 1,000,000 AL (Automatically Labeled Pairs), 5,000,000 CA (CAUSE Pairs), and 5,000,000 CO (CONCESSION Pairs).'}", "all_figures_names": ["4-Table1-1.png", "5-Table4-1.png", "5-Table5-1.png", "5-Table3-1.png", "4-Table2-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [4], "response": "{'Answer': 'The proposed approach with the proposed dataset leads to an improvement of about 0.01 accuracy compared to basic approach in the case of BiGRU encoder.'}", "all_figures_names": ["4-Table1-1.png", "5-Table5-1.png", "2-Figure1-1.png", "5-Table3-1.png", "5-Table4-1.png", "4-Table2-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [5], "response": "{'Answer': 'The model learns by using raw data to identify pairs of events and their relationships, such as cause and concession, using a seed lexicon. The polarity of the latter event is propagated to the former event, where the same polarity is used for the cause relation and the reversed polarity for the concession relation. This allows the model to learn the relationship between events and their polarities without the need for manual labeling. Additionally, the model encourages the two events to have the same polarity in cause relations and reversed polarities in concession relations. This helps the model learn the nuances of these relationships.'}", "all_figures_names": ["5-Table5-1.png", "5-Table4-1.png", "4-Table2-1.png", "4-Table1-1.png", "5-Table3-1.png", "2-Figure1-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [2], "response": "{'Answer': 'The size of seed lexicon is not mentioned in the provided context.'}", "all_figures_names": ["2-Figure1-1.png", "4-Table2-1.png", "4-Table1-1.png", "5-Table3-1.png", "5-Table4-1.png", "5-Table5-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}