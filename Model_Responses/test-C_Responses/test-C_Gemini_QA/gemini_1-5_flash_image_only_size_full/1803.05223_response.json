{"903": {"question": "what dataset statistics are provided?", "referred_figures_indices": [0], "response": "{'Answer': 'The provided dataset statistics are:\\n\\n- The number of answerable text-based and script-based questions.\\n- The number of unanswerable questions based on reasons like \"unknown\" or \"unfitting\".\\n- The performance of different models in answering the question (including chance, word overlap, sliding window, bilinear model, attentive reader, and human performance). \\n- The performance of different models for different question types in the dataset (including \"yes/no\", \"what\", \"why\", \"who\", \"where\", \"when\", \"how\", \"how long\", and \"how many/much\").\\n- The percentage of each question type in the dataset (including \"y/n\", \"what/which\", \"who/whose\", \"why\", \"where\", \"when\", \"how\", \"how long/often\", \"how many/much\", and \"rest\").'}", "all_figures_names": ["3-Table1-1.png", "6-Table2-1.png", "6-Figure4-1.png", "1-Figure1-1.png", "4-Figure3-1.png", "4-Figure2-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%)."}, "904": {"question": "what dataset statistics are provided?", "referred_figures_indices": [1], "response": "{'Answer': 'The dataset statistics are provided in the number of instances for each category, text-based, script-based, unknown, unfit, and total. The statistics include the number of instances for each category, and the total number of instances for each category.'}", "all_figures_names": ["1-Figure1-1.png", "3-Table1-1.png", "4-Figure3-1.png", "4-Figure2-1.png", "6-Table2-1.png", "6-Figure4-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text"}}