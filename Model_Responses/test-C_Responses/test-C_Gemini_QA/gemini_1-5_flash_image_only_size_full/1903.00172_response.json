{"1026": {"question": "Where did they get training data?", "referred_figures_indices": [1, 2, 4], "response": "{'Answer': 'The training data was taken from two datasets: ConciergeQA and AmazonQA. ConciergeQA had 33k questions and answers and AmazonQA had 314k.'}", "all_figures_names": ["3-Figure1-1.png", "7-Table4-1.png", "6-Table3-1.png", "8-Figure4-1.png", "5-Table1-1.png", "4-Figure2-1.png", "13-Figure5-1.png", "7-Figure3-1.png"], "referred_figures_names": ["7-Table4-1.png", "6-Table3-1.png", "5-Table1-1.png"], "answer": "AmazonQA and ConciergeQA datasets"}, "1027": {"question": "What extraction model did they use?", "referred_figures_indices": [4], "response": "{'Answer': 'The extraction model is called NEURON. It uses a multi-encoder constrained decoder model for tuple extraction from (q, a).'}", "all_figures_names": ["13-Figure5-1.png", "8-Figure4-1.png", "8-Table6-1.png", "6-Table3-1.png", "3-Figure1-1.png", "7-Figure3-1.png", "7-Table4-1.png", "5-Table1-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "Multi-Encoder, Constrained-Decoder model"}, "1028": {"question": "Which datasets did they experiment on?", "referred_figures_indices": [6], "response": "{'Answer': 'The datasets that were experimented on were ConciergeQA and AmazonQA'}", "all_figures_names": ["6-Table3-1.png", "4-Figure2-1.png", "8-Table6-1.png", "3-Figure1-1.png", "7-Table4-1.png", "8-Figure4-1.png", "5-Table1-1.png", "13-Figure5-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "ConciergeQA and AmazonQA"}}