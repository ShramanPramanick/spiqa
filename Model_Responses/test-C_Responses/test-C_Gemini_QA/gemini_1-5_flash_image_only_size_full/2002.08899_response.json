{"874": {"question": "Do they perform a quantitative analysis of their model displaying knowledge distortions?", "referred_figures_indices": [2, 0], "response": "{'Answer': 'The paper does not perform any quantitative analysis of its model displaying knowledge distortions. The paper focuses on analyzing the impact of adversarial attacks on the performance of neural machine translation systems.'}", "all_figures_names": ["5-Figure2-1.png", "4-Table1-1.png", "4-Table2-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table2-1.png", "5-Figure2-1.png"], "answer": "Yes"}, "875": {"question": "How do they damage different neural modules?", "referred_figures_indices": [2], "response": "{'Answer': 'The images show the effect of adversarial attacks on different neural modules. The first image shows the effect of adversarial attacks on the LSTM module, the second image shows the effect of adversarial attacks on the Lexicon Unit module, and the third image shows the effect of adversarial attacks on the LSTM module and the Lexicon Unit module. The attacks cause significant damage to the performance of the neural modules, especially on the tasks related to colors and the GEO task.'}", "all_figures_names": ["4-Table1-1.png", "2-Figure1-1.png", "4-Table2-1.png", "5-Figure2-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information."}}