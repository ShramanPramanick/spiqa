{"457": {"question": "What do they mean by answer styles?", "referred_figures_indices": [7], "response": "{'Answer': 'Answer styles refer to different ways of responding to a question. For example, an answer can be a simple yes/no, a short phrase, or a more detailed explanation. The paper explores the impact of learning to generate different answer styles in a machine reading comprehension system.'}", "all_figures_names": ["4-Figure3-1.png", "7-Figure5-1.png", "6-Table2-1.png", "2-Figure2-1.png", "8-Table5-1.png", "1-Figure1-1.png", "6-Table4-1.png", "5-Table1-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "referred_figures_indices": [3], "response": "```json\n{'Answer': 'No, there are two answer styles, one for NLG and one for Q\\&A.'}\n```", "all_figures_names": ["6-Table3-1.png", "4-Figure3-1.png", "7-Figure4-1.png", "5-Table1-1.png", "7-Figure5-1.png", "2-Figure2-1.png", "6-Table2-1.png", "1-Figure1-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "referred_figures_indices": [2, 0], "response": "{'Answer': 'The paper compares Masque to other models including BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, and Human Performance.'}", "all_figures_names": ["6-Table2-1.png", "6-Table3-1.png", "8-Table5-1.png", "7-Figure4-1.png", "1-Figure1-1.png", "4-Figure3-1.png", "7-Figure5-1.png", "6-Table4-1.png"], "referred_figures_names": ["8-Table5-1.png", "6-Table2-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "referred_figures_indices": [3], "response": "```json\n{'Answer': 'The model achieved 69.77 and 65.56 on R-L and B-1 scores, respectively.'}\n```", "all_figures_names": ["4-Figure3-1.png", "7-Figure5-1.png", "6-Table2-1.png", "8-Table5-1.png", "5-Table1-1.png", "1-Figure1-1.png", "6-Table3-1.png", "6-Table4-1.png"], "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "referred_figures_indices": [2], "response": "{'Answer': 'The answer style is the way in which an answer is presented, for example, whether it is a short answer, a long answer, or a yes/no answer. The table in Image 1 shows that the model achieves the best performance when trained on all data (ALL) and when using a multi-style learning approach.'}", "all_figures_names": ["7-Figure5-1.png", "6-Table4-1.png", "5-Table1-1.png", "2-Figure2-1.png", "8-Table5-1.png", "6-Table2-1.png", "7-Figure4-1.png", "4-Figure3-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}