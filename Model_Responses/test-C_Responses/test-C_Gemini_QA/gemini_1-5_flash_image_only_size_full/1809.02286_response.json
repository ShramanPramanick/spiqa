{"204": {"question": "Which baselines did they compare against?", "referred_figures_indices": [0, 3], "response": "{'Answer': 'They compared against a fully-connected neural network, a Bi-LSTM, and a model with and without POS tags.'}", "all_figures_names": ["6-Table1-1.png", "7-Figure3-1.png", "7-Figure4-1.png", "6-Table2-1.png", "3-Figure1-1.png", "4-Figure2-1.png"], "referred_figures_names": ["6-Table1-1.png", "6-Table2-1.png"], "answer": "Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks"}, "205": {"question": "Which baselines did they compare against?", "referred_figures_indices": [1, 2], "response": "{'Answer': 'The paper compares their proposed SATALSTM model to baselines like Bi-LSTM, Tree-structured models like 100D Latent Syntax Tree-LSTM, 300D Tree-based CNN, 300D SPIN-PI, 300D Gumbel Tree-LSTM, other neural models like 300D NSE, 300D Reinforced Self-Attention Network, 600D Residual stacked encoders, 600D BiLSTM with generalized pooling, and FC.'}", "all_figures_names": ["7-Figure4-1.png", "6-Table1-1.png", "6-Table2-1.png", "7-Figure3-1.png", "4-Figure2-1.png", "3-Figure1-1.png"], "referred_figures_names": ["6-Table1-1.png", "6-Table2-1.png"], "answer": "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."}}