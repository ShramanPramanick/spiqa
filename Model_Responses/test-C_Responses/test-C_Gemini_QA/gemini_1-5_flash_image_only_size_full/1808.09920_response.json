{"856": {"question": "What baseline did they compare Entity-GCN to?", "referred_figures_indices": [4], "response": "{'Answer': 'Entity-GCN was compared to the baseline model \"GloVe with R-GCN\".'}", "all_figures_names": ["3-Figure2-1.png", "1-Figure1-1.png", "8-Table4-1.png", "11-Table5-1.png", "6-Table2-1.png", "7-Table3-1.png", "5-Table1-1.png", "13-Figure3-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}, "858": {"question": "Did they use a relation extraction method to construct the edges in the graph?", "referred_figures_indices": [6], "response": "{'Answer': 'The images do not provide sufficient information to answer this question. While they mention relation extraction and graphs, they do not explicitly state whether a relation extraction method was used to construct the edges in the graph.'}", "all_figures_names": ["8-Table4-1.png", "1-Figure1-1.png", "13-Figure3-1.png", "7-Table3-1.png", "6-Table2-1.png", "5-Table1-1.png", "3-Figure2-1.png", "12-Table6-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "No"}, "859": {"question": "How did they get relations between mentions?", "referred_figures_indices": [3], "response": "{'Answer': 'The relation between mentions is calculated by applying the Entity-GCN model, which is a graph convolutional network. The model uses the input text and the coreference resolution of the text to predict the relations.  The model is trained on the WikiData dataset. The model is able to identify the relations with an accuracy of 68.5%.  The best 3 performing relations are member.of.political.party, record.label, and publisher, while the worst performing 3 relations are place.of.birth, place.of.death, and inception. '}", "all_figures_names": ["13-Figure3-1.png", "5-Table1-1.png", "12-Table6-1.png", "3-Figure2-1.png", "6-Table2-1.png", "8-Table4-1.png", "7-Table3-1.png", "1-Figure1-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."}, "862": {"question": "What performance does the Entity-GCN get on WIKIHOP?", "referred_figures_indices": [1], "response": "{'Answer': 'The Entity-GCN model achieves an accuracy of 68.5% on the WIKIHOP dataset when using an ensemble of 5 models, and 65.3% when using a single model.'}", "all_figures_names": ["12-Table6-1.png", "6-Table2-1.png", "3-Figure2-1.png", "11-Table5-1.png", "1-Figure1-1.png", "8-Table4-1.png", "13-Figure3-1.png", "7-Table3-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"}}