{"31": {"question": "What experiments are proposed to test that upper layers produce context-specific embeddings?", "referred_figures_indices": [0, 4, 3], "response": "{'Answer': 'The paper proposes to test context-specificity of upper layers by evaluating the similarity of words based on their embeddings. This is done by calculating the average cosine similarity between randomly sampled words from the embeddings generated by the different layers of the models (ELMo, BERT, and GPT2). The results of this experiment are shown in Image 3, where the average cosine similarity between randomly sampled words is plotted for each layer of the model. It can be seen that the average cosine similarity between words increases as the model goes up in layers, suggesting that the upper layers are more context-specific than the lower layers. Additionally, Image 2, comparing different pre-trained models and layers on different benchmark datasets, shows that upper layers generally perform better on various tasks. These results support the claim that upper layers are more context-specific. '}", "all_figures_names": ["6-Figure2-1.png", "8-Figure4-1.png", "8-Table1-1.png", "5-Figure1-1.png", "7-Figure3-1.png"], "referred_figures_names": ["6-Figure2-1.png", "7-Figure3-1.png", "5-Figure1-1.png"], "answer": "They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  \nThey plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.\nThey plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another."}, "32": {"question": "How do they calculate a static embedding for each word?", "referred_figures_indices": [2], "response": "{'Answer': 'The table in Image 2 shows the static embedding for each word. The static embedding is calculated based on the word embedding of the last layer of the language model, for example, the last layer of BERT (BERT, Layer 12).'}", "all_figures_names": ["6-Figure2-1.png", "5-Figure1-1.png", "8-Table1-1.png", "8-Figure4-1.png", "7-Figure3-1.png"], "referred_figures_names": ["8-Table1-1.png"], "answer": "They use the first principal component of a word's contextualized representation in a given layer as its static embedding."}}