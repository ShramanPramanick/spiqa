{"684": {"question": "What are the datasets used for the task?", "referred_figures_indices": [1], "response": "{'Answer': 'The datasets used in the task are CELEX, Festival, OpenLexique, IIT-Guwahati, and E-Hitz.'}", "all_figures_names": ["6-TableIV-1.png", "5-TableI-1.png", "3-Figure2-1.png", "2-Figure1-1.png", "5-TableII-1.png", "6-TableIII-1.png", "7-TableV-1.png"], "referred_figures_names": ["5-TableI-1.png"], "answer": "Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)"}, "685": {"question": "What is the accuracy of the model for the six languages tested?", "referred_figures_indices": [3], "response": "```json\n{'Answer': 'The word accuracy for the six languages tested, English CELEX, Dutch CELEX, Festival, OpenLexique, IIT-Guwahati and E-Hitz, are 98.5\u00b10.1, 99.47\u00b10.04, 99.990\u00b10.005, 99.96\u00b10.01, 94.9\u00b10.3, and 99.89\u00b10.07, respectively.'}\n```", "all_figures_names": ["5-TableI-1.png", "3-Figure2-1.png", "6-TableIV-1.png", "6-TableIII-1.png", "7-TableV-1.png", "2-Figure1-1.png", "5-TableII-1.png"], "referred_figures_names": ["6-TableIII-1.png"], "answer": "Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)"}, "686": {"question": "Which models achieve state-of-the-art performances?", "referred_figures_indices": [3], "response": "{'Answer': 'The BiLSTM-CNN-CRF (Base) model achieves state-of-the-art performances.'}", "all_figures_names": ["6-TableIII-1.png", "3-Figure2-1.png", "2-Figure1-1.png", "5-TableII-1.png", "6-TableIV-1.png", "5-TableI-1.png", "7-TableV-1.png"], "referred_figures_names": ["5-TableII-1.png"], "answer": "CELEX (Dutch and English) - SVM-HMM\nFestival, E-Hitz and OpenLexique - Liang hyphenation\nIIT-Guwahat - Entropy CRF"}}