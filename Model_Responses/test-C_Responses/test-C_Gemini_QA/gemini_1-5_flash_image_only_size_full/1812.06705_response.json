{"247": {"question": "How do the authors measure performance?", "referred_figures_indices": [2], "response": "{'Answer': 'The authors measure performance using the accuracy of the model on several benchmark datasets. The datasets include SST5, SST2, Subj, MPQA, RT, and TREC.  The authors report the accuracy scores in Table 2 in the paper.'}", "all_figures_names": ["5-Figure1-1.png", "8-Table4-1.png", "7-Table2-1.png", "5-Table1-1.png", "7-Table3-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "Accuracy across six datasets"}, "248": {"question": "Does the new objective perform better than the original objective bert is trained on?", "referred_figures_indices": [1], "response": "{'Answer': 'The new objective with C-BERT outperforms the original BERT objective on all six benchmarks.'}", "all_figures_names": ["7-Table3-1.png", "7-Table2-1.png", "5-Table1-1.png", "5-Figure1-1.png", "8-Table4-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}, "249": {"question": "Are other pretrained language models also evaluated for contextual augmentation? ", "referred_figures_indices": [2], "response": "{'Answer': 'The images provided do not provide information on whether other pretrained language models have been evaluated for contextual augmentation.'}", "all_figures_names": ["7-Table3-1.png", "5-Figure1-1.png", "7-Table2-1.png", "5-Table1-1.png", "8-Table4-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "No"}, "250": {"question": "Do the authors report performance of conditional bert on tasks without data augmentation?", "referred_figures_indices": [4], "response": "{'Answer': 'The authors reported performance on tasks without data augmentation in Table 4.'}", "all_figures_names": ["7-Table3-1.png", "8-Table4-1.png", "5-Table1-1.png", "5-Figure1-1.png", "7-Table2-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}}