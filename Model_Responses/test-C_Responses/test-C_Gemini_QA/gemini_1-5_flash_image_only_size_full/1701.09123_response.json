{"951": {"question": "what are the evaluation metrics?", "referred_figures_indices": [3], "response": "{'Answer': 'The evaluation metrics used are Precision (P), Recall (R) and F1 score.'}", "all_figures_names": ["17-Table10-1.png", "18-Table12-1.png", "6-Table2-1.png", "15-Table5-1.png", "20-Table16-1.png", "15-Table6-1.png", "11-Figure1-1.png", "16-Table8-1.png"], "referred_figures_names": ["15-Table5-1.png"], "answer": "Precision, Recall, F1"}, "952": {"question": "which datasets were used in evaluation?", "referred_figures_indices": [1], "response": "{'Answer': 'The evaluation was done using the following datasets: en-91-18-conll03 (CoNLL 2003), Illinois NER, Stanford NER, Freeling 3.1, Sonar nerd, and MEAN TIME.'}", "all_figures_names": ["16-Table8-1.png", "5-Table1-1.png", "22-Table19-1.png", "6-Table2-1.png", "18-Table13-1.png", "11-Figure1-1.png", "21-Table17-1.png", "22-Table18-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0"}}