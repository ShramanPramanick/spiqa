{"909": {"question": "What is best performing model among author's submissions, what performance it had?", "referred_figures_indices": [4], "response": "{'Answer': 'The best performing model is r28 with F1 score of 0.676, precision score of 0.615, and recall score of 0.751.'}", "all_figures_names": ["4-Table4-1.png", "2-Table1-1.png", "4-Table3-1.png", "3-Figure1-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."}, "910": {"question": "What extracted features were most influencial on performance?", "referred_figures_indices": [0], "response": "{'Answer': 'BERT (BERTWordEmb, BERTSentEmb, and BERTFinetune) is the most influential feature. It produces the highest F1 score in Image 0.  FastTextWordEmb is also notable. Combining BERT with FastTextWordEmb increases the F1 score in most cases. Overall, BERT is the most influential feature for the given task.'}", "all_figures_names": ["4-Table3-1.png", "2-Table1-1.png", "3-Table2-1.png", "3-Figure1-1.png", "4-Table4-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "Linguistic"}, "912": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [3], "response": "{'Answer': 'Ensemble schemes helped in boosting performance on the SLC task. For example, ensemble of r3, r6, r12 models within fold1 increased F1 score by 5 points. For the FLC task, ensemble of II and IV models within each fold gave a performance increase of 8 points in F1 score.'}", "all_figures_names": ["3-Table2-1.png", "2-Table1-1.png", "3-Figure1-1.png", "4-Table3-1.png", "4-Table4-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "The best ensemble topped the best single model by 0.029 in F1 score on dev (external)."}, "913": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [1, 4, 3], "response": "{'Answer': 'Yes, ensemble schemes helped in boosting performance. For example, in the \"Ensemble + of (r3, r6, r12) within Fold1\" experiment in Image 3, the F1 score was 665. In the \"Ensemble + of (r3, r6, r12) from each Fold1-3, i.e., |M| = 3\" experiment, the F1 score increased to 676.  In the \"Ensemble + of (II and IV) within Fold1\" experiment in Image 4, the F1 score was 116. In the \"Ensemble + of (II and IV) from each Fold1-3, i.e., |M| = 6\" experiment, the F1 score increased to 164. Therefore, the ensemble schemes helped to improve the performance by a significant amount.'}", "all_figures_names": ["3-Figure1-1.png", "3-Table2-1.png", "2-Table1-1.png", "4-Table3-1.png", "4-Table4-1.png"], "referred_figures_names": ["3-Table2-1.png", "4-Table4-1.png", "4-Table3-1.png"], "answer": "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"}, "914": {"question": "Which basic neural architecture perform best by itself?", "referred_figures_indices": [3], "response": "{'Answer': 'From Image 1, we can see that the neural architecture is **BERT** with **Fine-tuning**.'}", "all_figures_names": ["4-Table4-1.png", "3-Figure1-1.png", "2-Table1-1.png", "4-Table3-1.png", "3-Table2-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "BERT"}, "915": {"question": "What participating systems had better results than ones authors submitted?", "referred_figures_indices": [4], "response": "{'Answer': 'The system \\'ProperGander\\' under the \\'SLC\\' task achieved a better result than the authors submitted with a precision of 7009.'}", "all_figures_names": ["3-Figure1-1.png", "4-Table3-1.png", "4-Table4-1.png", "2-Table1-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."}, "916": {"question": "What is specific to multi-granularity and multi-tasking neural arhiteture design?", "referred_figures_indices": [3], "response": "```json\n{'Answer': 'Multi-granularity neural architecture design focuses on utilizing features at different levels, such as word embeddings, sentence embeddings, and document-level information.  Multi-tasking architectures involve training a single model to perform multiple tasks simultaneously. This approach enables the model to learn complementary information from different tasks and improve performance on each individual task.'}\n```", "all_figures_names": ["4-Table4-1.png", "3-Table2-1.png", "4-Table3-1.png", "3-Figure1-1.png", "2-Table1-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "An output layer for each task"}}