{"457": {"question": "What do they mean by answer styles?", "referred_figures_indices": [0], "response": "{'Answer': 'The answer styles refer to the different ways the answers are generated based on the questions.  In this context, there are two main answer styles: NLG (natural language generation), which generates human-like answers, and Q&A, which provides concise answers, often extracting them directly from the provided text.'}", "all_figures_names": ["5-Table1-1.png", "1-Figure1-1.png", "6-Table4-1.png", "6-Table3-1.png", "7-Figure4-1.png", "2-Figure2-1.png", "8-Table5-1.png", "7-Figure5-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "referred_figures_indices": [2], "response": "{'Answer': 'No. Some datasets have multiple answer styles.'}", "all_figures_names": ["6-Table4-1.png", "7-Figure4-1.png", "5-Table1-1.png", "6-Table3-1.png", "7-Figure5-1.png", "4-Figure3-1.png", "1-Figure1-1.png", "6-Table2-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "referred_figures_indices": [1, 7], "response": "{'Answer': 'The baselines that Masque is compared against include BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet and RMR+A2D.'}", "all_figures_names": ["4-Figure3-1.png", "6-Table2-1.png", "2-Figure2-1.png", "7-Figure5-1.png", "6-Table4-1.png", "7-Figure4-1.png", "1-Figure1-1.png", "8-Table5-1.png"], "referred_figures_names": ["6-Table2-1.png", "8-Table5-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "referred_figures_indices": [3], "response": "{'Answer': 'This model achieves 59.87 R-L, 26.13 M, 30.43 B-4, and 54.11 B-1 on NarrativeQA.'}", "all_figures_names": ["7-Figure5-1.png", "1-Figure1-1.png", "2-Figure2-1.png", "8-Table5-1.png", "4-Figure3-1.png", "6-Table2-1.png", "6-Table4-1.png", "5-Table1-1.png"], "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "referred_figures_indices": [7], "response": "{'Answer': 'The way the answer is formulated or generated, such as whether it is copied directly from the source, generated from vocabulary, or combines elements from both.'}", "all_figures_names": ["1-Figure1-1.png", "7-Figure4-1.png", "8-Table5-1.png", "4-Figure3-1.png", "6-Table4-1.png", "6-Table3-1.png", "7-Figure5-1.png", "5-Table1-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}