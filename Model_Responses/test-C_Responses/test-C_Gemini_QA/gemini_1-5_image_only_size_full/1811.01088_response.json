{"753": {"question": "Is the new model evaluated on the tasks that BERT and ELMo are evaluated on?", "referred_figures_indices": [0], "response": "{'Answer': 'Yes, the tables list the performance of the new model on tasks like CoLA, SST, MRPC, QQP, STS, MNLI, QNLI, RTE, and WNLI, which are standard tasks that BERT and ELMo are evaluated on.'}", "all_figures_names": ["4-Table1-1.png", "8-Table3-1.png", "12-Table4-1.png", "5-Table2-1.png", "6-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "Yes"}, "754": {"question": "Does the additional training on supervised tasks hurt performance in some tasks?", "referred_figures_indices": [4], "response": "{'Answer': 'Yes, in some cases, additional training on supervised tasks can hurt performance. For instance, in Image 3, directly training GPT on RTE leads to 54.2% accuracy. Training first on MNLI then RTE increases accuracy to 70.4%. However, further training on RTE or a combination of MNLI and RTE leads to a decrease in accuracy, indicating potential overfitting to the MNLI task.'}\n", "all_figures_names": ["6-Figure1-1.png", "12-Table4-1.png", "5-Table2-1.png", "8-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "Yes"}}