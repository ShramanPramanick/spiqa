{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "referred_figures_indices": [0, 2], "response": "{'Answer': 'The figure depicts the use of discretized and continuous features by HMM models.  Discretized features, while somewhat interpretable,  lack the nuance of continuous features. Continuous features, on the other hand, offer limited interpretability.'}", "all_figures_names": ["4-Figure2-1.png", "3-Table1-1.png", "4-Figure3-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "referred_figures_indices": [2, 3], "response": "{'Answer': 'The images don\\'t contain information to answer the question. The images show architectures, code, and tables with experimental results. However, there is no information about the specific knowledge that HMMs learn compared to LSTMs.'}", "all_figures_names": ["3-Table1-1.png", "2-Figure1-1.png", "4-Figure2-1.png", "4-Figure3-1.png"], "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "referred_figures_indices": [3], "response": "{'Answer': 'For the Linux text data, the difference in LL between the LSTM and HMM is between 0.49 and 0.09, while for the Shakespeare text data the difference is between 0.38 and 0.04.'}", "all_figures_names": ["2-Figure1-1.png", "4-Figure3-1.png", "4-Figure2-1.png", "3-Table1-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}