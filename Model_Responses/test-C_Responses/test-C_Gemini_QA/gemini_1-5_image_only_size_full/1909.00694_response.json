{"2": {"question": "What are the results?", "referred_figures_indices": [4, 1], "response": "{'Answer': 'The results are that the BERT encoder with ACP training data shows the highest accuracy of 0.933. This was achieved with automatically labelled pairs (AL), cause pairs (CA) and concession pairs (CO) training data.'}", "all_figures_names": ["2-Figure1-1.png", "5-Table4-1.png", "5-Table5-1.png", "4-Table2-1.png", "5-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["5-Table3-1.png", "5-Table4-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [3], "response": "{'Answer': 'Relations are used to propagate polarity by identifying the relationship between events and encouraging them to have the same or reverse polarity. For example, a \"CAUSE\" relation encourages events to have the same polarity, while a \"CONCESSION\" relation encourages events to have the reverse polarity.'}", "all_figures_names": ["5-Table3-1.png", "4-Table2-1.png", "5-Table4-1.png", "2-Figure1-1.png", "5-Table5-1.png", "4-Table1-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [3], "response": "{'Answer': 'Relations such as CAUSE and CONCESSION are used to propagate event polarity from one event to another. For example, a CAUSE relation between events will propagate the same polarity, while a CONCESSION relation will propagate the reverse polarity.'}", "all_figures_names": ["4-Table2-1.png", "5-Table5-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table3-1.png", "5-Table4-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [5, 4], "response": "{'Answer': 'The Japanese dataset used for event causality and polarity classification contains a total of 1,016,000 automatically labeled pairs and 5,000,000 each of CAUSE and CONCESSION pairs, with a total of 600,000 event polarity instances in the train, dev, and test sets.'}\n", "all_figures_names": ["5-Table4-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table3-1.png", "4-Table1-1.png", "4-Table2-1.png"], "referred_figures_names": ["4-Table2-1.png", "4-Table1-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [0], "response": "{'Answer': 'The Japanese data used for training includes 1,000,000 automatically labeled pairs, 5,000,000 CAUSE pairs, and 5,000,000 CONCESSION pairs.'}", "all_figures_names": ["4-Table2-1.png", "2-Figure1-1.png", "5-Table4-1.png", "4-Table1-1.png", "5-Table3-1.png", "5-Table5-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [5], "response": "{'Answer': 'For BERT encoder, the accuracy is enhanced from 0.876 to 0.886, an improvement of 1 percentage point. For BiGRU encoder, the accuracy improved from 0.830 to 0.879, an improvement of 4.9 percentage points.'}", "all_figures_names": ["5-Table5-1.png", "5-Table3-1.png", "4-Table2-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table4-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [4], "response": "{'Answer': 'The model uses a combination of automatically labelled data and a smaller amount of manually labelled data for cause, concession, and automatically labelled pairs.'}", "all_figures_names": ["5-Table3-1.png", "4-Table1-1.png", "5-Table5-1.png", "4-Table2-1.png", "2-Figure1-1.png", "5-Table4-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [1], "response": "{'Answer': 'The provided information does not contain the answer to how big the seed lexicon used for training is.'}", "all_figures_names": ["5-Table3-1.png", "4-Table1-1.png", "5-Table5-1.png", "5-Table4-1.png", "2-Figure1-1.png", "4-Table2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}