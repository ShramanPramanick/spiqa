{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "referred_figures_indices": [0, 2], "response": " {'Answer': 'The HMM models use the preceding characters as features. The interpretability of the features depends on the specific HMM model used. For example, the decision tree in Image 0 shows the features used by an HMM model to predict the next character in a sequence. The features are based on the preceding characters in the sequence, and the model can be interpreted as a series of decisions based on these features.'}", "all_figures_names": ["4-Figure3-1.png", "3-Table1-1.png", "4-Figure2-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "referred_figures_indices": [2, 3], "response": " {'Answer': 'The HMMs learn complementary features in the text related to spaces and comments.'}", "all_figures_names": ["2-Figure1-1.png", "3-Table1-1.png", "4-Figure3-1.png", "4-Figure2-1.png"], "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "referred_figures_indices": [1], "response": " {'Answer': 'The gap in performance between the HMMs and the LSTMs is small for the tasks shown. On the Linux text data, the HMMs outperform the LSTMs by about 0.1 perplexity points and on the Physionet data, the LSTMs outperform the HMMs by about 0.05 perplexity points.'}", "all_figures_names": ["2-Figure1-1.png", "3-Table1-1.png", "4-Figure3-1.png", "4-Figure2-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}