{"909": {"question": "What is best performing model among author's submissions, what performance it had?", "referred_figures_indices": [4], "response": " {'Answer': 'Ensemble of (II) and (IV) from each Fold1-3, i.e., M = 6 + postprocess'}", "all_figures_names": ["4-Table4-1.png", "2-Table1-1.png", "3-Figure1-1.png", "4-Table3-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."}, "910": {"question": "What extracted features were most influencial on performance?", "referred_figures_indices": [2], "response": " {'Answer': 'The most influential features for propaganda detection were found to be the combination of character-level features, POS, NER, multi-grain, and BERT sentence embeddings.'}", "all_figures_names": ["3-Table2-1.png", "3-Figure1-1.png", "4-Table3-1.png", "2-Table1-1.png", "4-Table4-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "Linguistic"}, "912": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [4], "response": " {'Answer': 'Yes, the ensemble schemes helped in boosting performance. The best ensemble scheme achieved an F1 score of 0.676, which is 0.011 higher than the best single model.'}", "all_figures_names": ["3-Table2-1.png", "3-Figure1-1.png", "2-Table1-1.png", "4-Table4-1.png", "4-Table3-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "The best ensemble topped the best single model by 0.029 in F1 score on dev (external)."}, "913": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [4, 1, 2], "response": " {'Answer': 'The ensemble schemes provided an improvement of 5.85% (66.26% vs 60.41%) and 1.34% (67.6% vs 66.26%) on F1 scores on Dev (external) and Test sets respectively.'}", "all_figures_names": ["3-Figure1-1.png", "4-Table3-1.png", "3-Table2-1.png", "2-Table1-1.png", "4-Table4-1.png"], "referred_figures_names": ["4-Table4-1.png", "4-Table3-1.png", "3-Table2-1.png"], "answer": "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"}, "914": {"question": "Which basic neural architecture perform best by itself?", "referred_figures_indices": [3], "response": " {'Answer': 'CNN and BERT Fine-tuning'}", "all_figures_names": ["3-Figure1-1.png", "4-Table4-1.png", "3-Table2-1.png", "4-Table3-1.png", "2-Table1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "BERT"}, "915": {"question": "What participating systems had better results than ones authors submitted?", "referred_figures_indices": [1], "response": " {'Answer': 'It seems that the systems with the best results were the ones that used an ensemble of models.'}", "all_figures_names": ["4-Table3-1.png", "3-Table2-1.png", "3-Figure1-1.png", "2-Table1-1.png", "4-Table4-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."}, "916": {"question": "What is specific to multi-granularity and multi-tasking neural arhiteture design?", "referred_figures_indices": [1], "response": " {'Answer': 'The multi-granularity and multi-tasking neural architecture design is specific to the FLC layer, which consists of a multi-tasking LSTM-CRF architecture. The FLC layer jointly performs Propaganda Fragment Detection (PFD) and FLC tasks (i.e., multi-grained propaganda detection).'}", "all_figures_names": ["2-Table1-1.png", "3-Figure1-1.png", "4-Table3-1.png", "4-Table4-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "An output layer for each task"}}