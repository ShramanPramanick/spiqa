{"962": {"question": "Do they build one model per topic or on all topics?", "referred_figures_indices": [6, 7], "response": " {'Answer': 'The system builds one model per topic.'}", "all_figures_names": ["15-Table7-1.png", "19-Figure1-1.png", "12-Table4-1.png", "22-Table11-1.png", "13-Table5-1.png", "16-Table8-1.png", "9-Table1-1.png", "20-Figure2-1.png"], "referred_figures_names": ["9-Table1-1.png", "20-Figure2-1.png"], "answer": "One model per topic."}, "963": {"question": "Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?", "referred_figures_indices": [3, 5], "response": " {'Answer': 'Yes, they evaluate the output of low-rank approximation quantitatively and qualitatively. Table 7 shows the results of hypothesis testing on whether matrix completion helps capture similar concepts. Table 15 shows that the proposed ILP+MC approach significantly outperforms the baselines on synthesized corpora. Besides, human evaluation results in Table 8 also show that ILP+MC is better than baselines.'}", "all_figures_names": ["24-Table14-1.png", "17-Table9-1.png", "16-Table8-1.png", "15-Table7-1.png", "27-Table15-1.png", "10-Table2-1.png", "12-Table4-1.png", "22-Table11-1.png"], "referred_figures_names": ["15-Table7-1.png", "10-Table2-1.png"], "answer": "They evaluate quantitatively."}}