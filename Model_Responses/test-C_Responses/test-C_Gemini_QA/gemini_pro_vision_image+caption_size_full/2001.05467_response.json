{"715": {"question": "To what other competitive baselines is this approach compared?", "referred_figures_indices": [5], "response": " {'Answer': 'The approach is compared to the following baselines: \\n* LSTM\\n* LSTM with attention\\n* HRED\\n* VHRED\\n* VHRED with attention\\n* MMI\\n* Reranking with RL\\n* LFT\\n* RL\\n* MinAvgOut + RL'}", "all_figures_names": ["3-Figure1-1.png", "3-Figure2-1.png", "6-Table3-1.png", "5-Figure6-1.png", "7-Table4-1.png", "5-Table1-1.png", "4-Figure4-1.png", "4-Figure3-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL"}, "717": {"question": "How much better were results of the proposed models than base LSTM-RNN model?", "referred_figures_indices": [2], "response": " {'Answer': 'The proposed models outperform the base LSTM-RNN model by a large margin on all the automatic evaluation metrics.'}", "all_figures_names": ["3-Figure2-1.png", "7-Table4-1.png", "5-Table1-1.png", "6-Table2-1.png", "3-Figure1-1.png", "4-Figure4-1.png", "5-Figure6-1.png", "4-Figure3-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "on diversity 6.87 and on relevance 4.6 points higher"}, "718": {"question": "Which one of the four proposed models performed best?", "referred_figures_indices": [1, 3], "response": " {'Answer': 'From Table 2 and Table 3, we can see that MinAvgOut+RL outperforms other models in terms of both automatic and human evaluations.'}", "all_figures_names": ["3-Figure1-1.png", "5-Table1-1.png", "5-Figure6-1.png", "7-Table4-1.png", "6-Table2-1.png", "3-Figure2-1.png", "6-Table3-1.png", "4-Figure3-1.png"], "referred_figures_names": ["5-Table1-1.png", "7-Table4-1.png"], "answer": "the hybrid model MinAvgOut + RL"}}