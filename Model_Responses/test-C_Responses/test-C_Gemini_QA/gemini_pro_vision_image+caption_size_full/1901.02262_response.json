{"457": {"question": "What do they mean by answer styles?", "referred_figures_indices": [4], "response": " {'Answer': 'In this paper, we propose a multi-style answer generation model, Masque, that can control the answer style by conditioning on an answer style label. We achieve this by introducing a mixture mechanism over three different distributions for generating words: (1) a vocabulary distribution, (2) a copying distribution over the question, and (3) a copying distribution over the passages. The mixture weights are dynamically computed based on the answer style label and the current decoding state. We show that our model can generate answers in different styles and that it outperforms strong baselines on two public QA datasets.'}", "all_figures_names": ["4-Figure3-1.png", "7-Figure4-1.png", "2-Figure2-1.png", "1-Figure1-1.png", "5-Table1-1.png", "6-Table3-1.png", "6-Table4-1.png", "7-Figure5-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "referred_figures_indices": [2], "response": " {'Answer': 'No, the experiments show that there are multiple \"answer styles\" per dataset.'}", "all_figures_names": ["2-Figure2-1.png", "6-Table4-1.png", "5-Table1-1.png", "7-Figure5-1.png", "1-Figure1-1.png", "8-Table5-1.png", "4-Figure3-1.png", "6-Table2-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "referred_figures_indices": [6, 0], "response": " {'Answer': 'The baselines that Masque is compared against are BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, and VNET.'}", "all_figures_names": ["6-Table2-1.png", "5-Table1-1.png", "7-Figure4-1.png", "7-Figure5-1.png", "2-Figure2-1.png", "6-Table4-1.png", "8-Table5-1.png", "6-Table3-1.png"], "referred_figures_names": ["8-Table5-1.png", "6-Table2-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "referred_figures_indices": [6], "response": " {'Answer': '59.87'}", "all_figures_names": ["4-Figure3-1.png", "6-Table2-1.png", "2-Figure2-1.png", "6-Table4-1.png", "7-Figure5-1.png", "5-Table1-1.png", "8-Table5-1.png", "7-Figure4-1.png"], "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "referred_figures_indices": [2], "response": " {'Answer': 'In this paper, we propose Masque, a neural question answering model that jointly reasons about answer existence and answer content. Masque consists of a) a multi-style answer generator that can generate answers in different answer styles, and b) a passage retriever that selects the most relevant passages for a given question. We show that Masque achieves state-of-the-art performance on two challenging question answering datasets: NarrativeQA and MS MARCO.'}", "all_figures_names": ["7-Figure4-1.png", "6-Table3-1.png", "5-Table1-1.png", "6-Table4-1.png", "7-Figure5-1.png", "8-Table5-1.png", "1-Figure1-1.png", "2-Figure2-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}