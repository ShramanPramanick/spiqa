{"883": {"question": "Do the methods that work best on academic papers also work best on Wikipedia?", "referred_figures_indices": [3], "response": " {'Answer': 'The methods that work best on academic papers do not also work best on Wikipedia.'}", "all_figures_names": ["1-Figure1-1.png", "4-Table2-1.png", "3-Figure2-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "No"}, "884": {"question": "What is their system's absolute accuracy?", "referred_figures_indices": [1], "response": " {'Answer': 'The absolute accuracy of their system is 59.07% on Wikipedia, 77.31% on Peer Review, and 79.54% on Cl.Sg.'}", "all_figures_names": ["3-Figure2-1.png", "4-Table1-1.png", "1-Figure1-1.png", "4-Table2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"}, "887": {"question": "Which languages do they use?", "referred_figures_indices": [2], "response": " {'Answer': 'Based on the confusion matrix in Image 2, the model was trained and tested on English Wikipedia.'}", "all_figures_names": ["4-Table1-1.png", "3-Figure2-1.png", "4-Table2-1.png", "1-Figure1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "English"}, "888": {"question": "How large is their data set?", "referred_figures_indices": [0], "response": " {'Answer': 'The size of the dataset is not explicitly stated in the information provided.'}", "all_figures_names": ["4-Table1-1.png", "1-Figure1-1.png", "4-Table2-1.png", "3-Figure2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "a sample of  29,794 wikipedia articles and 2,794 arXiv papers "}}