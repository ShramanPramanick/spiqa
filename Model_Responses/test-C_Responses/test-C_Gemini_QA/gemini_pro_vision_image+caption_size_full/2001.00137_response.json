{"48": {"question": "Do they report results only on English datasets?", "referred_figures_indices": [5, 7], "response": " {'Answer': 'No, the authors also report results on a Spanish dataset.'}", "all_figures_names": ["15-Figure3-1.png", "9-Table3-1.png", "16-Table7-1.png", "5-Figure1-1.png", "17-Figure5-1.png", "9-Table2-1.png", "8-Table1-1.png", "10-Table4-1.png"], "referred_figures_names": ["9-Table2-1.png", "10-Table4-1.png"], "answer": "Yes"}, "51": {"question": "Do they test their approach on a dataset without incomplete data?", "referred_figures_indices": [7], "response": " {'Answer': 'Yes, the authors test their approach on a dataset without incomplete data. They use the Sentiment140 dataset, which is a Twitter sentiment classification dataset. The dataset contains 25,000 tweets, half of which are positive and half of which are negative.'}", "all_figures_names": ["16-Table7-1.png", "14-Table6-1.png", "10-Table4-1.png", "9-Table2-1.png", "9-Table3-1.png", "15-Figure3-1.png", "12-Table5-1.png", "11-Figure2-1.png"], "referred_figures_names": ["11-Figure2-1.png"], "answer": "No"}, "52": {"question": "Do they test their approach on a dataset without incomplete data?", "referred_figures_indices": [1], "response": " {'Answer': 'Yes, the authors test their approach on a dataset without incomplete data. They use the Sentiment140 corpus, which is a dataset of tweets that have been labeled as positive or negative. The corpus contains both correctly spelled tweets and tweets with spelling errors.'}", "all_figures_names": ["14-Table6-1.png", "8-Table1-1.png", "11-Figure2-1.png", "17-Figure5-1.png", "5-Figure1-1.png", "12-Table5-1.png", "10-Table4-1.png", "9-Table3-1.png"], "referred_figures_names": ["8-Table1-1.png"], "answer": "No"}, "53": {"question": "Should their approach be applied only when dealing with incomplete data?", "referred_figures_indices": [0], "response": " {'Answer': 'Yes, because the approach is robust to noise in the data, as shown in Figure 4.'}", "all_figures_names": ["14-Table6-1.png", "9-Table2-1.png", "11-Figure2-1.png", "12-Table5-1.png", "8-Table1-1.png", "17-Figure4-1.png", "10-Table4-1.png", "15-Figure3-1.png"], "referred_figures_names": ["14-Table6-1.png"], "answer": "No"}, "54": {"question": "Should their approach be applied only when dealing with incomplete data?", "referred_figures_indices": [6], "response": " {'Answer': 'No. Their approach can be applied to any noisy text, not only incomplete data.'}", "all_figures_names": ["16-Table7-1.png", "11-Figure2-1.png", "10-Table4-1.png", "17-Figure4-1.png", "15-Figure3-1.png", "14-Table6-1.png", "5-Figure1-1.png", "12-Table5-1.png"], "referred_figures_names": ["5-Figure1-1.png"], "answer": "No"}, "55": {"question": "By how much do they outperform other models in the sentiment in intent classification tasks?", "referred_figures_indices": [5, 6], "response": " {'Answer': 'The stacked DEBERT model outperforms other models in the sentiment intent classification tasks. For the original dataset, the stacked DEBERT model achieves an F1-score of 80.0% while the best baseline model, BERT, only achieves 76.0%. For the dataset with STT errors, the stacked DEBERT model achieves an F1-score of 97.1% while the best baseline model, BERT, only achieves 96.2%'}", "all_figures_names": ["9-Table3-1.png", "17-Figure4-1.png", "11-Figure2-1.png", "9-Table2-1.png", "12-Table5-1.png", "14-Table6-1.png", "16-Table7-1.png", "8-Table1-1.png"], "referred_figures_names": ["14-Table6-1.png", "16-Table7-1.png"], "answer": "In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average"}}