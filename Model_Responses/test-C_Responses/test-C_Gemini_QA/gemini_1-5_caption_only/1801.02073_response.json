{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "referred_figures_indices": [3], "response": "{'Answer': 'While the provided context mentions answer selection and triggering, it does not explicitly state whether an indexing-based method is used to create a sample of a QA Wikipedia dataset. Therefore, the answer cannot be inferred from the given information.'}", "all_figures_names": ["3-Figure2-1.png", "4-Table3-1.png", "3-Figure1-1.png", "3-Table2-1.png", "2-Table1-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "referred_figures_indices": [1], "response": "{'Answer': \"The captions don't specify the number of question types found in the datasets. However, Figure 1 shows the distributions of question types in percentages.\"}", "all_figures_names": ["3-Table2-1.png", "3-Figure1-1.png", "3-Figure2-1.png", "2-Table1-1.png", "4-Table3-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "referred_figures_indices": [4], "response": "{'Answer': 'While the provided captions describe the datasets and their characteristics, they do not explain how contextual similarities across datasets are analyzed. Therefore, I cannot answer your question based on the given captions.'}", "all_figures_names": ["3-Figure1-1.png", "4-Table3-1.png", "3-Figure2-1.png", "3-Table2-1.png", "2-Table1-1.png"], "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}