{"182": {"question": "Do they inspect their model to see if their model learned to associate image parts with words related to entities?", "referred_figures_indices": [4], "response": "{'Answer': 'Yes, they inspect their model\\'s use of visual features by visualizing the attention weights assigned to InceptionNet image tags. This helps understand if the model associates relevant image parts with entity-related words, as shown in Figure 2 and Table 2.'}", "all_figures_names": ["3-Figure2-1.png", "7-Table2-1.png", "1-Figure1-1.png", "8-Table3-1.png", "8-Figure3-1.png", "6-Table1-1.png"], "referred_figures_names": ["8-Figure3-1.png"], "answer": "Yes"}, "183": {"question": "Do they inspect their model to see if their model learned to associate image parts with words related to entities?", "referred_figures_indices": [3], "response": "{'Answer': 'Yes, they visualize the modality attention to analyze if their model associates image parts with entity-related words, as shown in Figure 3 and Table 3, where they present visual tags from InceptionNet alongside model predictions.'}", "all_figures_names": ["3-Figure2-1.png", "8-Figure3-1.png", "6-Table1-1.png", "7-Table2-1.png", "1-Figure1-1.png", "8-Table3-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}}