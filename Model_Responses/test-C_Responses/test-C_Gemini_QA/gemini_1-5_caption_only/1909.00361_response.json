{"1107": {"question": "How big are the datasets used?", "referred_figures_indices": [3], "response": "{'Answer': 'This document does not specify the exact size of any datasets used. It does reference several datasets like SQuAD, CMRC 2018, and DRCD, but doesn\u2019t provide their sizes.'}", "all_figures_names": ["7-Table3-1.png", "3-Figure1-1.png", "5-Figure2-1.png", "6-Table1-1.png", "8-Figure3-1.png", "8-Table4-1.png", "7-Table2-1.png"], "referred_figures_names": ["6-Table1-1.png"], "answer": "Evaluation datasets used:\nCMRC 2018 - 18939 questions, 10 answers\nDRCD - 33953 questions, 5 answers\nNIST MT02/03/04/05/06/08 Chinese-English - Not specified\n\nSource language train data:\nSQuAD - Not specified"}, "1108": {"question": "Is this a span-based (extractive) QA task?", "referred_figures_indices": [6], "response": "{'Answer': 'Yes, the captions mention \"machine reading comprehension\" and datasets like SQuAD, CMRC, and DRCD, which are commonly used for extractive QA tasks. Extractive QA involves finding an answer within the given text, making it span-based.'} \n", "all_figures_names": ["8-Figure3-1.png", "7-Table2-1.png", "3-Figure1-1.png", "5-Figure2-1.png", "7-Table3-1.png", "8-Table4-1.png", "6-Table1-1.png"], "referred_figures_names": ["6-Table1-1.png"], "answer": "Yes"}}