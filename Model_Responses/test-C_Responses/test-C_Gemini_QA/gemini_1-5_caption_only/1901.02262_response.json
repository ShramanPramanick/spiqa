{"457": {"question": "What do they mean by answer styles?", "referred_figures_indices": [4], "response": "{'Answer': 'Based on the caption \"Figure 5: Lengths of answers generated by Masque broken down by the answer style and query type on the NLG dev. set. The error bars indicate standard errors.\", it seems \"answer styles\" refer to different ways a question can be answered by the model, potentially impacting the length of the generated answer.  Without further context, it is difficult to be more specific.'}", "all_figures_names": ["4-Figure3-1.png", "2-Figure2-1.png", "6-Table2-1.png", "7-Figure4-1.png", "5-Table1-1.png", "6-Table3-1.png", "7-Figure5-1.png", "6-Table4-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "referred_figures_indices": [5], "response": "{'Answer': 'No, there can be multiple answer styles per dataset, such as NLG and Q&A in the MS MARCO dataset.'}", "all_figures_names": ["7-Figure5-1.png", "8-Table5-1.png", "4-Figure3-1.png", "1-Figure1-1.png", "7-Figure4-1.png", "5-Table1-1.png", "6-Table2-1.png", "2-Figure2-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "referred_figures_indices": [3, 5], "response": "{'Answer': 'Based on the provided captions, Masque is compared against Seo et al. (2017), Tay et al. (2018), Bauer et al. (2018), Indurthi et al. (2018), Hu et al. (2018), Yan et al. (2019), Shao (unpublished), a variant of Tan et al. (2018), Li (unpublished), a model using Devlin et al. (2018) and See et al. (2017), Qian (unpublished), and Wu et al. (2018) on different tasks and datasets, as shown in Table 2, Table 3, Table 4 and Table 5.'}", "all_figures_names": ["6-Table3-1.png", "7-Figure4-1.png", "6-Table4-1.png", "8-Table5-1.png", "2-Figure2-1.png", "6-Table2-1.png", "4-Figure3-1.png", "5-Table1-1.png"], "referred_figures_names": ["8-Table5-1.png", "6-Table2-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "referred_figures_indices": [1], "response": "{'Answer': 'The provided context does not contain the performance achieved on NarrativeQA. While some tables mention \"NarrativeQA test set\" and \"competing models,\" the specific results are not presented.'}", "all_figures_names": ["6-Table4-1.png", "8-Table5-1.png", "5-Table1-1.png", "4-Figure3-1.png", "1-Figure1-1.png", "7-Figure4-1.png", "6-Table2-1.png", "6-Table3-1.png"], "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "referred_figures_indices": [2], "response": "{'Answer': 'An \"answer style\" refers to the different approaches or formats a model can use to generate an answer, such as Natural Language Generation (NLG) for producing human-like text or Question Answering (Q\\&A) for extracting specific information.'}", "all_figures_names": ["7-Figure4-1.png", "2-Figure2-1.png", "5-Table1-1.png", "6-Table3-1.png", "8-Table5-1.png", "6-Table2-1.png", "7-Figure5-1.png", "1-Figure1-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}