{"2": {"question": "What are the results?", "referred_figures_indices": [0, 3], "response": "{'Answer': 'The provided context does not contain the results for any experiment or analysis. It presents tables and figures summarizing model performance, dataset details, and examples, but not the actual results.'}", "all_figures_names": ["5-Table3-1.png", "4-Table2-1.png", "5-Table5-1.png", "5-Table4-1.png", "2-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["5-Table3-1.png", "5-Table4-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [0], "response": "{'Answer': 'Relations like CAUSE and CONCESSION are used to propagate polarity from a latter event with known polarity to a former event with unknown polarity. For CAUSE relations, the same polarity is propagated, while for CONCESSION relations, the reversed polarity is used.'}", "all_figures_names": ["2-Figure1-1.png", "4-Table1-1.png", "5-Table3-1.png", "5-Table5-1.png", "4-Table2-1.png", "5-Table4-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [3], "response": "{'Answer': 'Based on the provided captions, specifically Caption 3, relations are used to propagate polarity by considering the type of discourse relation between two events. For instance, if the relation is \"CAUSE,\" the polarity of the latter event is propagated to the former event. On the other hand, if the relation is \"CONCESSION,\" the reversed polarity of the latter event is applied to the former.'}", "all_figures_names": ["5-Table4-1.png", "5-Table3-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table5-1.png", "4-Table2-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [0, 2], "response": "{'Answer': 'The provided context does not contain information about the size of Japanese data. Therefore, I cannot answer your question.'}", "all_figures_names": ["4-Table2-1.png", "5-Table4-1.png", "4-Table1-1.png", "5-Table3-1.png", "5-Table5-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table2-1.png", "4-Table1-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [2], "response": "{'Answer': 'The answer about the size of the Japanese data can be found in Table 1, Table 2, or Table 5, but the content of these tables is not provided in the context.'}", "all_figures_names": ["2-Figure1-1.png", "5-Table5-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [5], "response": "{'Answer': \"This question cannot be answered from the given context. While the captions mention using a smaller labeled dataset enhanced with a proposed approach, there's no information on the degree of improvement compared to a basic approach.\"}", "all_figures_names": ["2-Figure1-1.png", "4-Table2-1.png", "5-Table3-1.png", "5-Table5-1.png", "4-Table1-1.png", "5-Table4-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [0], "response": "{'Answer': 'While the provided captions don\\'t contain the answer to how the model learns using raw data, they describe a method focusing on event pairs and discourse relations to learn polarity in text, particularly in cases of cause and concession.'}", "all_figures_names": ["2-Figure1-1.png", "4-Table1-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table5-1.png", "5-Table3-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [5], "response": "{'Answer': \"The provided captions don't specify the exact size of the seed lexicon used for training the model. Therefore, I cannot provide an answer to your question.\"}", "all_figures_names": ["2-Figure1-1.png", "5-Table5-1.png", "4-Table2-1.png", "5-Table3-1.png", "5-Table4-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}