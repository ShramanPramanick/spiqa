{"204": {"question": "What is difference in peformance between proposed model and state-of-the art on other question types?", "referred_figures_indices": [3], "response": " {'Answer': 'The proposed model outperforms the previous state-of-the-art model on DROP by 1.4 EM and 1.5 F1 points on the development set, and by 0.7 EM and 0.6 F1 points on the test set. The ablation study shows that the performance gain mainly comes from the better answer span prediction module and the number prediction module.'}", "all_figures_names": ["6-Table3-1.png", "7-Table4-1.png", "6-Table1-1.png", "6-Table2-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1."}, "205": {"question": "What is the performance of proposed model on entire DROP dataset?", "referred_figures_indices": [0], "response": " {'Answer': 'The proposed model achieves state-of-the-art performance on the DROP dataset, with an F1 score of 80.73 on the test set and 80.25 on the development set.'}", "all_figures_names": ["6-Table3-1.png", "6-Table1-1.png", "6-Table2-1.png", "7-Table4-1.png"], "referred_figures_names": ["6-Table3-1.png"], "answer": "The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev"}}