{"909": {"question": "What is best performing model among author's submissions, what performance it had?", "referred_figures_indices": [2], "response": " {'Answer': 'The best performing model is the one that uses BERT + Fine-tune, which achieved an F1 score of 0.676 on the development set and 0.650 on the test set.'}", "all_figures_names": ["3-Figure1-1.png", "2-Table1-1.png", "3-Table2-1.png", "4-Table4-1.png", "4-Table3-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."}, "910": {"question": "What extracted features were most influencial on performance?", "referred_figures_indices": [2], "response": " {'Answer': 'The most influential features were\u304c\u51fa\u308bBERT and FastText embeddings.'}", "all_figures_names": ["3-Figure1-1.png", "3-Table2-1.png", "4-Table3-1.png", "4-Table4-1.png", "2-Table1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "Linguistic"}, "912": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [4], "response": " {'Answer': 'Yes, the ensemble schemes helped in boosting performance. The F1 score of the best single model is 0.676, while the F1 score of the best ensemble model is 0.699. This is an improvement of 0.023. The ensemble model also has a higher precision and recall than the best single model.'}", "all_figures_names": ["3-Figure1-1.png", "3-Table2-1.png", "4-Table4-1.png", "2-Table1-1.png", "4-Table3-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "The best ensemble topped the best single model by 0.029 in F1 score on dev (external)."}, "913": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [1, 3, 2], "response": " {'Answer': 'The ensemble schemes provided an absolute improvement of 0.6-1.5% F1 score on the dev set and 0.3-0.8% on the test set.'}", "all_figures_names": ["3-Figure1-1.png", "4-Table3-1.png", "4-Table4-1.png", "3-Table2-1.png", "2-Table1-1.png"], "referred_figures_names": ["4-Table3-1.png", "3-Table2-1.png", "4-Table4-1.png"], "answer": "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"}, "914": {"question": "Which basic neural architecture perform best by itself?", "referred_figures_indices": [2], "response": " {'Answer': 'CNN + BERT + Finetune'}", "all_figures_names": ["2-Table1-1.png", "3-Figure1-1.png", "4-Table3-1.png", "3-Table2-1.png", "4-Table4-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "BERT"}, "915": {"question": "What participating systems had better results than ones authors submitted?", "referred_figures_indices": [2], "response": " {'Answer': 'It seems like the best external F1 score among the participating systems is 0.676, while the best external F1 score the authors achieved is 0.675. The best external P and R scores among the participating systems are 0.751 and 0.645 respectively, while the best external P and R scores the authors achieved are 0.747 and 0.635 respectively.'}", "all_figures_names": ["4-Table4-1.png", "4-Table3-1.png", "3-Table2-1.png", "2-Table1-1.png", "3-Figure1-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."}, "916": {"question": "What is specific to multi-granularity and multi-tasking neural arhiteture design?", "referred_figures_indices": [4], "response": " The image shows the system architecture of the propaganda detection model. The model consists of two main components: a multi-granularity component and a multi-tasking component. The multi-granularity component is responsible for learning representations of the input text at different levels of granularity. The multi-tasking component is responsible for learning to perform multiple tasks related to propaganda detection, such as identifying propaganda techniques and classifying the propaganda into different categories.", "all_figures_names": ["4-Table4-1.png", "2-Table1-1.png", "4-Table3-1.png", "3-Table2-1.png", "3-Figure1-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "An output layer for each task"}}