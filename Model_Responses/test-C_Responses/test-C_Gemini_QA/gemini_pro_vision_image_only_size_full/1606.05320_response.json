{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "referred_figures_indices": [1, 3], "response": " {'Answer': 'HMMs are typically trained using Baum-Welch algorithm, which is an iterative algorithm that finds the most likely sequence of hidden states given a sequence of observations. The Baum-Welch algorithm is guaranteed to converge to a local optimum, but it is not guaranteed to find the global optimum. The interpretability of HMMs depends on the specific application. In some cases, it is possible to interpret the hidden states as physical entities, such as the parts of speech in a sentence. In other cases, the hidden states are more abstract and difficult to interpret.'}", "all_figures_names": ["2-Figure1-1.png", "4-Figure2-1.png", "3-Table1-1.png", "4-Figure3-1.png"], "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "referred_figures_indices": [3, 0], "response": " {'Answer': 'The HMMs learn the state probabilities while the LSTMs learn the transition probabilities.'}", "all_figures_names": ["4-Figure3-1.png", "2-Figure1-1.png", "3-Table1-1.png", "4-Figure2-1.png"], "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "referred_figures_indices": [1], "response": " {'Answer': 'The gap in performance between the HMMs and the LSTMs is not large, the difference is less than 0.1 perplexity on all datasets except the single patient Physionet dataset, on which LSTMs outperform HMMs by perplexity of 0.14 on the continuous features and 0.08 on the discretized features.'}", "all_figures_names": ["2-Figure1-1.png", "3-Table1-1.png", "4-Figure3-1.png", "4-Figure2-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}