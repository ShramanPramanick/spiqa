{"1275": {"question": "what are the state of the art methods?", "referred_figures_indices": [3], "response": " {'Answer': 'The paper proposes a video captioning model based on the Transformer, which is pre-trained on large-scale video-text pairs and then fine-tuned on video captioning datasets. The model achieves state-of-the-art results on several video captioning benchmarks.'}", "all_figures_names": ["4-Figure2-1.png", "2-Figure1-1.png", "4-Table1-1.png", "4-Table2-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al."}}