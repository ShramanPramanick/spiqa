{"654": {"question": "How they extract \"structured answer-relevant relation\"?", "referred_figures_indices": [0], "response": " {'Answer': '32.6 F (0.3 C)'}", "all_figures_names": ["1-Figure1-1.png", "3-Table2-1.png", "2-Table1-1.png", "6-Table4-1.png", "3-Figure2-1.png", "8-Figure5-1.png", "4-Figure3-1.png", "7-Figure4-1.png"], "referred_figures_names": ["1-Figure1-1.png"], "answer": "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."}, "656": {"question": "How big are significant improvements?", "referred_figures_indices": [0], "response": " {'Answer': 'In the first image, our model outperforms the previous state-of-the-art model on the Du Split by a large margin (45.66 vs. 44.91) and achieves comparable performance on the Zhou Split (44.35 vs. 44.24). In the second image, our model significantly outperforms the baseline on both the automatic metrics (BLEU: 29.11 vs. 20.80; METEOR: 21.50 vs. 16.72) and human evaluation (R-L: 45.97 vs. 37.63). In the third image, our model achieves better performance than the hybrid model on sentences with more than 20 words (BLEU: 22.04 vs. 20.58; METEOR: 17.38 vs. 16.66; R-L: 38.27 vs. 37.53). In the fourth image, our model can correctly extract the answer \"32.6\" from the sentence, while the baseline model fails. In the fifth image, our model can correctly identify the N-ary relation \"Beyonc\u00e9 received commercial success selling: one million digital copies worldwide; in six days\". In the sixth image, our model can correctly identify the N-ary relation \"The daily mean temperature in January: is; 32.6 \u00b0F (0.3 \u00b0C)\". In the seventh image, our model can correctly identify the N-ary relation \"temperatures: drop; to 10 \u00b0F (-12 \u00b0C); several times per winter; usually\".'}", "all_figures_names": ["6-Table4-1.png", "7-Figure4-1.png", "2-Table1-1.png", "8-Figure5-1.png", "4-Figure3-1.png", "7-Table5-1.png", "1-Figure1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["6-Table4-1.png"], "answer": "Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"}}