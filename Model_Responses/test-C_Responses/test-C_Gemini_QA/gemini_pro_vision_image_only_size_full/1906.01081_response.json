{"220": {"question": "How many people participated in their evaluation study of table-to-text models?", "referred_figures_indices": [0], "response": " {'Answer': '7 human evaluators participated in their study.'}", "all_figures_names": ["5-Table1-1.png", "11-Table5-1.png", "8-Table3-1.png", "8-Table4-1.png", "11-Figure6-1.png", "12-Table6-1.png", "9-Figure5-1.png", "6-Figure2-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "about 500"}, "222": {"question": "By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?", "referred_figures_indices": [7, 2, 1], "response": " {'Answer': 'PARENT-C correlates 0.08 more with human judgements in comparison to BLEU, 0.07 more than BLEU-T, 0.06 more than RG-F, 0.01 more than PARENT-W.'}", "all_figures_names": ["8-Table3-1.png", "8-Table4-1.png", "6-Table2-1.png", "2-Figure1-1.png", "12-Table6-1.png", "6-Figure2-1.png", "11-Figure6-1.png", "7-Figure3-1.png"], "referred_figures_names": ["7-Figure3-1.png", "6-Table2-1.png", "8-Table4-1.png"], "answer": "Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge."}, "223": {"question": "By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?", "referred_figures_indices": [3], "response": " {'Answer': 'PARENT metric correlates 0.15 more with human judgements compared to BLEU, 0.08 more compared to METEOR, 0.19 more compared to CIDEr, and 0.06 more compared to ROUGE.'}", "all_figures_names": ["7-Figure3-1.png", "11-Figure6-1.png", "2-Figure1-1.png", "6-Table2-1.png", "5-Table1-1.png", "7-Figure4-1.png", "6-Figure2-1.png", "9-Figure5-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "Their average correlation tops the best other model by 0.155 on WikiBio."}}