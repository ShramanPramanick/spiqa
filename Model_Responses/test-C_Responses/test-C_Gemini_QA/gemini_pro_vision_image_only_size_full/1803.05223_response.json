{"903": {"question": "what dataset statistics are provided?", "referred_figures_indices": [5], "response": " {'Answer': 'The dataset statistics that are provided are: \\n 1. 10,160 text-based answerable questions \\n 2. 3,914 script-based answerable questions \\n 3. 9,974 unknown questions \\n 4. 3,172 unfit questions \\n 5. 72% attentive reader accuracy \\n 6. 98.2% human performance accuracy'}", "all_figures_names": ["1-Figure1-1.png", "6-Figure4-1.png", "4-Figure2-1.png", "4-Figure3-1.png", "6-Table2-1.png", "3-Table1-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%)."}, "904": {"question": "what dataset statistics are provided?", "referred_figures_indices": [0], "response": " {'Answer': 'The dataset contains 14074 text-based answerable questions, 3914 script-based answerable questions, 9974 unknown answerable questions, and 3172 unfitting questions. The distribution of question types is as follows: 29% yes/no questions, 14% what/which questions, 12% who/whose questions, 12% why questions, 9% where questions, 7% how questions, 6% when questions, 5% how long/often questions, and 4% how many/much questions.'}", "all_figures_names": ["3-Table1-1.png", "4-Figure2-1.png", "1-Figure1-1.png", "4-Figure3-1.png", "6-Table2-1.png", "6-Figure4-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text"}}