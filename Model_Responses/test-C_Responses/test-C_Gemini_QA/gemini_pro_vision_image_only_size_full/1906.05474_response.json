{"1363": {"question": "Could you tell me more about the metrics used for performance evaluation?", "referred_figures_indices": [1], "response": " {'Answer': 'In this paper, we use the standard metrics for each task as shown in Table 1. For sentence similarity, we use the Pearson correlation coefficient. For named entity recognition (NER), we use the F1 score. For relation extraction, we use the micro F1 score. For document classification, we use the F1 score. For inference, we use accuracy. For the overall performance, we use the average F1 score of all tasks.'}", "all_figures_names": ["5-Table3-1.png", "3-Table1-1.png", "4-Table2-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy"}}