{"457": {"question": "What do they mean by answer styles?", "referred_figures_indices": [6], "response": "{'Answer': 'Answer styles refer to the different ways in which a model generates an answer. In the context of the caption provided, the model can generate an answer either in a natural language generation (NLG) style or a question-and-answer (Q&A) style.'}", "all_figures_names": ["1-Figure1-1.png", "8-Table5-1.png", "4-Figure3-1.png", "7-Figure4-1.png", "2-Figure2-1.png", "6-Table4-1.png", "5-Table1-1.png", "6-Table2-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "referred_figures_indices": [7], "response": "{'Answer': 'This question is unanswerable from the given context because none of the captions contain information about the number of \"answer styles\" per dataset.'}", "all_figures_names": ["7-Figure5-1.png", "1-Figure1-1.png", "8-Table5-1.png", "6-Table4-1.png", "6-Table3-1.png", "2-Figure2-1.png", "6-Table2-1.png", "5-Table1-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "referred_figures_indices": [0, 2], "response": "{'Answer': 'The baselines that Masque is compared against are aSeo et al. (2017), bTay et al. (2018), cBauer et al. (2018), dIndurthi et al. (2018), eHu et al. (2018), fResults on the NarrativeQA validation set, aSeo et al. (2017), bYan et al. (2019), cShao (unpublished), a variant of Tan et al. (2018), dLi (unpublished), a model using Devlin et al. (2018) and See et al. (2017), eQian (unpublished), fWu et al. (2018). Whether the competing models are ensemble models or not is unreported.'}", "all_figures_names": ["8-Table5-1.png", "6-Table4-1.png", "6-Table2-1.png", "5-Table1-1.png", "7-Figure5-1.png", "4-Figure3-1.png", "7-Figure4-1.png", "6-Table3-1.png"], "referred_figures_names": ["8-Table5-1.png", "6-Table2-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "referred_figures_indices": [0], "response": "{'Answer': 'I cannot find the answer to your question in the context.'}", "all_figures_names": ["8-Table5-1.png", "4-Figure3-1.png", "6-Table3-1.png", "7-Figure5-1.png", "1-Figure1-1.png", "7-Figure4-1.png", "5-Table1-1.png", "6-Table2-1.png"], "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "referred_figures_indices": [7], "response": "{'Answer': 'An \"answer style\" is the top level classification of an answer created by a model. A model can classify an answer as either \"NLG\" (Natural Language Generation) or \"Q&A\" (Question and Answer) based on the type of answer created.'}", "all_figures_names": ["7-Figure4-1.png", "1-Figure1-1.png", "6-Table3-1.png", "6-Table4-1.png", "7-Figure5-1.png", "6-Table2-1.png", "4-Figure3-1.png", "5-Table1-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}