{"585": {"question": "What evaluations did the authors use on their system?", "referred_figures_indices": [7, 0, 6], "response": "{'Answer': 'This paper uses the BLEU score to evaluate its system. The BLEU score is a common metric for evaluating the quality of machine translation systems. It measures the similarity between a machine-generated translation and a human-generated translation.'}", "all_figures_names": ["10-Table6-1.png", "5-Figure3-1.png", "8-Table2-1.png", "4-Figure1-1.png", "9-Table5-1.png", "9-Table4-1.png", "8-Table1-1.png", "9-Table3-1.png"], "referred_figures_names": ["9-Table3-1.png", "10-Table6-1.png", "8-Table1-1.png"], "answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."}}