{"434": {"question": "What other sentence embeddings methods are evaluated?", "referred_figures_indices": [4, 0], "response": "{'Answer': 'I cannot find the answer to this question in the provided document.'}", "all_figures_names": ["4-Table1-1.png", "6-Table4-1.png", "3-Figure1-1.png", "8-Table7-1.png", "6-Table3-1.png", "3-Figure2-1.png", "7-Table5-1.png", "7-Table6-1.png"], "referred_figures_names": ["6-Table3-1.png", "4-Table1-1.png"], "answer": "GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent"}, "435": {"question": "What other sentence embeddings methods are evaluated?", "referred_figures_indices": [5], "response": "{'Answer': 'I cannot find that answer based on the context you provided. The context describes various methods and tasks used to evaluate SBERT, but does not specifically mention evaluations of other sentence embedding methods.'}", "all_figures_names": ["7-Table6-1.png", "5-Table2-1.png", "3-Figure1-1.png", "6-Table4-1.png", "8-Table7-1.png", "7-Table5-1.png", "3-Figure2-1.png", "4-Table1-1.png"], "referred_figures_names": ["7-Table5-1.png"], "answer": "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."}}