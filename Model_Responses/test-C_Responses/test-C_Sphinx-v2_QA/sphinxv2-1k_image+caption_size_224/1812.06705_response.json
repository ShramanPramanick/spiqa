{"247": {"question": "How do the authors measure performance?", "response": {"7-Table2-1.png": ["No", "The authors measure performance by calculating the average of the top 1, 2, 3, and 4 rankings from the model's output."]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "Accuracy across six datasets"}, "248": {"question": "Does the new objective perform better than the original objective bert is trained on?", "response": {"7-Table2-1.png": ["No", "Yes, the new objective performs better than the original objective bert is trained on."]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}, "249": {"question": "Are other pretrained language models also evaluated for contextual augmentation? ", "response": {"7-Table2-1.png": ["No", "Yes, the table shows that other pretrained language models such as CNN, w2v, w-C-BERT, and w2v-BERT are also evaluated for contextual augmentation."]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "No"}, "250": {"question": "Do the authors report performance of conditional bert on tasks without data augmentation?", "response": {"7-Table2-1.png": ["No", "No, the authors report performance of conditional bert on tasks with data augmentation."]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}}