{"457": {"question": "What do they mean by answer styles?", "response": {"5-Table1-1.png": ["1. NarrativeQA: 32,747 2. Summary: 3,461 3. ALL: 808,731 4. MS MARCO: 503,370 5. NLG: 135,725 6. Dev.: 101,093 7. Eval.: 101,092", "NarrativeQA: Summary."]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "response": {"5-Table1-1.png": ["No", "Yes."]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "response": {"6-Table2-1.png": ["No", "BiDAF, Deep Cascade QA*, S-Net+CES2S, BERT+Multi-PGCNet, Selector+CCG*, V-NET, Masque (NLG: single), Masque (NLG: ensemble), Masque (Q&A: single), Masque (Q&A: ensemble)."], "8-Table5-1.png": ["Yes", "54.11, 30.43, 26.13, 18.11, 48.70, 20.98, 18.11, 24.62, 54.74, 21.95, 59.87, 50.09."]}, "referred_figures_names": ["6-Table2-1.png", "8-Table5-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "response": {"8-Table5-1.png": ["Yes", "53.3"]}, "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "response": {"5-Table1-1.png": ["Yes", "An \"answer style\" refers to the type of answer that is generated by a system. The table shows the number of questions used in the experiments for different answer styles, including narrativeQA, summary, and others."]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}