{"747": {"question": "How do the backoff strategies work?", "response": {"4-Figure1-1.png": ["No", "The backoff strategies work by training the foreground model on a smaller, domain-specific dataset and the background model on a larger dataset (e.g., the IMDB movie corpus). During training, both models are trained to reconstruct the correct word from the orthography and context of the individual words using synthetically corrupted inputs. The training process is then stopped after a certain number of iterations or a certain number of epochs, and the model is evaluated on a separate validation set. The backoff strategies are designed to prevent overfitting by allowing the model to train on a smaller dataset and to prevent underfitting by allowing the model to train on a larger dataset. The choice of the number of iterations or epochs is a trade-off between the size of the dataset and the amount of training data available."]}, "referred_figures_names": ["4-Figure1-1.png"], "answer": "In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus."}, "748": {"question": "How do the backoff strategies work?", "response": {"4-Figure1-1.png": ["No", "The backoff strategies work by training the foreground model on a smaller, domain-specific dataset and the background model on a larger dataset (e.g., the IMDB movie corpus). During training, both models are trained to reconstruct the correct word from the orthography and context of the individual words using synthetically corrupted inputs. The training process is then stopped after a certain number of iterations or a certain number of epochs, and the model is evaluated on a separate validation set. The backoff strategies are designed to prevent overfitting by allowing the model to train on a smaller dataset and to prevent underfitting by allowing the model to train on a larger dataset. The choice of the number of iterations or epochs is a trade-off between the size of the dataset and the amount of training data available."]}, "referred_figures_names": ["4-Figure1-1.png"], "answer": "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus."}, "749": {"question": "How do the backoff strategies work?", "response": {"4-Figure1-1.png": ["No", "The backoff strategies work by training the foreground model on a smaller, domain-specific dataset and the background model on a larger dataset (e.g., the IMDB movie corpus). During training, both models are trained to reconstruct the correct word from the orthography and context of the individual words using synthetically corrupted inputs. The training process is then stopped after a certain number of iterations or a certain number of epochs, and the model is evaluated on a separate validation set. The backoff strategies are designed to prevent overfitting by allowing the model to train on a smaller dataset and to prevent underfitting by allowing the model to train on a larger dataset. The choice of the number of iterations or epochs is a trade-off between the size of the dataset and the amount of training data available."]}, "referred_figures_names": ["4-Figure1-1.png"], "answer": "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"}}