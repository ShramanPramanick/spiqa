{"1049": {"question": "what were the baselines?", "response": {"5-Table6-1.png": ["No", "T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T-CNN, T"]}, "referred_figures_names": ["5-Table6-1.png"], "answer": "BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN"}, "1053": {"question": "What BERT models are used?", "response": {"5-Table6-1.png": ["Yes", "Tiny E2E."], "5-Table7-1.png": ["No", "Pre-trained weights BERT-uncased BERT-cased Batch size 8 4 Learning rate (Adam) 2.5 x 10 -2 2.5 x 10 -6 Number of epochs 3 2 Max length (input tokens) 113 219 Dropout rate (last layer) 0.75 0.75"]}, "referred_figures_names": ["5-Table6-1.png", "5-Table7-1.png"], "answer": "BERT-base, BERT-large, BERT-uncased, BERT-cased"}}