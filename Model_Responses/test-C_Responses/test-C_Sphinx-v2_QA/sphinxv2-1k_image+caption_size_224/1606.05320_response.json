{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "response": {"4-Figure3-1.png": ["No", "The HMM models use a combination of hidden state dimensions and feature vectors, which are used to represent the data. The hidden state dimensions are not interpretable, as they are not directly related to the input features. The feature vectors, on the other hand, are interpretable as they contain information about the characteristics of the data."], "4-Figure2-1.png": ["No", "The HMM models use features related to spaces and comments, which are interpretable."]}, "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "response": {"4-Figure3-1.png": ["No", "hidden state dimension"], "4-Figure2-1.png": ["No", "I'm sorry, but I cannot see the image you are referring to. Can you please provide a description of the image or the text that you want me to analyze?"]}, "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "response": {"3-Table1-1.png": ["No", "11"]}, "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}