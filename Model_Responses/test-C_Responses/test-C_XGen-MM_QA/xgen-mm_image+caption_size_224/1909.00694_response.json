{"2": {"question": "What are the results?", "response": {"5-Table3-1.png": ["Yes ", "The results are as follows: AL, ALP, ALP+CA, ALP+CA+CO, ACP, ACP+AL, ACP+AL+CA, ACP+AL+CA+CO, ACP+AL+CA+CO+CA+CO, ACP+AL+CA+CO+CA+CO+CA+CO, ACP+AL+CA+CO+CA+CO+CA+CO+CA+CO, ACP+AL+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO, ACP+AL+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+CA+CO+"], "5-Table4-1.png": ["Yes ", "Training dataset Encoder Acc ACP(6K) Bert 0.876 ACP(AL+CA)CO Bigru 0.879 "]}, "referred_figures_names": ["5-Table3-1.png", "5-Table4-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "response": {"2-Figure1-1.png": ["Yes ", "The polarity of a latter event is automatically identified as either positive or negative, according to the seed lexicon (the positive word is colored red and the negative word blue). "]}, "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "response": {"2-Figure1-1.png": ["Yes ", "The polarity of a latter event is automatically identified as either positive or negative, according to the seed lexicon (the positive word is colored red and the negative word blue). "]}, "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "response": {"4-Table2-1.png": ["Yes ", "299,834 "], "4-Table1-1.png": ["Yes ", "1,000,000. "]}, "referred_figures_names": ["4-Table2-1.png", "4-Table1-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "response": {"4-Table2-1.png": ["Yes ", "299,834 "]}, "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "response": {"5-Table4-1.png": ["Yes ", "0.879 "]}, "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "response": {"2-Figure1-1.png": ["Yes ", "Their model learns using mostly raw data by identifying the polarity of a latter event and propagating it to the former event. The polarity of the latter event is automatically identified as either positive or negative, according to the seed lexicon. The same polarity as the latter event is used for the discourse relation CAUSE, and the reversed polarity for CONCESSION. "]}, "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "response": {"4-Table1-1.png": ["Yes ", "5,000,000 "]}, "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}