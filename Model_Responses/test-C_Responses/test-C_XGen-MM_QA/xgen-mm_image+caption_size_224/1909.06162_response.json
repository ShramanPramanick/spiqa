{"909": {"question": "What is best performing model among author's submissions, what performance it had?", "response": {"3-Table2-1.png": ["Yes ", "MIC-CIS. "]}, "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."}, "910": {"question": "What extracted features were most influencial on performance?", "response": {"4-Table3-1.png": ["Yes ", "BERT, T5-BERT, FastText-Sentiment, BERT-Layout, BERT-Word-Sentiment, BERT-Word-Tropical, BERT-Word-Word-Sentiment, BERT-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Word-Sentiment, BERT-Word-Word-Word-Word"]}, "referred_figures_names": ["4-Table3-1.png"], "answer": "Linguistic"}, "912": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "response": {"4-Table3-1.png": ["Yes ", "Yes. "]}, "referred_figures_names": ["4-Table3-1.png"], "answer": "The best ensemble topped the best single model by 0.029 in F1 score on dev (external)."}, "913": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "response": {"4-Table3-1.png": ["Yes ", "Yes. "], "4-Table4-1.png": ["Yes ", "Yes, ensemble schemes helped in boosting performance by 0.076. "], "3-Table2-1.png": ["Yes ", "Yes. "]}, "referred_figures_names": ["4-Table3-1.png", "4-Table4-1.png", "3-Table2-1.png"], "answer": "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"}, "914": {"question": "Which basic neural architecture perform best by itself?", "response": {"4-Table3-1.png": ["Yes ", "B1. "]}, "referred_figures_names": ["4-Table3-1.png"], "answer": "BERT"}, "915": {"question": "What participating systems had better results than ones authors submitted?", "response": {"3-Table2-1.png": ["Yes ", "MIC-CIS. "]}, "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."}, "916": {"question": "What is specific to multi-granularity and multi-tasking neural arhiteture design?", "response": {"3-Figure1-1.png": ["Yes. ", "The specific feature of multi-granularity and multi-tasking neural architecture design is the presence of a binary classification component at the last hidden layer in the recurrent architecture that jointly performs PFD, FLC, and SLC tasks. "]}, "referred_figures_names": ["3-Figure1-1.png"], "answer": "An output layer for each task"}}