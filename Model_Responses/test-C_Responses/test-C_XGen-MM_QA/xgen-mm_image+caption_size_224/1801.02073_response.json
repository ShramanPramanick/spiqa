{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "response": {"3-Table2-1.png": ["Yes. ", "Yes. "]}, "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "response": {"3-Figure1-1.png": ["Yes ", "Five. "]}, "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "response": {"2-Table1-1.png": ["No. ", "The table shows the comparisons between four corpora for answer selection, including the number of questions, answer candidates, words/token types, average length of questions/answer candidates, and the percentage of overlapping words between question-answer pairs normalized by the questions/answers lengths. "]}, "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}