{"457": {"question": "What do they mean by answer styles?", "response": {"5-Table1-1.png": ["No ", "Narrative, summary, and question. "]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "response": {"5-Table1-1.png": ["Yes ", "Yes. "]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "response": {"8-Table5-1.png": ["Yes ", "RM+RR, RM+RR+AD, Conv+AD, MHGPAMNIC, MPGNGMNIC, DECAProF, MLPGNGMNIC. "], "6-Table2-1.png": ["BIDAF, Net-Cascade QA, Select-PGNE, BERT+PGNE, SERT+PGNE, NLG (single), NLG (ensemble), VNNET, QA (single), QA (ensemble), Human Performance. ", "BIDAF, Net-Cascade QA, Select-PGNE, BERT+PGNE, SERT+PGNE, NLG (single), NLG (ensemble), VNNET, QA (single), QA (ensemble), Human Performance. "]}, "referred_figures_names": ["8-Table5-1.png", "6-Table2-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "response": {"8-Table5-1.png": ["Yes ", "50.53 "]}, "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "response": {"5-Table1-1.png": ["No ", "N/A. "]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}