{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "response": {"3-Table2-1.png": [" Yes", " Yes, the image shows a table with statistics of the silver-standard dataset and the accuracies of answer retrieval in percent. The table also includes information about the robustness of the silver-standard and the coverage of retrieved passages."]}, "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "response": {"3-Figure1-1.png": [" Yes", " The image shows a graph displaying the distribution of question types in percentages. There are several types of questions, including wikiq, sqld, sqad, and infobq. The graph indicates that the majority of questions fall under the sqld and sqad categories. The image also shows a bar graph, which provides a visual representation of the data."]}, "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "response": {"2-Table1-1.png": [" Yes", " The image shows a table with four columns, each representing a different dataset. The table is comparing the answer selection process across these datasets. The datasets are analyzed using various annotation methods, such as annotation for answer extraction, selection, and triggering. The table also shows statistics on the number of questions and answer candidates, as well as the average length of questions and answer candidates. Additionally, the table displays macro average percentages of overlapping words between question-answer pairs, which are normal"]}, "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}