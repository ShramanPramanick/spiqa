{"483": {"question": "Do they conduct any human evaluation?", "response": {"2-Figure1-1.png": [" No", " Yes"]}, "referred_figures_names": ["2-Figure1-1.png"], "answer": "Yes"}, "485": {"question": "How do they enrich the positional embedding with length information", "response": {"4-Figure3-1.png": [" Yes", " The positional embedding is enriched with length information by adding a length encoding to the input. This is done by multiplying the positional encoding with a length encoding, which is then added to the input. This allows the model to understand the length of the input sequence and make decisions accordingly."]}, "referred_figures_names": ["4-Figure3-1.png"], "answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative)."}, "486": {"question": "How do they condition the output to a given target-source class?", "response": {"2-Figure2-1.png": [" Yes", " The image shows a diagram of a neural network, with labels such as \"benign\" and \"malignant\" indicating the different classes the network is designed to recognize. The network is trained to produce outputs of different lengths based on the length ratio classes, which allows it to generate outputs that match the target-source class. This is done by adjusting the network's parameters and using data from the target-source classes to fine-tune the model."]}, "referred_figures_names": ["2-Figure2-1.png"], "answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group."}}