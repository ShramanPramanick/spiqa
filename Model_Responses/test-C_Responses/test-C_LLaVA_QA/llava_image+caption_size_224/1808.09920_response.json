{"856": {"question": "What baseline did they compare Entity-GCN to?", "response": {"6-Table2-1.png": [" Yes", " They compared Entity-GCN to a baseline that uses a pre-trained language model (ELMo) and applies R-GCN to reason among entities in the text."]}, "referred_figures_names": ["6-Table2-1.png"], "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}, "858": {"question": "Did they use a relation extraction method to construct the edges in the graph?", "response": {"3-Figure2-1.png": [" Yes", " Yes"]}, "referred_figures_names": ["3-Figure2-1.png"], "answer": "No"}, "859": {"question": "How did they get relations between mentions?", "response": {"3-Figure2-1.png": [" Yes", " They got relations between mentions by analyzing the text and identifying patterns of co-occurrence, exact matches, and coreference."]}, "referred_figures_names": ["3-Figure2-1.png"], "answer": "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."}, "862": {"question": "What performance does the Entity-GCN get on WIKIHOP?", "response": {"6-Table2-1.png": [" Yes", " The Entity-GCN outperforms recent prior work on the WIKIHOP closed test set and public validation set."]}, "referred_figures_names": ["6-Table2-1.png"], "answer": "During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"}}