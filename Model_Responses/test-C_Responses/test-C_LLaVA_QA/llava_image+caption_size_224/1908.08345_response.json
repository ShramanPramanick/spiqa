{"836": {"question": "What rouge score do they achieve?", "response": {"5-Table1-1.png": [" Yes", " The table shows the average document and summary length, as well as the proportion of novel bi-grams that do not appear in source documents but do appear in the gold summaries. It does not provide a Rouge score."], "6-Table2-1.png": [" Yes", " The table shows the ROUGE F1 score for different models and systems. The ROUGE F1 score is a measure of the performance of a text summarization system. The table shows that the model named \"lead-3\" achieves a score of 42.4. This score indicates that the model is performing well in terms of summarization accuracy."], "7-Table4-1.png": [" Yes", " The table shows the ROUGE F1 results on the XSum test set. The results for comparison systems are taken from the authors' respective papers or obtained on their data by running publicly released software. The table shows the ROUGE F1 results on the XSum test set. The results for comparison systems are taken from the authors' respective papers or obtained on their data by running publicly released software. The table shows the ROUGE F1 results on the XSum test set"], "7-Table3-1.png": [" Yes", " The table shows the ROUGE recall results on the NYT test set. The results for comparison systems are taken from the authors' respective papers or obtained on their data by running publicly released software. The table cells are filled with -- whenever results are not available. The specific ROUGE score achieved is not provided in the image or caption."]}, "referred_figures_names": ["5-Table1-1.png", "6-Table2-1.png", "7-Table4-1.png", "7-Table3-1.png"], "answer": "Best results on unigram:\nCNN/Daily Mail: Rogue F1 43.85\nNYT: Rogue Recall 49.02\nXSum: Rogue F1 38.81"}, "837": {"question": "What rouge score do they achieve?", "response": {"6-Table2-1.png": [" Yes", " The table shows the ROUGE F1 score for different models and systems. The ROUGE F1 score is a measure of the performance of a text summarization system. The table shows that the model named \"lead-3\" achieves a score of 42.4. This score indicates that the model is performing well in terms of summarization accuracy."], "7-Table4-1.png": [" Yes", " The table shows the ROUGE F1 results on the XSum test set. The results for comparison systems are taken from the authors' respective papers or obtained on their data by running publicly released software. The table shows the ROUGE F1 results on the XSum test set. The results for comparison systems are taken from the authors' respective papers or obtained on their data by running publicly released software. The table shows the ROUGE F1 results on the XSum test set"]}, "referred_figures_names": ["6-Table2-1.png", "7-Table4-1.png"], "answer": "Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55"}}