{"585": {"question": "What evaluations did the authors use on their system?", "response": {"9-Table3-1.png": [" Yes", " The authors used evaluations such as precision, recall, and F1 score to assess the performance of their system."], "8-Table1-1.png": [" Yes", " The authors used the NIST Chinese-English translation tasks to evaluate their system. They used the BLEU score, which is case-insensitive, to measure the performance of their proposed models. The BLEU scores are the average scores on all test sets."], "10-Table6-1.png": [" Yes", " The authors used the average cosine similarity of adjacent sentences (coherence) on all test sets to evaluate their system."]}, "referred_figures_names": ["9-Table3-1.png", "8-Table1-1.png", "10-Table6-1.png"], "answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."}}