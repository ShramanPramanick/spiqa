{"801": {"question": "how small of a dataset did they train on?", "response": {"4-Table1-1.png": [" Yes", " The dataset they trained on is quite small, as the numbers in the table are in the thousands."]}, "referred_figures_names": ["4-Table1-1.png"], "answer": "23085 hours of data"}, "802": {"question": "what was their character error rate?", "response": {"6-Table4-1.png": [" Yes", " The character error rate for the models is 7.67%."], "6-Table2-1.png": [" Yes", " The character error rate for the models in the table is not provided."], "6-Table3-1.png": [" Yes", " The character error rate for the 9-layers LSTM model is 2.4%."]}, "referred_figures_names": ["6-Table4-1.png", "6-Table2-1.png", "6-Table3-1.png"], "answer": "2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning."}, "803": {"question": "what was their character error rate?", "response": {"6-Table2-1.png": [" Yes", " The character error rate for the models in the table is not provided."], "6-Table3-1.png": [" Yes", " The character error rate for the 9-layers LSTM model is 2.4%."]}, "referred_figures_names": ["6-Table2-1.png", "6-Table3-1.png"], "answer": "Their best model achieved a 2.49% Character Error Rate."}, "804": {"question": "which lstm models did they compare with?", "response": {"6-Table2-1.png": [" Yes", " They compared the LSTM models with different numbers of layers, including 6, 7, 8, and 9 layers. The models were trained using regular Xavier Initialization, layer-wise training with CE criterion and CE + sMBR criteria. The teacher of the 9-layer model is an 8-layers sMBR model, while the others' teacher is a CE model."], "6-Table3-1.png": [" Yes", " They compared the performance of 9-layers, 2-layers regular-trained, and 2-layers distilled LSTM models."]}, "referred_figures_names": ["6-Table2-1.png", "6-Table3-1.png"], "answer": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."}}