{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "response": {"4-Figure3-1.png": [" Yes", " The HMM models use features such as the presence or absence of certain characters, as well as the order in which they appear. These features are used to predict the hidden state of an individual character in a given sequence. The interpretable nature of these features can vary depending on the specific application and the quality of the training data. In some cases, the features may be highly interpretable, allowing for accurate predictions. In other cases, the features may be less interpretable, leading to less accurate predictions"], "4-Figure2-1.png": [" Yes", " The HMM models use features related to spaces and comments in the text. These features are interpretable, as they are based on the context and structure of the text, allowing the models to learn and recognize patterns in the data."]}, "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "response": {"4-Figure3-1.png": [" No", " The HMMs learn the hidden state dimension of the hybrid algorithm, which is not directly learned by the LSTMs."], "4-Figure2-1.png": [" Yes", " The HMMs learn information about spaces and comments, while the LSTMs focus on learning more general features in the text."]}, "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "response": {"3-Table1-1.png": [" Yes", " The LSTMs have a larger gap in performance compared to the HMMs."]}, "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}