{"747": {"question": "How do the backoff strategies work?", "response": {"4-Figure1-1.png": [" No", " The backoff strategies work by using a larger dataset to train the background model, which helps in reconstructing the correct word from the orthography and context of the individual words. This is done by using synthetically corrupted inputs during training, which helps in improving the accuracy of the model."]}, "referred_figures_names": ["4-Figure1-1.png"], "answer": "In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus."}, "748": {"question": "How do the backoff strategies work?", "response": {"4-Figure1-1.png": [" No", " The backoff strategies work by using a larger dataset to train the background model, which helps in reconstructing the correct word from the orthography and context of the individual words. This is done by using synthetically corrupted inputs during training, which helps in improving the accuracy of the model."]}, "referred_figures_names": ["4-Figure1-1.png"], "answer": "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus."}, "749": {"question": "How do the backoff strategies work?", "response": {"4-Figure1-1.png": [" No", " The backoff strategies work by using a larger dataset to train the background model, which helps in reconstructing the correct word from the orthography and context of the individual words. This is done by using synthetically corrupted inputs during training, which helps in improving the accuracy of the model."]}, "referred_figures_names": ["4-Figure1-1.png"], "answer": "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"}}