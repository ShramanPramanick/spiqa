{"457": {"question": "What do they mean by answer styles?", "response": {"5-Table1-1.png": [" Yes", " The answer styles refer to the different ways in which the participants responded to the questions."]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "response": {"5-Table1-1.png": [" Yes", " Yes"]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "response": {"6-Table2-1.png": [" Yes", " The baselines that Masque is compared against include models such as BERT, BERT+MLP-GN, BERT+MLP-GN+R, BERT+MLP-GN+R+B, BERT+MLP-GN+R+B+I, BERT+MLP-GN+R+B+I+L, BERT+MLP-GN+R+B+I+L+N, BERT+MLP-"], "8-Table5-1.png": [" Yes", " The baselines that Masque is compared against are models B-1, B-4, and B-5."]}, "referred_figures_names": ["6-Table2-1.png", "8-Table5-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "response": {"8-Table5-1.png": [" Yes", " The performance achieved on NarrativeQA is shown in the table."]}, "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "response": {"5-Table1-1.png": [" Yes", " An \"answer style\" refers to the way in which a question is answered. In this context, it likely refers to the format or presentation of the data provided in the table."]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}