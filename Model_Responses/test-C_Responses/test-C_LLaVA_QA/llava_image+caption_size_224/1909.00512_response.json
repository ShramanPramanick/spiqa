{"31": {"question": "What experiments are proposed to test that upper layers produce context-specific embeddings?", "response": {"5-Figure1-1.png": [" No", " The proposed experiments involve comparing the average cosine similarity between uniformly randomly sampled words in different layers of BERT, ELMo, and GPT-2. The idea is to test whether the upper layers of these models produce context-specific embeddings, as the lower layers are more anisotropic. The experiments will be conducted by comparing the similarity between uniformly randomly sampled words in different layers, and observing if there is a significant difference in the similarity between the words in the lower"], "7-Figure3-1.png": [" No", " The proposed experiments involve testing the upper layers of the models (ELMo, BERT, and GPT-2) to see if they produce context-specific embeddings. The image shows a graph with the average intra-sentence similarity, which is the average cosine similarity between each word representation in a sentence and their mean. This statistic reflects how context-specificity manifests in the representation space, and the results show that the models produce different context-specific embedd"], "6-Figure2-1.png": [" Yes", " The experiments proposed to test that upper layers produce context-specific embeddings involve comparing the average self-similarity of uniformly randomly sampled words after adjusting for anisotropy in all three models. The higher the layer, the lower the self-similarity, suggesting that contextualized word representations are more context-specific in higher layers."]}, "referred_figures_names": ["5-Figure1-1.png", "7-Figure3-1.png", "6-Figure2-1.png"], "answer": "They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  \nThey plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.\nThey plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another."}, "32": {"question": "How do they calculate a static embedding for each word?", "response": {"8-Table1-1.png": [" Yes", " The static embeddings for each word are calculated by taking the first principal component of a word's contextualized representations in a given layer. This is done for the contextualizing models such as ELMo, BERT, and GPT-2. The static embeddings created using these models often outperform GloVe and FastText vectors."]}, "referred_figures_names": ["8-Table1-1.png"], "answer": "They use the first principal component of a word's contextualized representation in a given layer as its static embedding."}}