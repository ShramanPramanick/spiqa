{"2": {"question": "What are the results?", "response": {"5-Table3-1.png": [" Yes", " The results show that the ACP model performs well on the ACP test set, with a score of 0.93. The other models also perform well, with the BERT model achieving a score of 0.88."], "5-Table4-1.png": [" Yes", " The results show that BERT, a language model, has been trained on a small labeled training dataset. The model has been trained only with the AL data, which is a part of the full dataset. The performance of the model with the full dataset is not provided in the image, but the results show that the model has been trained on a limited dataset."]}, "referred_figures_names": ["5-Table3-1.png", "5-Table4-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "response": {"2-Figure1-1.png": [" Yes", " Relations are used to propagate polarity by connecting events with a discourse relation, such as CAUSE or CONCESSION. The polarity of the latter event is automatically identified as either positive or negative, and this polarity is then used to determine the polarity of the former event. This process helps to maintain consistency and coherence in the discourse."]}, "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "response": {"2-Figure1-1.png": [" Yes", " Relations are used to propagate polarity by connecting events with a discourse relation, such as CAUSE or CONCESSION. The polarity of the latter event is automatically identified as either positive or negative, and this polarity is then used to determine the polarity of the former event. This process helps to maintain consistency and coherence in the discourse."]}, "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "response": {"4-Table1-1.png": [" Yes", " The Japanese data is quite large, as evidenced by the large number of pairs and the vast amount of data."], "4-Table2-1.png": [" Yes", " The Japanese data is quite large, as it is described as having \"a lot of data\" and \"a large amount of data.\""]}, "referred_figures_names": ["4-Table1-1.png", "4-Table2-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "response": {"4-Table2-1.png": [" Yes", " The Japanese data is quite large, as it is described as having \"a lot of data\" and \"a large amount of data.\""]}, "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "response": {"5-Table4-1.png": [" No", " The table shows the results of training a BERT model on a small labeled training dataset. The table compares the performance of the BERT model trained only with the AL data to the performance of the model trained on the full dataset. The table also shows the improvements in performance when using the proposed approach compared to the basic approach."]}, "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "response": {"2-Figure1-1.png": [" Yes", " The model learns using mostly raw data by analyzing pairs of events and determining their relationship through the use of a discourse relation, such as cause or conceission. The model is able to identify the polarity of a latter event and use that information to determine the polarity of a former event. This process is repeated for multiple pairs of events to build a comprehensive understanding of the relationships between them."]}, "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "response": {"4-Table1-1.png": [" Yes", " The seed lexicon used for training is 5,000,000."]}, "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}