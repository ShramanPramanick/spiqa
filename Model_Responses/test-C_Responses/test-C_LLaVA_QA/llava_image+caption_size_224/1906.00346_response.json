{"1011": {"question": "IS the graph representation supervised?", "response": {"3-Figure2-1.png": [" Yes", " Yes"], "3-Table1-1.png": [" Yes", " Yes"]}, "referred_figures_names": ["3-Figure2-1.png", "3-Table1-1.png"], "answer": "The graph representation appears to be semi-supervised. It is included in the learning pipeline for the medical recommendation, where the attention model is learned. (There is some additional evidence that is unavailable in parsed text)"}, "1012": {"question": "Is the G-BERT model useful beyond the task considered?", "response": {"3-Figure2-1.png": [" Yes", " Yes, the G-BERT model is useful beyond the task considered, as it is a framework that combines ontology embedding, BERT, and fine-tuned classifier. This framework can be applied to various medical code tasks, such as medication recommendation, diagnosis, and more. The model's ability to derive ontology embedding for medical code laid in leaf nodes by cooperating ancestors information makes it versatile and adaptable to different medical scenarios."]}, "referred_figures_names": ["3-Figure2-1.png"], "answer": "There is nothing specific about the approach that depends on medical recommendations. The approach combines graph data and text data into a single embedding."}}