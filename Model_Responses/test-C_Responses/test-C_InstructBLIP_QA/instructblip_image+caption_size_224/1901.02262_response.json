{"457": {"question": "What do they mean by answer styles?", "response": {"5-Table1-1.png": ["1", "ANSWER 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}, "458": {"question": "Is there exactly one \"answer style\" per dataset?", "response": {"5-Table1-1.png": ["1", "No"]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "Yes"}, "459": {"question": "What are the baselines that Masque is compared against?", "response": {"8-Table5-1.png": ["1", "Bauer et al. (2018) Indurthi et al. (2018) Hu et al. (2018) Seo et al. (2017) Tay et al. (2018)"], "6-Table2-1.png": ["1", "Baselines"]}, "referred_figures_names": ["8-Table5-1.png", "6-Table2-1.png"], "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}, "460": {"question": "What is the performance achieved on NarrativeQA?", "response": {"8-Table5-1.png": ["10.5", "eHu et al. (2018)"]}, "referred_figures_names": ["8-Table5-1.png"], "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}, "461": {"question": "What is an \"answer style\"?", "response": {"5-Table1-1.png": ["1", "Question 2"]}, "referred_figures_names": ["5-Table1-1.png"], "answer": "well-formed sentences vs concise answers"}}