{"856": {"question": "What baseline did they compare Entity-GCN to?", "response": {"6-Table2-1.png": ["2019", "Recent prior work without learning any language model to process the input but relying on a pretrained one (ELMo \u2013 without fine-tunning it) and applying R-GCN to reason among entities in the text with coreference for unmasked dataset and without coreference for the masked one."]}, "referred_figures_names": ["6-Table2-1.png"], "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}, "858": {"question": "Did they use a relation extraction method to construct the edges in the graph?", "response": {"3-Figure2-1.png": ["Yes", "Yes"]}, "referred_figures_names": ["3-Figure2-1.png"], "answer": "No"}, "859": {"question": "How did they get relations between mentions?", "response": {"3-Figure2-1.png": ["1", "By analyzing the co-occurrence of entities in the same document."]}, "referred_figures_names": ["3-Figure2-1.png"], "answer": "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."}, "862": {"question": "What performance does the Entity-GCN get on WIKIHOP?", "response": {"6-Table2-1.png": ["10.5", "100.00%"]}, "referred_figures_names": ["6-Table2-1.png"], "answer": "During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"}}