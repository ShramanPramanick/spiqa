{"434": {"question": "What other sentence embeddings methods are evaluated?", "response": {"4-Table1-1.png": ["1", "SemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset"], "6-Table3-1.png": ["10-fold cross validation", "Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic. Methods are trained on two topics, and are evaluated on the third topic."]}, "referred_figures_names": ["4-Table1-1.png", "6-Table3-1.png"], "answer": "GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent"}, "435": {"question": "What other sentence embeddings methods are evaluated?", "response": {"7-Table5-1.png": ["1", "10-fold cross-validation"]}, "referred_figures_names": ["7-Table5-1.png"], "answer": "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."}}