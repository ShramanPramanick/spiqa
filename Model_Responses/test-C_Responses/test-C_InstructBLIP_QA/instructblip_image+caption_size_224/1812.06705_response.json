{"247": {"question": "How do the authors measure performance?", "response": {"7-Table2-1.png": ["1", "Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. \u201cw/\u201d represents \u201cwith\u201d, lines marked with \u201c*\u201d are experiments results from Kobayashi(Kobayashi, 2018)."]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "Accuracy across six datasets"}, "248": {"question": "Does the new objective perform better than the original objective bert is trained on?", "response": {"7-Table2-1.png": ["Yes", "Yes"]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}, "249": {"question": "Are other pretrained language models also evaluated for contextual augmentation? ", "response": {"7-Table2-1.png": ["Yes", "Yes"]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "No"}, "250": {"question": "Do the authors report performance of conditional bert on tasks without data augmentation?", "response": {"7-Table2-1.png": ["Yes", "Yes"]}, "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}}