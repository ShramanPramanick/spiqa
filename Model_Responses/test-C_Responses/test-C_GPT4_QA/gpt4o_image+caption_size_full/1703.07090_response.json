{"801": {"question": "how small of a dataset did they train on?", "referred_figures_indices": [3], "response": "{'Answer': 'The training was performed on a dataset of 16150 data points from Shenma Voice Search and 6935 data points from Amap.'}", "all_figures_names": ["6-Table3-1.png", "6-Table2-1.png", "5-Figure1-1.png", "4-Table1-1.png", "6-Table4-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "23085 hours of data"}, "802": {"question": "what was their character error rate?", "referred_figures_indices": [1, 0, 2], "response": "{'Answer': 'The character error rates (CER) vary depending on the model and training method used:\n\n- Image 0: \n  - 9-layers LSTM: 2.49%\n  - 2-layers regular-trained LSTM: 3.06%\n  - 2-layers distilled LSTM: 2.63%\n\n- Image 1:\n  - Shenma model: 7.87%\n  - Amap CE + sMBR: 6.81%\n  - Shenma model + Amap sMBR: 6.26%\n\n- Image 2:\n  - ", "all_figures_names": ["6-Table3-1.png", "6-Table4-1.png", "6-Table2-1.png", "5-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["6-Table4-1.png", "6-Table3-1.png", "6-Table2-1.png"], "answer": "2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning."}, "803": {"question": "what was their character error rate?", "referred_figures_indices": [1, 2], "response": "{'Answer': 'Their character error rate (CER) was 2.49% for the 9-layers LSTM model according to Image 1.'}", "all_figures_names": ["5-Figure1-1.png", "6-Table3-1.png", "6-Table2-1.png", "4-Table1-1.png", "6-Table4-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Their best model achieved a 2.49% Character Error Rate."}, "804": {"question": "which lstm models did they compare with?", "referred_figures_indices": [4, 3], "response": "{'Answer': '9-layers LSTM, 2-layers regular-trained LSTM, 2-layers distilled LSTM'}", "all_figures_names": ["5-Figure1-1.png", "6-Table4-1.png", "4-Table1-1.png", "6-Table2-1.png", "6-Table3-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."}}