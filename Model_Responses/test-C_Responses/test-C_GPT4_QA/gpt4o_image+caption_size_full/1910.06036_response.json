{"654": {"question": "How they extract \"structured answer-relevant relation\"?", "referred_figures_indices": [2], "response": "{'Answer': 'They extract \"structured answer-relevant relation\" using a combination of sentence parsing and relation extraction techniques, as seen in Image 0. The process involves identifying entities, actions, and attributes from sentences, then linking them into meaningful relationships with confidence scores. This structured information is then used to match and generate questions based on the extracted relations, which is illustrated in Image 1 and Image 2.'}", "all_figures_names": ["3-Figure2-1.png", "7-Figure4-1.png", "1-Figure1-1.png", "2-Table1-1.png", "6-Table4-1.png", "7-Table5-1.png", "3-Table2-1.png", "4-Figure3-1.png"], "referred_figures_names": ["1-Figure1-1.png"], "answer": "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."}, "656": {"question": "How big are significant improvements?", "referred_figures_indices": [5], "response": "{'Answer': 'Significant improvements are around 2-3 BLEU points, 0.4-0.5 METEOR points, and 3-5 ROUGE-L points.'}", "all_figures_names": ["8-Figure5-1.png", "2-Table1-1.png", "4-Figure3-1.png", "5-Table3-1.png", "3-Figure2-1.png", "6-Table4-1.png", "3-Table2-1.png", "7-Figure4-1.png"], "referred_figures_names": ["6-Table4-1.png"], "answer": "Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"}}