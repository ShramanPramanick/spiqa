{"1363": {"question": "Could you tell me more about the metrics used for performance evaluation?", "referred_figures_indices": [2], "response": "{'Answer': 'The metrics used for performance evaluation include Pearson correlation coefficient (Pearson) for tasks such as sentence similarity (MedSTS, BIOSSES), F1 score for Named Entity Recognition (NER) tasks (BC5CDR-disease, BC5CDR-chemical, ShARe/CLEFE), micro F1 score for relation extraction tasks (DDI, ChemProt, i2b2 2010), and accuracy (acc) for document classification (HoC) and inference tasks (MedNLI).'}", "all_figures_names": ["4-Table2-1.png", "5-Table3-1.png", "3-Table1-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy"}}