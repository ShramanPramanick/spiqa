{"2": {"question": "What are the results?", "referred_figures_indices": [3, 5], "response": "{'Answer': 'The BiGRU model trained with AL+CA+CO achieved an accuracy of 0.879 for small labeled training data (Table 4). For the ACP test set (Table 3), BERT trained with ACP only achieved the highest accuracy of 0.933.'}", "all_figures_names": ["4-Table1-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table4-1.png", "4-Table2-1.png", "5-Table3-1.png"], "referred_figures_names": ["5-Table4-1.png", "5-Table3-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [3], "response": "{'Answer': 'Relations are used to propagate polarity by focusing on pairs of events connected with discourse relations such as CAUSE or CONCESSION. In CAUSE relations, the polarity of the latter event is used directly for the former event, whereas in CONCESSION relations, the polarity is reversed.'}", "all_figures_names": ["5-Table4-1.png", "5-Table3-1.png", "4-Table2-1.png", "2-Figure1-1.png", "4-Table1-1.png", "5-Table5-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [2], "response": "{'Answer': 'Relations are used to propagate polarity by connecting former and latter events with discourse relations such as CAUSE or CONCESSION. In the case of CAUSE, the polarity of the latter event is propagated to the former event with the same polarity. In the case of CONCESSION, the polarity is propagated with the reversed polarity.'}", "all_figures_names": ["5-Table4-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table3-1.png", "4-Table2-1.png", "5-Table5-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [2, 4], "response": "{'Answer': 'The Japanese data consists of 11,000,000 pairs (1,000,000 Automatically Labeled Pairs, 5,000,000 Cause Pairs, and 5,000,000 Concession Pairs).' }", "all_figures_names": ["5-Table5-1.png", "2-Figure1-1.png", "4-Table1-1.png", "5-Table4-1.png", "4-Table2-1.png", "5-Table3-1.png"], "referred_figures_names": ["4-Table1-1.png", "4-Table2-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [1], "response": "{'Answer': 'The Japanese data consists of 11,000,000 pairs.'}", "all_figures_names": ["2-Figure1-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table3-1.png", "5-Table5-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [1], "response": "{'Answer': 'The improvements using the proposed approach (AL+CA+CO) compared to the basic approach (only AL) are as follows: For BERT, the accuracy improved from 0.863 to 0.866 on small labeled data, showing a modest improvement. For BiGRU, accuracy improved from 0.830 to 0.879, showing a significant improvement.'}", "all_figures_names": ["5-Table5-1.png", "5-Table4-1.png", "4-Table1-1.png", "4-Table2-1.png", "5-Table3-1.png", "2-Figure1-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [3], "response": "{'Answer': 'Their model learns using mostly raw data by propagating event polarity through different event pairs connected by discourse relations (CAUSE or CONCESSION). Automatic labeling (AL) assigns polarity based on a seed lexicon and propagates it to other events, while CA and CO pairs encourage the same or reversed polarity, respectively. This method leverages large amounts of automatically labeled and related data (AL, CA, CO) to improve learning and performance.'}", "all_figures_names": ["4-Table2-1.png", "5-Table5-1.png", "5-Table4-1.png", "2-Figure1-1.png", "5-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [2], "response": "{'Answer': 'The size of the seed lexicon is not explicitly mentioned in the provided images and captions, so the specific size cannot be determined directly from this information.'}", "all_figures_names": ["5-Table4-1.png", "4-Table2-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table3-1.png", "5-Table5-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}