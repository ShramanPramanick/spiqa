{"204": {"question": "What is difference in peformance between proposed model and state-of-the art on other question types?", "referred_figures_indices": [2], "response": "{'Answer': 'The proposed model outperforms the state-of-the-art models, particularly in the multi-span and date question types, as evidenced in the development and test results. In the multi-span category, the proposed model achieves an EM/F1 of 54.78/77.86 compared to MTMSN_LARGE at 25.1/62.8; and in the date category, it achieves 57.72/64.61 compared to MTMSN_LARGE at 55.7/69.0. Overall, the proposed model achieves higher EM and F1 scores across all question types on the development", "all_figures_names": ["7-Table4-1.png", "6-Table3-1.png", "6-Table2-1.png", "6-Table1-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1."}, "205": {"question": "What is the performance of proposed model on entire DROP dataset?", "referred_figures_indices": [2], "response": "{'Answer': 'The proposed model achieves an Exact Match (EM) score of 77.63 and an F1 score of 80.73 on the entire DROP dataset as evidenced from Image 2.'}", "all_figures_names": ["7-Table4-1.png", "6-Table2-1.png", "6-Table3-1.png", "6-Table1-1.png"], "referred_figures_names": ["6-Table3-1.png"], "answer": "The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev"}}