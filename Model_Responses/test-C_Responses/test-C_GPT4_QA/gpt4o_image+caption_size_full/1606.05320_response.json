{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "referred_figures_indices": [1, 3], "response": "{'Answer': 'The HMM models use state probabilities as features. The interpretability of these features can be enhanced when visualized, as shown for both HMM and LSTM components on Linux data; they reveal meaningful patterns related to spaces and comments. Additionally, decision trees can predict individual hidden states of the hybrid model, further aiding interpretability.'}", "all_figures_names": ["2-Figure1-1.png", "4-Figure2-1.png", "3-Table1-1.png", "4-Figure3-1.png"], "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "referred_figures_indices": [3, 0], "response": "{'Answer': 'HMMs learn complementary features in the text related to spaces and comments that LSTMs do not.'}", "all_figures_names": ["4-Figure3-1.png", "2-Figure1-1.png", "3-Table1-1.png", "4-Figure2-1.png"], "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "referred_figures_indices": [1], "response": "{'Answer': 'The gap in performance between the HMMs and the LSTMs decreases as the LSTM dimension increases. For example, on the Linux text data, HMM with 10 states has a loglikelihood (LL) of -2.76, while LSTM with a dimension of 5 has an LL of -2.54, and with a dimension of 10, it achieves -2.17. Similarly, on the Shakespeare text data, HMM with 10 states has an LL of -2.69, while LSTM with a dimension of 5 has an LL of -2.41", "all_figures_names": ["4-Figure2-1.png", "3-Table1-1.png", "4-Figure3-1.png", "2-Figure1-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}