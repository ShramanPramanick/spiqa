{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "referred_figures_indices": [3], "response": "{'Answer': 'Yes'}", "all_figures_names": ["4-Table3-1.png", "3-Figure1-1.png", "3-Figure2-1.png", "3-Table2-1.png", "2-Table1-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "referred_figures_indices": [3], "response": "{'Answer': '8'}", "all_figures_names": ["2-Table1-1.png", "3-Table2-1.png", "4-Table3-1.png", "3-Figure1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "referred_figures_indices": [2], "response": "{\n  'Answer': 'They analyze contextual similarities across datasets by comparing the distributions of answer categories (Image 0), question types (Image 4), and key metrics for answer selection and retrieval (Images 1, 2, and 3) across different datasets (WikiQA, SelQA, SQuAD, InfoboxQA).'\n}", "all_figures_names": ["3-Figure2-1.png", "4-Table3-1.png", "2-Table1-1.png", "3-Table2-1.png", "3-Figure1-1.png"], "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}