{"248": {"question": "How were the human judgements assembled?", "referred_figures_indices": [0, 4], "response": "{'Image': 4, 'Rationale': 'It provides details on the judgement tasks related to human annotations including adequacy, fluency, and ranking with their corresponding agreement metrics.'}\n\nThe answer is: The human judgements were assembled for three tasks: adequacy, fluency, and ranking. Specific metrics such as P(A), P(E), and K were used to measure the agreement of these tasks among annotators.", "all_figures_names": ["12-Table3-1.png", "17-Figure11-1.png", "8-Figure2-1.png", "14-Figure8-1.png", "11-Table2-1.png", "8-Figure3-1.png", "9-Figure4-1.png", "15-Figure9-1.png"], "referred_figures_names": ["12-Table3-1.png", "11-Table2-1.png"], "answer": "50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale."}}