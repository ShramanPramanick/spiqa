{"812": {"question": "By how much did they improve?", "referred_figures_indices": [3, 4], "response": "{'Image': 2, 'Rationale': 'Image 2 shows the F1 scores for different methods, indicating the performance improvement directly.'} \n\nThe answer is: 0.230 (improvement from the SVM_cs score of 0.251 to the biLSTM+Multitask score of 0.481).", "all_figures_names": ["3-Table2-1.png", "2-Table1-1.png", "5-Figure2-1.png", "3-Figure1-1.png", "5-Table3-1.png"], "referred_figures_names": ["3-Figure1-1.png", "5-Table3-1.png"], "answer": "They decrease MAE in 0.34"}, "813": {"question": "What dataset did they use?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'Image 1 provides information about the cardinality and class distributions of the datasets used, which is directly related to the question.'}\n\nThe answer is : The dataset used is described in \"Table 2 Cardinality and class distributions of the datasets.\"", "all_figures_names": ["5-Table3-1.png", "3-Table2-1.png", "5-Figure2-1.png", "2-Table1-1.png", "3-Figure1-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": " high-quality datasets  from SemEval-2016 \u201cSentiment Analysis in Twitter\u201d task"}}