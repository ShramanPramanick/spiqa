{"654": {"question": "How they extract \"structured answer-relevant relation\"?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'Image 1 provides a clear example of structured answer-relevant relation with a definition and an example sentence, showing how it is derived from the text.'}\n\nThe answer is: Structured answer-relevant relations are extracted by identifying relevant information from a given sentence and structuring it into a format that directly connects the question with the answer. In the example, the relation \"The daily mean temperature in January; is; 32.6 \u00b0F (0.3 \u00b0C)\" is derived from the sentence about New York City's temperature. This relation helps in forming accurate questions and predictions about the", "all_figures_names": ["6-Table4-1.png", "1-Figure1-1.png", "3-Table2-1.png", "7-Figure4-1.png", "3-Figure2-1.png", "8-Figure5-1.png", "4-Figure3-1.png", "5-Table3-1.png"], "referred_figures_names": ["1-Figure1-1.png"], "answer": "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."}, "656": {"question": "How big are significant improvements?", "referred_figures_indices": [7], "response": "{'Image': 2, 'Rationale': 'Provides a comparison of improvements between models, specifically mentioning improvement percentages'}\n\nThe answer is: Significant improvements are measured as percentage increases in performance metrics, ranging from approximately 1.31% to 7.09%, depending on the specific metric and context, as shown in the table.", "all_figures_names": ["3-Figure2-1.png", "1-Figure1-1.png", "7-Table5-1.png", "7-Figure4-1.png", "4-Figure3-1.png", "8-Figure5-1.png", "5-Table3-1.png", "6-Table4-1.png"], "referred_figures_names": ["6-Table4-1.png"], "answer": "Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"}}