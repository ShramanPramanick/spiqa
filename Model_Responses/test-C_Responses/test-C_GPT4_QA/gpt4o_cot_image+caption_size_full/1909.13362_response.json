{"684": {"question": "What are the datasets used for the task?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'Image 2 provides a clear list of datasets used and their details such as the language, word count, and encoding, which is directly relevant to the question about the datasets used for the task.'}\n\nThe answer is: The datasets used for the task are English CELEX, Dutch CELEX, Festival, OpenLexique, IIT-Guwahati, and E-Hitz.", "all_figures_names": ["6-TableIII-1.png", "5-TableII-1.png", "5-TableI-1.png", "7-TableV-1.png", "2-Figure1-1.png", "6-TableIV-1.png", "3-Figure2-1.png"], "referred_figures_names": ["5-TableI-1.png"], "answer": "Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)"}, "685": {"question": "What is the accuracy of the model for the six languages tested?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'Image 4 provides the specific accuracy values of the model for the six languages tested, which directly answers the question.'}\n\nThe answer is: The accuracy of the model for the six languages tested are as follows: English CELEX: 98.5 \u00b1 0.1%, Dutch CELEX: 99.47 \u00b1 0.04%, Festival: 99.990 \u00b1 0.005%, OpenLexique: 99.98 \u00b1 0.01%, IIT-Guwahati: 94.9 \u00b1 0.3%, and E-Hitz:", "all_figures_names": ["3-Figure2-1.png", "5-TableI-1.png", "2-Figure1-1.png", "7-TableV-1.png", "6-TableIII-1.png", "6-TableIV-1.png", "5-TableII-1.png"], "referred_figures_names": ["6-TableIII-1.png"], "answer": "Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)"}, "686": {"question": "Which models achieve state-of-the-art performances?", "referred_figures_indices": [5], "response": "{'Image': 6, 'Rationale': 'Image 6 directly compares the reported accuracies of different models, specifically highlighting performance metrics to identify state-of-the-art models.'}\n\nThe answer is: The models achieving state-of-the-art performances are HMM-SVM with 98.86% accuracy and the BiLSTM-CNN-CRF (Base) with 98.5 \u00b1 0.1% accuracy.", "all_figures_names": ["3-Figure2-1.png", "6-TableIII-1.png", "5-TableI-1.png", "7-TableV-1.png", "2-Figure1-1.png", "5-TableII-1.png", "6-TableIV-1.png"], "referred_figures_names": ["5-TableII-1.png"], "answer": "CELEX (Dutch and English) - SVM-HMM\nFestival, E-Hitz and OpenLexique - Liang hyphenation\nIIT-Guwahat - Entropy CRF"}}