{"883": {"question": "Do the methods that work best on academic papers also work best on Wikipedia?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'Image 1 provides comparative experimental results for various models on both academic papers and Wikipedia articles, which directly addresses the effectiveness of methods on different datasets.'}\n\nThe answer is: The methods that work best on academic papers do not necessarily work best on Wikipedia, as indicated by the different performance results shown in the table. For instance, the \"Joint\" model achieves the highest accuracy on Wikipedia, while other methods may perform better on academic papers, suggesting dataset-specific effectiveness.", "all_figures_names": ["4-Table2-1.png", "4-Table1-1.png", "1-Figure1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "No"}, "884": {"question": "What is their system's absolute accuracy?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'Image 2 provides experimental results, including accuracy percentages for different systems, which is directly relevant to determining absolute accuracy.'}\nThe answer is: 59.4%", "all_figures_names": ["1-Figure1-1.png", "3-Figure2-1.png", "4-Table1-1.png", "4-Table2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"}, "887": {"question": "Which languages do they use?", "referred_figures_indices": [0], "response": "{'Image': 1, 'Rationale': 'The experimental results table lists various datasets and corresponding results for different models, showing language-related identifiers like cs.ai, cs.cl, cs.lg, which indicate specific languages or subjects related to computer science.'}\n\nThe answer is: The languages used are cs.ai, cs.cl, and cs.lg.", "all_figures_names": ["4-Table2-1.png", "4-Table1-1.png", "1-Figure1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "English"}, "888": {"question": "How large is their data set?", "referred_figures_indices": [0], "response": "{'Image': 1, 'Rationale': 'Image 1 shows a confusion matrix with the counts for different categories, which can be used to infer the size of the data set.'}\n\nThe answer is: 3183 (sum of all entries in the confusion matrix).", "all_figures_names": ["4-Table1-1.png", "4-Table2-1.png", "1-Figure1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "a sample of  29,794 wikipedia articles and 2,794 arXiv papers "}}