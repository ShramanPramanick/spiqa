{"2": {"question": "What are the results?", "referred_figures_indices": [2, 0], "response": "{'Image': 0, 'Rationale': 'This image provides a detailed performance comparison of various models, specifically addressing the results of different training datasets and encoders on accuracy.'}\n\nThe answer is: According to the performance data, the model with the ACP dataset using BERT as the encoder achieves the highest accuracy of 0.933.", "all_figures_names": ["5-Table3-1.png", "5-Table5-1.png", "5-Table4-1.png", "2-Figure1-1.png", "4-Table2-1.png", "4-Table1-1.png"], "referred_figures_names": ["5-Table4-1.png", "5-Table3-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'This image provides a detailed explanation on how relations like CAUSE and CONCESSION are used to propagate polarity between events.'}\n\nThe answer is: Polarity is propagated between events through the use of discourse relations such as CAUSE and CONCESSION. In the case of the CAUSE relation, the polarity of the latter event is propagated to the former event directly. For the CONCESSION relation, the polarity of the latter event is reversed and then propagated to the former event. This propagation process can be encouraged to either maintain or reverse polarity based on the specific discourse relation connecting the events", "all_figures_names": ["5-Table5-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table3-1.png", "2-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'This image provides an explanation of how polarity is propagated through different types of relations (CAUSE and CONCESSION) and explains the mechanism of propagating polarity in event pairs.'}\n\nThe answer is: Relations such as CAUSE and CONCESSION are used to propagate polarity in pairs of events. For CAUSE relations, the polarity of the latter event is propagated to the former event with the same polarity. For CONCESSION relations, the polarity is reversed when propagated from the latter event to the former event.", "all_figures_names": ["4-Table2-1.png", "5-Table5-1.png", "2-Figure1-1.png", "5-Table4-1.png", "5-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [3, 5], "response": "{'Image': 5, 'Rationale': 'This table provides the number of pairs in each dataset category, giving a clear indication of the data size.'}\n\nThe answer is: The Japanese data consists of 1,000,000 Automatically Labeled (AL) pairs, 5,000,000 Cause (CA) pairs, and 5,000,000 Concession (CO) pairs, making a total of 11,000,000 pairs.", "all_figures_names": ["5-Table5-1.png", "5-Table3-1.png", "5-Table4-1.png", "4-Table2-1.png", "2-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png", "4-Table1-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [3], "response": "{'Image': 5, 'Rationale': 'Image 5 contains statistics on the number of pairs in the AL, CA, and CO datasets, providing a clear quantification of the size of the Japanese data.'}\n\nThe answer is : The Japanese data contains 11,000,000 pairs (1,000,000 AL pairs, 5,000,000 CA pairs, and 5,000,000 CO pairs).", "all_figures_names": ["2-Figure1-1.png", "5-Table5-1.png", "5-Table4-1.png", "4-Table2-1.png", "5-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Rationale': 'Displays accuracy improvements with smaller labeled datasets enhanced by proposed approach vs. basic approach.'}\n\nThe answer is : The improvements from the proposed approach over the basic approach are 0.010 (from 0.876 to 0.886) for BERT and 0.049 (from 0.830 to 0.879) for BiGRU when using smaller labeled data.", "all_figures_names": ["5-Table3-1.png", "4-Table2-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table4-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Rationale': 'It provides an overview of the method used to train the model, specifically explaining how the model uses automatically labeled pairs, cause pairs, and concession pairs to learn from raw data.'}\n\nThe answer is: The model learns using mostly raw data by focusing on pairs of events connected with discourse relations, such as CAUSE or CONCESSION. For AL pairs, the polarity of a latter event is automatically identified and propagated to the former event. For CA and CO pairs, the model encourages the polarities to be the same or reversed depending on the discourse relation, thereby learning the data's structure", "all_figures_names": ["4-Table2-1.png", "5-Table5-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table3-1.png", "5-Table4-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [4], "response": "{'Image': 0, 'Rationale': 'Image 0 explains the process of determining the polarity of events and mentions the usage of a seed lexicon for the positive and negative words, which is directly related to the size of the seed lexicon used for training.'}\n\nThe answer is: The size of the seed lexicon used for training is detailed in the method description, but an exact numerical size is not provided in the available images.", "all_figures_names": ["2-Figure1-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table3-1.png", "4-Table1-1.png", "5-Table5-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}