{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'Image 2 provides details on the statistics of the silver-standard dataset and the accuracies of answer retrieval, which likely involve their indexing-based method.'}\n\nThe answer is: Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset.", "all_figures_names": ["2-Table1-1.png", "3-Figure1-1.png", "3-Table2-1.png", "4-Table3-1.png", "3-Figure2-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'Image 1 shows the distributions of question types in the datasets, directly addressing the question about the number of question types found in the datasets.'}\n\nThe answer is: 7 question types (What, How, Who, When, Where, Which, Why).", "all_figures_names": ["2-Table1-1.png", "3-Figure1-1.png", "4-Table3-1.png", "3-Figure2-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "referred_figures_indices": [4], "response": "{'Image': 0, 'Rationale': 'This table shows how models trained on different datasets perform on other datasets, indicating how contextual similarities are analyzed and evaluated across datasets.'}\n\nThe answer is: Contextual similarities across datasets are analyzed by training models on one dataset and evaluating their performance on other datasets. Metrics like Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and F1 score are used to compare and analyze the models' effectiveness in transferring knowledge across different datasets.", "all_figures_names": ["4-Table3-1.png", "3-Table2-1.png", "3-Figure1-1.png", "3-Figure2-1.png", "2-Table1-1.png"], "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}