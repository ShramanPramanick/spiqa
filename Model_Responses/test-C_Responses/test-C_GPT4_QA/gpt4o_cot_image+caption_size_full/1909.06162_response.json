{"909": {"question": "What is best performing model among author's submissions, what performance it had?", "referred_figures_indices": [4], "response": "{'Image': 2, 'Rationale': 'This image provides the performances of different models along with their F1-scores, which is essential to determine the best-performing model and its performance.'}\n\nThe answer is : The best performing model among the author's submissions is the ensemble+ of {r4, r7, r12} from each Fold1-5 with relax-voting (M \u2265 30%), which had an F1 score of .679 on Dev (external).", "all_figures_names": ["2-Table1-1.png", "4-Table4-1.png", "4-Table3-1.png", "3-Figure1-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."}, "910": {"question": "What extracted features were most influencial on performance?", "referred_figures_indices": [3], "response": "{'Image': 1, 'Rationale': 'This image contains a table displaying the effect of different features (e.g., Polarity, POS, NER, BERTSentEmb) and ensembles on the performance metrics (F1, Precision, Recall), which directly addresses the question about which extracted features were most influential on performance.'}\n\nThe answer is: The features Polarity, POS, NER, and BERTSentEmb were influential on performance, as indicated by the increase in F1 scores when these features were used. The ensemble of these features across different folds further improved performance.", "all_figures_names": ["2-Table1-1.png", "4-Table4-1.png", "3-Figure1-1.png", "4-Table3-1.png", "3-Table2-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "Linguistic"}, "912": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'This image contains a detailed table comparing the performance of different models and their ensemble schemes, clearly displaying the improvement ensembles provide.'}\n\nThe answer is: Ensemble schemes did help in boosting performance. For instance, ensemble models like relax-voting (Image 1: r19) resulted in an improvement of F1 score to 0.673 from individual model scores such as logisticReg + FastTextSentEmb (F1 = 0.606) and BERT + Fine-tune (F1 = 0.662). Thus, ensemble techniques showed a significant improvement in performance.", "all_figures_names": ["3-Table2-1.png", "4-Table3-1.png", "4-Table4-1.png", "2-Table1-1.png", "3-Figure1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "The best ensemble topped the best single model by 0.029 in F1 score on dev (external)."}, "913": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [4, 0, 3], "response": "{'Image': 0, 'Rationale': 'Image 0 contains comparative performance metrics of various classifiers and ensemble schemes, including F1, precision, and recall scores, which are essential to assess the effectiveness of ensemble methods.'}\n\nThe answer is: Ensemble schemes did help in boosting performance. For example, the ensemble of (r3, r6, r12) with majority-voting (M > 50%) increased the F1 score from .664 (r11) and .662 (r12) up to .666 in Dev (internal), Fold1. Similarly, for each Fold1-5, the majority-v", "all_figures_names": ["4-Table3-1.png", "2-Table1-1.png", "3-Figure1-1.png", "4-Table4-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Table2-1.png", "4-Table3-1.png", "4-Table4-1.png"], "answer": "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"}, "914": {"question": "Which basic neural architecture perform best by itself?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Rationale': 'Image 0 contains detailed performance metrics (F1, precision, recall) for various basic neural architectures on internal and external datasets, which directly answers the question about the performance of different architectures.'}\n\nThe answer is : The BERT fine-tuning architecture (r11) performs the best by itself, especially on the external dataset, where it achieves the highest F1 score of .654.", "all_figures_names": ["4-Table3-1.png", "3-Table2-1.png", "4-Table4-1.png", "3-Figure1-1.png", "2-Table1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "BERT"}, "915": {"question": "What participating systems had better results than ones authors submitted?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'Image 2 directly compares the results of multiple participants, making it the most relevant to determine which systems had better results than the ones the authors submitted.'}\n\nThe answer is: For the SLC task, Ituorp, ProperGander, and YMJA had better results than the authors' submitted system MIC-CIS. For the FLC task, newspeak and Antiganda had better results than MIC-CIS.", "all_figures_names": ["4-Table4-1.png", "3-Figure1-1.png", "3-Table2-1.png", "2-Table1-1.png", "4-Table3-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."}, "916": {"question": "What is specific to multi-granularity and multi-tasking neural arhiteture design?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'Image 4 provides a detailed diagram showing the multi-tasking structure and multi-granularity features of the neural architecture, which directly addresses the question.'}\n\nThe answer is: Multi-granularity and multi-tasking neural architecture design involves integrating various granular levels of data analysis (such as word-level, sentence-level, and document-level) and performing multiple tasks simultaneously (such as classification and sequence tagging). This approach is exemplified by systems that use techniques like LSTM-CRF for sequence tagging, word embeddings from models like BERT, and ensemble methods combining classifiers like Logistic Regression,", "all_figures_names": ["3-Table2-1.png", "4-Table4-1.png", "2-Table1-1.png", "4-Table3-1.png", "3-Figure1-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "An output layer for each task"}}