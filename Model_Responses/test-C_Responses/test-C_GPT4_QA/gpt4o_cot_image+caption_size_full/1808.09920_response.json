{"856": {"question": "What baseline did they compare Entity-GCN to?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Rationale': 'Image 7 directly lists the baseline models compared to Entity-GCN, providing the necessary information to answer the question.'}\n\nThe answer is: FastQA, BiDAF, Coref-GRU, MHPGM, Weaver/Jenga, and MHQA-GRN.", "all_figures_names": ["12-Table6-1.png", "3-Figure2-1.png", "5-Table1-1.png", "7-Table3-1.png", "1-Figure1-1.png", "8-Table4-1.png", "13-Figure3-1.png", "6-Table2-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}, "858": {"question": "Did they use a relation extraction method to construct the edges in the graph?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'Image 1 shows a graph where edges are constructed based on relations such as co-occurrence, exact match, and coreference, directly related to the question about relation extraction.'}\n\nThe answer is: Yes.", "all_figures_names": ["1-Figure1-1.png", "3-Figure2-1.png", "8-Table4-1.png", "6-Table2-1.png", "12-Table6-1.png", "7-Table3-1.png", "13-Figure3-1.png", "5-Table1-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "No"}, "859": {"question": "How did they get relations between mentions?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'Image 4 contains a graphical representation of the relations between mentions, showing how nodes (mentions) are connected by different types of edges, which is directly related to understanding how relations between mentions are established.'}\n\nThe answer is: Relations between mentions are determined by organizing supporting documents as a graph where nodes represent mentions of either candidate entities or query entities. These nodes are connected by different types of relations: co-occurrence in the same document (solid edges), exact matches (dashed edges), and coreference (bold-red line).", "all_figures_names": ["5-Table1-1.png", "1-Figure1-1.png", "13-Figure3-1.png", "6-Table2-1.png", "3-Figure2-1.png", "11-Table5-1.png", "7-Table3-1.png", "12-Table6-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."}, "862": {"question": "What performance does the Entity-GCN get on WIKIHOP?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'Image 1 contains a table that directly lists the performance results of the Entity-GCN on the WIKIHOP dataset, providing the accuracy metrics needed to answer the question.'}\n\nThe answer is : The performance of the Entity-GCN on WIKIHOP is 67.6 for the unmasked test set without coreference, 66.4 for the unmasked test set with coreference, and 71.2 for the unmasked test set with an ensemble of 5 models. For the masked test set with an ensemble of 5 models, it is 71", "all_figures_names": ["3-Figure2-1.png", "6-Table2-1.png", "13-Figure3-1.png", "8-Table4-1.png", "12-Table6-1.png", "1-Figure1-1.png", "5-Table1-1.png", "11-Table5-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"}}