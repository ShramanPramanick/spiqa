{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "referred_figures_indices": [1], "response": "{'Answer': 'The provided images do not include explicit information about the employment of an indexing-based method to create a sample of a QA Wikipedia dataset.'}", "all_figures_names": ["3-Figure2-1.png", "3-Table2-1.png", "4-Table3-1.png", "2-Table1-1.png", "3-Figure1-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "referred_figures_indices": [3], "response": "The question is asking for the number of question types found in the datasets analyzed according to the input images. By analyzing the bar charts in images 3 and 4, one can categorize the types of questions based on the starting word (e.g., \"What,\" \"How,\" \"Who,\" etc.) and the type of expected answer (e.g., \"Numeric,\" \"Person,\" \"Entity,\" etc.).\n\nFrom image 3, we can see seven question types based on the starting word:\n1. What\n2. How\n3. Who\n4. When\n5. Where\n6. Which\n7. Why\n\n", "all_figures_names": ["4-Table3-1.png", "3-Table2-1.png", "2-Table1-1.png", "3-Figure1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "referred_figures_indices": [2], "response": "{'Answer': 'The provided images do not explicitly explain the methodology for analyzing contextual similarities across datasets. However, they do provide statistical data and results from different question-answer datasets, which suggest a comparison based on evaluation metrics such as MAP (Mean Average Precision), MRR (Mean Reciprocal Rank), and F1 scores. These metrics could be used to understand the performance of models on various datasets and infer contextual similarities based on the results.'}", "all_figures_names": ["3-Table2-1.png", "3-Figure2-1.png", "2-Table1-1.png", "3-Figure1-1.png", "4-Table3-1.png"], "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}