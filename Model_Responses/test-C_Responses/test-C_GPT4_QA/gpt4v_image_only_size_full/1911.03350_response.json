{"1381": {"question": "How they evaluate quality of generated output?", "referred_figures_indices": [0], "response": "{'Answer': 'They evaluate the quality of generated output using a combination of human judgment and automated metrics. Human judgments are reflected in the scores for Answerability, Correctness, External Knowledge, Relevance, and Soundness, where a score closer to the \"human\" baseline indicates better quality. Automated metrics include BLEU scores (BLEU1 to BLEU4), Self-BLEU scores (Self-B1 to Self-B4), and QA-based metrics (QA_source, QA_context) for assessing the quality of questions generated in question-answering systems. Additionally, they analyse the distribution of question types and the effect of different", "all_figures_names": ["9-Table4-1.png", "4-Table1-1.png", "7-Table3-1.png", "8-Figure1-1.png", "7-Table2-1.png", "9-Figure2-1.png"], "referred_figures_names": ["9-Table4-1.png"], "answer": "Through human evaluation where they are asked to evaluate the generated output on a likert scale."}}