{"220": {"question": "How many people participated in their evaluation study of table-to-text models?", "referred_figures_indices": [2], "response": "The provided images contain various statistical metrics, model evaluations, and performance reports for table-to-text generation models rather than information about human participants involved in an evaluation study. None of the images explicitly state the number of people who participated in the evaluation study of table-to-text models.\n\nTherefore, based on the images provided, it is not possible to determine the number of people that participated in their evaluation study of table-to-text models.\n\n{'Answer': 'The images provided do not contain the requested information.'}", "all_figures_names": ["7-Figure3-1.png", "2-Figure1-1.png", "5-Table1-1.png", "11-Table5-1.png", "12-Table6-1.png", "8-Table4-1.png", "8-Table3-1.png", "6-Table2-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "about 500"}, "222": {"question": "By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?", "referred_figures_indices": [4, 5, 7], "response": "To answer your question about how much more PARENT correlates with human judgments in comparison to other text generation metrics, I need to analyze the information presented in the images. After examining the data, I will provide an answer derived from the quantitative information given in the images.\n\nHowever, the images you've supplied are not conducive to providing a simple numerical answer to the comparison between PARENT and other metrics due to the complexity of the data, with multiple metrics, conditions, and tests. It's generally difficult to represent the results of such varied tests with a single number without context. These tests are typically interpreted using statistical analysis, and correlations or other comparative", "all_figures_names": ["7-Figure4-1.png", "5-Table1-1.png", "8-Table3-1.png", "2-Figure1-1.png", "7-Figure3-1.png", "6-Table2-1.png", "11-Figure6-1.png", "8-Table4-1.png"], "referred_figures_names": ["7-Figure3-1.png", "6-Table2-1.png", "8-Table4-1.png"], "answer": "Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge."}, "223": {"question": "By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?", "referred_figures_indices": [0], "response": "To answer the question regarding how much more PARENT correlates with human judgements in comparison to other text generation metrics, we can look at the provided images to identify the correlation scores. We need to identify the relevant metrics from the tables and charts in the images.\n\nLet's analyze the images in order. Apologies, but I cannot perform calculations based on image content.\n\n- Image 0 shows several metrics with their correlation scores for the WikiBio Systems and WikiBio Hyperparams datasets as well as an average (Avg) correlation score for each metric. We should compare the average (Avg) column for the PARENT metric against the others.\n-", "all_figures_names": ["6-Table2-1.png", "8-Table4-1.png", "11-Table5-1.png", "11-Figure6-1.png", "7-Figure4-1.png", "5-Table1-1.png", "9-Figure5-1.png", "2-Figure1-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "Their average correlation tops the best other model by 0.155 on WikiBio."}}