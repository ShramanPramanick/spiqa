{"951": {"question": "what are the evaluation metrics?", "referred_figures_indices": [4], "response": "{'Answer': 'Precision (P), Recall (R), F1 score (F1), F1 score (F1), Train (L), Test (T-F1)'}", "all_figures_names": ["19-Table14-1.png", "15-Table6-1.png", "22-Table18-1.png", "9-Table3-1.png", "15-Table5-1.png", "22-Table19-1.png", "20-Table16-1.png", "17-Table10-1.png"], "referred_figures_names": ["15-Table5-1.png"], "answer": "Precision, Recall, F1"}, "952": {"question": "which datasets were used in evaluation?", "referred_figures_indices": [0], "response": "{'Answer': 'CoNLL 2003, CoNLL 2002, GermEval 2014, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0, EFE 2000, Frankfurter Rundschau 1992, Wikipedia/LCC news, De Morgen 2000, Egunkaria 1999-2003, Wikinews 2013'}", "all_figures_names": ["5-Table1-1.png", "19-Table15-1.png", "19-Table14-1.png", "17-Table10-1.png", "9-Table3-1.png", "11-Figure1-1.png", "6-Table2-1.png", "13-Table4-1.png"], "referred_figures_names": ["5-Table1-1.png"], "answer": "CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0"}}