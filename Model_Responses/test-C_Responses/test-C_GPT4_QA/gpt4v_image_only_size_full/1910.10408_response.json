{"483": {"question": "Do they conduct any human evaluation?", "referred_figures_indices": [5], "response": "{'Answer': 'Yes, they conduct human evaluation.'}", "all_figures_names": ["5-Table3-1.png", "4-Figure3-1.png", "7-Table6-1.png", "4-Table2-1.png", "4-Table1-1.png", "2-Figure1-1.png", "7-Table7-1.png", "6-Table5-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "Yes"}, "485": {"question": "How do they enrich the positional embedding with length information", "referred_figures_indices": [7], "response": "{'Answer': 'Positional embeddings are enriched with length information by adding length tokens or embeddings. As indicated in one of the provided images, tokens such as <short>, <norm>, and <long> are prepended to the input sequence to provide the model with an indication of the expected output length. This approach allows the Neural Machine Translation (NMT) system to adjust the translation based on the anticipated length of the output.'}", "all_figures_names": ["2-Figure1-1.png", "2-Figure2-1.png", "5-Table3-1.png", "6-Table4-1.png", "7-Table6-1.png", "7-Table7-1.png", "4-Table2-1.png", "4-Figure3-1.png"], "referred_figures_names": ["4-Figure3-1.png"], "answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative)."}, "486": {"question": "How do they condition the output to a given target-source class?", "referred_figures_indices": [7], "response": "{'Answer': 'The conditioning to a target-source class seems to be done by incorporating the length ratio and length information into the translation model. Specifically, there is evidence of using tokens such as \"<short>\", \"<norm>\", and \"<long>\" to represent the length of the source sentence relative to the target (as shown in Image 7), and adjusting the Translation Model (TM) based on this length information (referenced in Image 4). Furthermore, Image 3 shows different scaling factors and length ratios being taken into account, adjusting the likelihood ratio (LR) corresponding to the source (LR^src) and the reference (LR^", "all_figures_names": ["7-Table6-1.png", "4-Table2-1.png", "6-Table5-1.png", "5-Table3-1.png", "4-Figure3-1.png", "4-Table1-1.png", "2-Figure1-1.png", "2-Figure2-1.png"], "referred_figures_names": ["2-Figure2-1.png"], "answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group."}}