{"951": {"question": "What are the qualitative experiments performed on benchmark datasets?", "referred_figures_indices": [2, 3], "response": "{'Answer': 'The qualitative experiments performed on benchmark datasets include evaluating word representation models on a variety of tasks and datasets. Metrics such as MaxCos, AvgCos, L_approx, and L_comp are used to measure model performance (shown in Image 0). Word similarity and relatedness tasks are performed, where models are evaluated on how well they identify nearest neighbors for given words in different contexts (Image 1). Models are also assessed using precision and F1 scores on various datasets (Turney and Pantel 2015, Baroni et al. 2012, Kotlerman et al. 2010), as shown in", "all_figures_names": ["4-Table2-1.png", "4-Table1-1.png", "5-Table4-1.png", "5-Table3-1.png"], "referred_figures_names": ["5-Table4-1.png", "5-Table3-1.png"], "answer": "Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset."}}