{"801": {"question": "how small of a dataset did they train on?", "referred_figures_indices": [2], "response": "{'Answer': 'Two datasets were used for training: \"Shenma Voice Search\" with 16,150 training samples and \"Amap\" with 6,935 training samples.'}", "all_figures_names": ["6-Table3-1.png", "6-Table2-1.png", "4-Table1-1.png", "5-Figure1-1.png", "6-Table4-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "23085 hours of data"}, "802": {"question": "what was their character error rate?", "referred_figures_indices": [1, 4, 2], "response": "{'Answer': 'The character error rates (CER) vary based on different models and training methods. They are as follows: \n\nFor the 9-layers LSTM model, the CER is 2.49%.\nFor the 2-layers regular-trained LSTM model, the CER is 3.06%.\nFor the 2-layers distilled LSTM model, the CER is 2.63%.\nFor the Shenma model, the CER is 7.87%.\nFor the Amap CE + sMBR training method, the CER is 6.81%.\nFor the Shen", "all_figures_names": ["4-Table1-1.png", "6-Table3-1.png", "6-Table4-1.png", "5-Figure1-1.png", "6-Table2-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png", "6-Table4-1.png"], "answer": "2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning."}, "803": {"question": "what was their character error rate?", "referred_figures_indices": [3, 1], "response": "{'Answer': 'The character error rates vary across different models, layers, and training methods as follows:\n\n- Xavier Init CE (Cross Entropy) for different layers ranges from 3.72% to 3.93%.\n- Layer-wise CE (Cross Entropy) for different layers ranges from 2.82% to 3.68%.\n- CE+sMBR (Cross Entropy + State-level Minimum Bayes Risk) for different layers ranges from 2.49% to 2.85%.\n\nIn terms of models:\n- A 9-layers LSTM model has a CER of 2.49", "all_figures_names": ["5-Figure1-1.png", "6-Table2-1.png", "4-Table1-1.png", "6-Table3-1.png", "6-Table4-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Their best model achieved a 2.49% Character Error Rate."}, "804": {"question": "which lstm models did they compare with?", "referred_figures_indices": [4, 3], "response": "{'Answer': '9-layers LSTM, 2-layers regular-trained LSTM, and 2-layers distilled LSTM'}", "all_figures_names": ["5-Figure1-1.png", "6-Table4-1.png", "4-Table1-1.png", "6-Table2-1.png", "6-Table3-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."}}