{"248": {"question": "How were the human judgements assembled?", "referred_figures_indices": [4, 2], "response": "{'Answer': 'The human judgements appear to have been assembled through a methodical process involving the evaluation of machine translation outputs. Specifically, there seem to be judgments on the quality of translations from English to another language (possibly Tamil) involving different models or approaches such as RNNSearch+Word2Vec and RNNMorph. These judgments include assessments of adequacy, fluency, and ranking of translations as seen in the provided tables and analysis. There was likely a procedure for annotators to provide their evaluations, which were quantified and summarized in statistical forms such as probability agreements and kappa statistics to measure agreement among different annotators (", "all_figures_names": ["17-Figure11-1.png", "9-Figure4-1.png", "11-Table2-1.png", "16-Figure10-1.png", "12-Table3-1.png", "8-Figure3-1.png", "11-Figure5-1.png", "14-Figure8-1.png"], "referred_figures_names": ["12-Table3-1.png", "11-Table2-1.png"], "answer": "50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale."}}