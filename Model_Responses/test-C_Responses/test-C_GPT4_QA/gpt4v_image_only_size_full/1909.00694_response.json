{"2": {"question": "What are the results?", "referred_figures_indices": [3, 0], "response": "{'Answer': 'The results are various performance measures of models tested on different training datasets. The models used are BiGRU and BERT. The Accuracy (Acc) is the performance metric provided. The highest accuracy achieved for the full dataset is 0.933 with the BERT encoder on the ACP training dataset. Other configurations with different combinations of training datasets (AL, CA, CO) and encoders result in varying accuracies. Additional results include details of the datasets, such as the number of positive and negative events in the ACP dataset, examples of polarity scores for different events, and statistics of different pairs available in the", "all_figures_names": ["5-Table3-1.png", "2-Figure1-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table5-1.png", "4-Table1-1.png"], "referred_figures_names": ["5-Table4-1.png", "5-Table3-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [1], "response": "{'Answer': 'Relations such as CAUSE and CONCESSION are used to determine how the polarity of one event can influence the polarity of another. In the case of CAUSE relations, the polarity of the latter event is encouraged to be the same as the former event. For CONCESSION relations, the latter event is encouraged to have the reverse polarity of the former event.'}", "all_figures_names": ["5-Table5-1.png", "2-Figure1-1.png", "5-Table4-1.png", "5-Table3-1.png", "4-Table1-1.png", "4-Table2-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [0], "response": "{'Answer': 'Relations are used to determine how the polarity of one event may affect or relate to the polarity of another event. In the context of the images provided, polarity propagation seems to be linked with linguistic analysis, specifically through the use of event pairs that are automatically labeled, and through cause and concession relations in texts. For example, a cause relation implies that the polarity of the former event is transferred to the latter event, meaning that if the first event has a positive polarity, the second event is likely to have a positive polarity as well. Conversely, a concession relation often inverts the polarity, so a positive event may lead to", "all_figures_names": ["2-Figure1-1.png", "4-Table1-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table5-1.png", "5-Table3-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [0, 1], "response": "{'Answer': 'The Japanese dataset consists of 1,000,000 pairs for Automatically Labeled (AL) data, 5,000,000 pairs each for CAUSE (CA) and CONCESSION (CO) data.'}", "all_figures_names": ["4-Table1-1.png", "4-Table2-1.png", "2-Figure1-1.png", "5-Table4-1.png", "5-Table3-1.png", "5-Table5-1.png"], "referred_figures_names": ["4-Table1-1.png", "4-Table2-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [1], "response": "{'Answer': 'The Japanese data consists of several datasets with a total of 11,000,000 pairs, broken down as follows: 1,000,000 Automatically Labeled Pairs (AL), 5,000,000 CAUSE Pairs (CA), and 5,000,000 CONCESSION Pairs (CO).'}", "all_figures_names": ["2-Figure1-1.png", "4-Table2-1.png", "5-Table3-1.png", "5-Table5-1.png", "5-Table4-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [1], "response": "{'Answer': 'The improvements vary depending on the model. For the BERT encoder, the accuracy improved from 0.876 to 0.886 when the AL dataset was added to the ACP (6K) dataset, representing an improvement of 0.010 or 1%. For the BiGRU encoder, the accuracy improved from 0.830 to 0.879 when the AL, CA, and CO datasets were added, representing an improvement of 0.049 or 4.9%.'}", "all_figures_names": ["4-Table1-1.png", "5-Table4-1.png", "4-Table2-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table3-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [3], "response": "{'Answer': 'The model learns using mostly raw data by being trained on large datasets consisting of automatically labeled pairs (AL), as well as cause (CA) and concession (CO) event pairs. These datasets contain examples with event polarity annotations, showing whether events have a positive or negative connotation. Through these annotations and the sheer volume of data, the model is able to learn and predict polarity scores for input events, as well as understand the relations between pairs of events, such as whether one causes or is a concession to the other.'}", "all_figures_names": ["5-Table3-1.png", "5-Table5-1.png", "5-Table4-1.png", "2-Figure1-1.png", "4-Table1-1.png", "4-Table2-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [3], "response": "{'Answer': '6000'}", "all_figures_names": ["5-Table4-1.png", "5-Table3-1.png", "5-Table5-1.png", "4-Table1-1.png", "4-Table2-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}