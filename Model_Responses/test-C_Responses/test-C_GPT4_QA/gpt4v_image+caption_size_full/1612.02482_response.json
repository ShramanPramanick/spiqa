{"248": {"question": "How were the human judgements assembled?", "referred_figures_indices": [0, 3], "response": "{'Answer': 'The human judgements were assembled using intra-annotator agreement metrics and rankings to evaluate different models. Table 3 shows the intra-annotator ranking for two models, indicating the consistency of an annotator\u2019s judgement on the translations provided by each model. Similarly, Table 2 presents the intra-annotator agreement for RNNMorph based on tasks like adequacy, fluency, and ranking, using kappa statistics to display agreement levels. Such tables and figures usually arise from a study where multiple translations are assessed by human evaluators to measure the quality and reliability of machine translation systems.'}", "all_figures_names": ["12-Table3-1.png", "17-Figure11-1.png", "14-Figure8-1.png", "11-Table2-1.png", "10-Table1-1.png", "8-Figure2-1.png", "8-Figure3-1.png", "16-Figure10-1.png"], "referred_figures_names": ["12-Table3-1.png", "11-Table2-1.png"], "answer": "50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale."}}