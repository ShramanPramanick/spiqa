{"720": {"question": "How much is proposed model better than baselines in performed experiments?", "referred_figures_indices": [1], "response": "{'Answer': 'The proposed model GDP is better than baselines E2ECM and CDM in the performed experiments. It demonstrates larger BLEU scores on both DSTC2 and Maluuba datasets, indicating better language generation capabilities. Furthermore, GDP utilizes a bigger model size and incorporates more dialogue policy parameters, as highlighted by the RNN-based dialogue policy maker specified in the first image caption. The performance improvement is quantifiable based on the metrics provided in the second image's caption.'}", "all_figures_names": ["8-Figure3-1.png", "7-Table2-1.png", "4-Figure2-1.png", "6-Table1-1.png", "8-Table3-1.png", "2-Figure1-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)\nGDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)\nGDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)"}}