{"2": {"question": "What are the results?", "referred_figures_indices": [1, 5], "response": "{'Answer': 'The results are as follows:\n\n- For the ACP dataset, the number of positive events is 299,834 in the Train set, 50,118 in the Dev set, and 50,046 in the Test set. The number of negative events is 300,164 in the Train set, 49,882 in the Dev set, and 49,954 in the Test set.\n- Using a small labeled training dataset (Table 4), BERT trained with AL data achieves an accuracy of 0.886, while BiGRU trained with AL+CA+CO achieves an accuracy of ", "all_figures_names": ["4-Table2-1.png", "5-Table4-1.png", "5-Table5-1.png", "2-Figure1-1.png", "4-Table1-1.png", "5-Table3-1.png"], "referred_figures_names": ["5-Table4-1.png", "5-Table3-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [3], "response": "{'Answer': 'Relations are used to propagate polarity by connecting pairs of events (the former event and the latter event) with a discourse relation such as CAUSE or CONCESSION. In Automatically Labeled (AL) pairs, the polarity of the latter event is automatically identified based on a seed lexicon, and the polarity is propagated to the former event. When the relation is CAUSE, the same polarity is propagated, and when the relation is CONCESSION, the opposite polarity is propagated. In CA and CO pairs, where the polarity of the latter event is not known, the discourse relation induces the system to encourage either the same", "all_figures_names": ["4-Table2-1.png", "4-Table1-1.png", "5-Table3-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table4-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [0], "response": "{'Answer': 'Relations are used to propagate polarity by connecting pairs of events, where the polarity of a latter event influences the polarity of a former event through a specific discourse relation, such as CAUSE or CONCESSION. For CAUSE relations, the polarity is propagated in the same direction, meaning that if the latter event is positive, the former event is encouraged to have the same positive polarity, and if the latter event is negative, the former event is encouraged to have the same negative polarity. For CONCESSION relations, the polarity is propagated in the reverse direction, such that if the latter event is positive, the former event is", "all_figures_names": ["2-Figure1-1.png", "5-Table3-1.png", "4-Table1-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table5-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [5, 0], "response": "{'Answer': '11,000,000'}", "all_figures_names": ["4-Table1-1.png", "5-Table4-1.png", "5-Table5-1.png", "5-Table3-1.png", "2-Figure1-1.png", "4-Table2-1.png"], "referred_figures_names": ["4-Table2-1.png", "4-Table1-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [1], "response": "{'Answer': 'The Japanese data, referred to as the ACP dataset, consists of 299,834 positive events and 300,164 negative events in the training set, 50,118 positive events and 49,882 negative events in the development (Dev) set, and 50,046 positive events and 49,954 negative events in the test set. Additionally, the AL dataset contains 1,000,000 pairs, the CA dataset contains 5,000,000 pairs, and the CO dataset consists of 5,000,000 pairs.'}", "all_figures_names": ["5-Table4-1.png", "4-Table2-1.png", "4-Table1-1.png", "5-Table5-1.png", "5-Table3-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [4], "response": "{'Answer': 'The improvements of supervised learning results trained on smaller labeled data enhanced with the proposed approach compared to the basic approach are as follows:\n\n- When using the BERT encoder, there is an improvement from 0.876 accuracy with the ACP (6K) alone to 0.886 accuracy with ACP (6K) + AL, and further to 0.879 accuracy with ACP (6K) + AL + CA + CO.\n- When using the BiGRU encoder, the improvement is from 0.830 accuracy with ACP (6K) alone to 0.879 accuracy with A", "all_figures_names": ["4-Table1-1.png", "5-Table5-1.png", "4-Table2-1.png", "5-Table3-1.png", "5-Table4-1.png", "2-Figure1-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [1], "response": "{'Answer': 'Their model learns using mostly raw data by focusing on pairs of events connected with discourse relations (CAUSE or CONCESSION) and using datasets labeled for these relations (AL for Automatically Labeled Pairs, CA for CAUSE pairs, CO for CONCESSION pairs). The model is trained to identify the polarity of events as positive or negative. In the AL set, polarity is automatically identified based on a seed lexicon and then propagated, with the same polarity for CAUSE and reversed polarity for CONCESSION. For CA and CO, polarity propagation encourages the same or reverse polarity based on the discourse relation. The effectiveness", "all_figures_names": ["4-Table1-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table3-1.png", "4-Table2-1.png", "5-Table4-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [5], "response": "{'Answer': 'The seed lexicon used for training isn't explicitly quantified in the given data. However, it is mentioned that the polarity of a latter event is identified according to the seed lexicon, and examples of automatically labeled pairs (AL) are provided in Image 0, suggesting that the seed lexicon contains words with known polarities (positive words colored red and negative words blue) used to automatically label these event pairs. The specific size of this lexicon is not provided in the captions or images.'}", "all_figures_names": ["2-Figure1-1.png", "5-Table3-1.png", "5-Table5-1.png", "5-Table4-1.png", "4-Table2-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}