{"903": {"question": "what dataset statistics are provided?", "referred_figures_indices": [5], "response": "{\n  \"Answer\": \n  {\n    \"Baseline Model Performances\": {\n      \"Accuracy Values by Question Type\": \"Image 0 shows a bar chart of baseline model performances on different question types. The caption suggests these are accuracy values for questions occurring more than 25 times.\", \n      \"Overall Accuracy\": \"Image 3 presents a table with the accuracy of baseline systems on text-based questions, commonsense-based questions, and the overall test set in percentages.\"\n    },\n    \"Example Texts and Questions\": {\n      \"Reading Comprehension Example\": \"Image 1 provides an example of a text snippet followed by two reading", "all_figures_names": ["6-Figure4-1.png", "1-Figure1-1.png", "4-Figure3-1.png", "6-Table2-1.png", "4-Figure2-1.png", "3-Table1-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%)."}, "904": {"question": "what dataset statistics are provided?", "referred_figures_indices": [3], "response": "{'Answer': 'The dataset statistics provided include accuracy of different baseline systems on text-based and commonsense-based questions, distribution of question types, distribution of question categories, and accuracy values of baseline models on various question types.'}", "all_figures_names": ["1-Figure1-1.png", "6-Table2-1.png", "4-Figure2-1.png", "3-Table1-1.png", "4-Figure3-1.png", "6-Figure4-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text"}}