{"31": {"question": "What experiments are proposed to test that upper layers produce context-specific embeddings?", "referred_figures_indices": [0, 4, 3], "response": "{'Answer': 'The proposed experiments to test if upper layers produce context-specific embeddings include:\n1. Analyzing the average self-similarity of word representations in different contexts, where a lower self-similarity in higher layers suggests that those layers produce more context-specific embeddings (Image 0, Caption 0).\n2. Comparing the performance of first principal component static embeddings derived from contextualized word representations (from ELMo, BERT, GPT-2) across various benchmark tasks, to see how well these static embeddings perform compared to non-contextualized vectors (Image 1, Caption 1).\n3. Calculating", "all_figures_names": ["6-Figure2-1.png", "8-Table1-1.png", "8-Figure4-1.png", "7-Figure3-1.png", "5-Figure1-1.png"], "referred_figures_names": ["6-Figure2-1.png", "5-Figure1-1.png", "7-Figure3-1.png"], "answer": "They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  \nThey plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.\nThey plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another."}, "32": {"question": "How do they calculate a static embedding for each word?", "referred_figures_indices": [2], "response": "{'Answer': 'For the contextualizing models (ELMo, BERT, GPT-2), they calculate a static embedding for each word by using the first principal component of a word\u2019s contextualized representations in a given layer.'}", "all_figures_names": ["5-Figure1-1.png", "6-Figure2-1.png", "8-Table1-1.png", "7-Figure3-1.png", "8-Figure4-1.png"], "referred_figures_names": ["8-Table1-1.png"], "answer": "They use the first principal component of a word's contextualized representation in a given layer as its static embedding."}}