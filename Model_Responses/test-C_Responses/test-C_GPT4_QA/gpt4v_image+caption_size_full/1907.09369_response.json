{"601": {"question": "Do they report results only on English data?", "referred_figures_indices": [7], "response": "{'Answer': 'Yes, they report results only on English data.'}", "all_figures_names": ["5-Table6-1.png", "4-Table5-1.png", "2-Table1-1.png", "5-Table7-1.png", "3-Table3-1.png", "3-Figure1-1.png", "4-Table4-1.png", "2-Table2-1.png"], "referred_figures_names": ["2-Table2-1.png"], "answer": "Yes"}, "603": {"question": "What are the hyperparameters of the bi-GRU?", "referred_figures_indices": [1, 5], "response": "{'Answer': 'The images provided do not explicitly list the hyperparameters of the bi-GRU (bidirectional Gated Recurrent Unit). Hyperparameters usually include information such as the number of layers, the number of units in each layer, learning rate, batch size, and other settings used to configure the model before training. For specific details regarding the hyperparameters, we would need access to the model configuration or textual descriptions from the experiment's documentation or publication.'}", "all_figures_names": ["4-Table4-1.png", "4-Table5-1.png", "2-Table1-1.png", "5-Table6-1.png", "5-Table7-1.png", "3-Figure1-1.png", "3-Table3-1.png", "2-Table2-1.png"], "referred_figures_names": ["4-Table5-1.png", "3-Figure1-1.png"], "answer": "They use the embedding layer with a size 35 and embedding dimension of 300. They use a dense layer with 70 units and a dropout layer with a rate of 50%."}}