{"1063": {"question": "What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?", "referred_figures_indices": [6], "response": "{'Answer': 'The performance decreases as error increases. For the WordPiece tokenizer, the Pearson-Spearman correlation coefficient drops from 0.89 with 0% error to 0.35 with 20% error. For the WhiteSpc tokenizer, it decreases from 0.69 to 0.22, and for the N-gram (n=6) tokenizer, it goes from 0.73 to 0.25.'}", "all_figures_names": ["6-Table2-1.png", "3-Table1-1.png", "2-Figure1-1.png", "4-Figure3-1.png", "2-Figure2-1.png", "4-Figure4-1.png", "5-Figure5-1.png"], "referred_figures_names": ["5-Figure5-1.png"], "answer": "10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%\n50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%"}, "1064": {"question": "Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?", "referred_figures_indices": [0, 6], "response": "To determine which sentiment analysis dataset has a larger performance drop when a 10% error is introduced, we need to look at the changes in performance metrics in the images provided.\n\nFrom Image 0 and Caption 0:\n- For the SST-2 dataset at 10 epochs, F1 Score drops from 0.82 (0% error) to 0.66 (10% error), which is a drop of 0.16.\n- For the SST-2 dataset at 50 epochs, F1 Score drops from 0.84 (0% error) to 0.68 (10% error), which is", "all_figures_names": ["4-Figure4-1.png", "3-Table1-1.png", "6-Table2-1.png", "2-Figure2-1.png", "2-Figure1-1.png", "5-Figure5-1.png", "4-Figure3-1.png"], "referred_figures_names": ["4-Figure4-1.png", "4-Figure3-1.png"], "answer": "SST-2 dataset"}}