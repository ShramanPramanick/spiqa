{"193": {"question": "What supplemental tasks are used for multitask learning?", "referred_figures_indices": [3], "response": "{'Answer': 'The supplemental tasks used for multitask learning, as inferred from the images, include various natural language processing tasks such as related question/comment selection, information retrieval (IR), and tasks that involve feature-rich systems. Other tasks involve using neural attention mechanisms and joint learning for external comment selection. Moreover, the models utilize LSTM (Long Short-Term Memory) networks for encoding and leverage techniques like attention and augmented features to improve the performance on tasks like community question answering (cQA).'}", "all_figures_names": ["5-Figure4-1.png", "3-Figure2-1.png", "5-Figure3-1.png", "3-Figure1-1.png", "7-Table3-1.png", "7-Table4-1.png", "5-Table2-1.png", "8-Figure5-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question"}, "195": {"question": "How much performance gap between their approach and the strong handcrafted method?", "referred_figures_indices": [0], "response": "{'Answer': 'In Task A, their approach has a 0.001 gap compared to the strong handcrafted method. In Task B, their approach has a 0.059 gap. In Task C, their approach has a 0.055 gap.'}", "all_figures_names": ["7-Table4-1.png", "8-Figure5-1.png", "5-Table2-1.png", "5-Figure4-1.png", "5-Figure3-1.png", "3-Figure2-1.png", "3-Figure1-1.png", "7-Table3-1.png"], "referred_figures_names": ["7-Table4-1.png"], "answer": "0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C"}}