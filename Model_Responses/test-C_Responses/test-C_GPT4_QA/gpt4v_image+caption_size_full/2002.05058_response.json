{"989": {"question": "How much better peformance is achieved in human evaluation when model is trained considering proposed metric?", "referred_figures_indices": [0, 4], "response": "{'Answer': 'It's not possible to quantify the exact performance improvement from the available images because the metric improvements are shown in different contexts and datasets without a baseline comparison for each. However, the proposed metric generally shows higher correlation values compared to other metrics, which suggests better performance in aligning with human judgments.'}", "all_figures_names": ["6-Table2-1.png", "7-Table5-1.png", "7-Table4-1.png", "7-Table3-1.png", "6-Table1-1.png", "4-Figure1-1.png"], "referred_figures_names": ["6-Table2-1.png", "6-Table1-1.png"], "answer": "Pearson correlation to human judgement - proposed vs next best metric\nSample level comparison:\n- Story generation: 0.387 vs 0.148\n- Dialogue: 0.472 vs 0.341\nModel level comparison:\n- Story generation:  0.631 vs 0.302\n- Dialogue: 0.783 vs 0.553"}}