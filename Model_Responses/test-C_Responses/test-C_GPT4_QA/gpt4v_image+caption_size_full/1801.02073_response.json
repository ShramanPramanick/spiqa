{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "referred_figures_indices": [3], "response": "{'Answer': 'Yes'}", "all_figures_names": ["3-Figure1-1.png", "4-Table3-1.png", "3-Figure2-1.png", "3-Table2-1.png", "2-Table1-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "referred_figures_indices": [4], "response": "{'Answer': 'Six'}", "all_figures_names": ["2-Table1-1.png", "3-Table2-1.png", "3-Figure2-1.png", "4-Table3-1.png", "3-Figure1-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "referred_figures_indices": [2], "response": "{'Answer': 'The analysis of contextual similarities across datasets likely involves looking at the distribution of answer categories, statistics of the silver-standard dataset, comparisons between the corpora for answer selection, the distribution of question types, and the results of answer selection and triggering trained and evaluated across all corpora splits. Details such as the robustness of the silver-standard, coverage of retrieved passages, annotations for answer triggering, and the overlapping word statistics between question-answer pairs, as well as performance metrics like Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and F1 scores, are used to analyze and compare the datasets.'}", "all_figures_names": ["3-Figure2-1.png", "3-Table2-1.png", "2-Table1-1.png", "3-Figure1-1.png", "4-Table3-1.png"], "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}