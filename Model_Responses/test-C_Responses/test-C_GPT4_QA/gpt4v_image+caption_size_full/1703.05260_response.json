{"93": {"question": "Did the annotators agreed and how much?", "referred_figures_indices": [5], "response": "{'Answer': 'The annotators generally agreed, with some variation depending on the scenario and type of annotation.'}", "all_figures_names": ["7-Figure9-1.png", "4-Figure3-1.png", "8-Figure11-1.png", "2-Table1-1.png", "7-Figure8-1.png", "6-Figure4-1.png", "7-Figure7-1.png", "6-Figure6-1.png"], "referred_figures_names": ["6-Figure4-1.png"], "answer": "For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%."}, "94": {"question": "Did the annotators agreed and how much?", "referred_figures_indices": [0], "response": "{'Answer': 'The annotators generally agreed, with average Fleiss\u2019 Kappa values for agreement ranging from moderate to strong across different scenarios. The overall average Fleiss\u2019 Kappa for all labels is 0.64 for events and 0.77 for participants, indicating a moderate level of agreement for events and a substantial agreement for participants. Coreference agreement is high, with an overall average of 90.5%. Specific agreement percentages for each scenario vary, with TREE having the lowest coreference agreement at 78.3% and CAKE having the highest at 94.7%.'}", "all_figures_names": ["6-Figure4-1.png", "8-Figure11-1.png", "2-Figure2-1.png", "4-Figure3-1.png", "7-Figure9-1.png", "6-Figure5-1.png", "8-Figure10-1.png", "3-Table2-1.png"], "referred_figures_names": ["6-Figure4-1.png"], "answer": "Moderate agreement of 0.64-0.68 Fleiss\u2019 Kappa over event type labels, 0.77 Fleiss\u2019 Kappa over participant labels, and good agreement of 90.5% over coreference information."}}