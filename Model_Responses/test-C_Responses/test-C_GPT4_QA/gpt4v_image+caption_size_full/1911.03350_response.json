{"1381": {"question": "How they evaluate quality of generated output?", "referred_figures_indices": [0], "response": "{'Answer': 'The quality of generated output is evaluated using several metrics that assess different aspects of quality. These include measures of answerability, correctness, external knowledge, relevance, soundness, BLEU scores, Self-BLEU scores, and question-answer related metrics (QA_source and QA_context). Specific metrics like BLEU are used to measure the linguistic quality of generated texts by comparing them with reference texts. Human evaluation is also conducted where human judges rate the outputs based on the mentioned criteria. Additionally, the distribution of the first word frequency per models for specific datasets (SQuAD and QuAC) is analyzed, and correlation matrices are", "all_figures_names": ["9-Table4-1.png", "8-Figure1-1.png", "7-Table2-1.png", "7-Table3-1.png", "9-Figure2-1.png", "4-Table1-1.png"], "referred_figures_names": ["9-Table4-1.png"], "answer": "Through human evaluation where they are asked to evaluate the generated output on a likert scale."}}