{"753": {"question": "Is the new model evaluated on the tasks that BERT and ELMo are evaluated on?", "referred_figures_indices": [4], "response": "{'Answer': 'Yes, the new model is evaluated on tasks that BERT and ELMo are evaluated on.'}", "all_figures_names": ["5-Table2-1.png", "12-Table4-1.png", "8-Table3-1.png", "6-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "Yes"}, "754": {"question": "Does the additional training on supervised tasks hurt performance in some tasks?", "referred_figures_indices": [0], "response": "{'Answer': 'Yes, the additional training on supervised tasks can potentially hurt performance on some tasks. The tables and figures provided in the images show that while supplemental training on intermediate tasks (reflected in the acronym \"STILTs,\" which stands for Supplementary Training on Intermediate Labeled-data Tasks) can lead to improved performance on some tasks, there are also instances where performance on particular tasks is worse compared to a baseline model without such additional training. The tables illustrate the varying impact of training on additional supervised tasks for models like BERT and GPT across different datasets like CoLA, SST, MRPC, QQP, STS, MN", "all_figures_names": ["4-Table1-1.png", "8-Table3-1.png", "12-Table4-1.png", "5-Table2-1.png", "6-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "Yes"}}