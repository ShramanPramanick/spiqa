{"585": {"question": "What evaluations did the authors use on their system?", "referred_figures_indices": [6, 1, 0], "response": "{'Answer': 'The authors evaluated their system using BLEU scores for translation quality on different NIST test sets (Image 0 and Image 4), coherence measurement through average cosine similarity of adjacent sentences (Image 1), qualitative analysis with translation examples (Image 2), analysis on the number of words in translations that are also in the dynamic and topic cache (Image 5 and Image 6), and the effect of the gating mechanism on the BLEU scores (Image 4).'}", "all_figures_names": ["8-Table1-1.png", "10-Table6-1.png", "9-Table5-1.png", "5-Figure2-1.png", "8-Table2-1.png", "9-Table4-1.png", "9-Table3-1.png", "5-Figure3-1.png"], "referred_figures_names": ["9-Table3-1.png", "10-Table6-1.png", "8-Table1-1.png"], "answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."}}