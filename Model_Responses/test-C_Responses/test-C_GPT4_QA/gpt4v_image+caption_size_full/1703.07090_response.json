{"801": {"question": "how small of a dataset did they train on?", "referred_figures_indices": [2], "response": "{'Answer': 'The smallest dataset they trained on has 7308 samples.'}", "all_figures_names": ["6-Table3-1.png", "6-Table4-1.png", "4-Table1-1.png", "6-Table2-1.png", "5-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "23085 hours of data"}, "802": {"question": "what was their character error rate?", "referred_figures_indices": [3, 0, 2], "response": "{'Answer': 'The character error rate (CER) varied across different models and training methods. Here are the reported CERs:\n\n- Shenma distilled model: 7.87%\n- Amap CE + sMBR: 6.81%\n- Shenma model + Amap sMBR: 6.26%\n- 6-layer model with Xavier Initialization: 3.72% and with CE+sMBR: 2.85%\n- 7-layer model with Layer-wise CE: 3.68% and with CE+sMBR: 2.81%\n- 8-layer model with", "all_figures_names": ["6-Table4-1.png", "5-Figure1-1.png", "6-Table2-1.png", "6-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table4-1.png", "6-Table2-1.png"], "answer": "2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning."}, "803": {"question": "what was their character error rate?", "referred_figures_indices": [1, 3], "response": "{\"Answer\": \"2.49%, 3.06%, 2.63%, 7.87%, 6.81%, 6.26%, 3.72%, 3.93%, 3.81%, 3.87%\"}", "all_figures_names": ["4-Table1-1.png", "6-Table3-1.png", "6-Table4-1.png", "6-Table2-1.png", "5-Figure1-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Their best model achieved a 2.49% Character Error Rate."}, "804": {"question": "which lstm models did they compare with?", "referred_figures_indices": [1, 0], "response": "{'Answer': '9-layers LSTM, 2-layers regular-trained LSTM, 2-layers distilled LSTM'}", "all_figures_names": ["6-Table2-1.png", "6-Table3-1.png", "5-Figure1-1.png", "4-Table1-1.png", "6-Table4-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."}}