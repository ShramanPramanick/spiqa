{"1340": {"question": "Which are the sequence model architectures this method can be transferred across?", "referred_figures_indices": [4], "response": "{'Answer': 'LSTM, CNN, and Transformer'}", "all_figures_names": ["8-Table4-1.png", "7-Figure1-1.png", "8-Table3-1.png", "7-Table2-1.png", "9-Table8-1.png", "9-Table6-1.png", "9-Table7-1.png", "6-Table1-1.png"], "referred_figures_names": ["9-Table8-1.png"], "answer": "The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models"}, "1341": {"question": " What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?", "referred_figures_indices": [7], "response": "Based on the provided images and captions, the inference speed improvement of the proposed method over the newest state-of-the-art methods can be determined by comparing the inference speeds from Image 7/Table 3.\n\nFor the MSRA dataset, the inference speed of the proposed method using LSTM is 85.43 sentences per second, while the inference speed of the LR-CNN (Gui et al.), which appears to be the best among the state-of-the-art methods mentioned, is 23.20 sentences per second.\n\nTo calculate the percentage of improvement:\n\n((Proposed method speed - State-of-the-art method speed) / State-of-the-art method", "all_figures_names": ["9-Table5-1.png", "9-Table8-1.png", "9-Table6-1.png", "6-Table1-1.png", "7-Table2-1.png", "9-Table7-1.png", "8-Table4-1.png", "8-Table3-1.png"], "referred_figures_names": ["8-Table3-1.png"], "answer": "Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)"}}