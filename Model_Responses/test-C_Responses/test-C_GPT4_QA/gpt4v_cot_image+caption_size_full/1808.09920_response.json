{"856": {"question": "What baseline did they compare Entity-GCN to?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Rationale': 'This image shows a comparison table of several models including the Entity-GCN, providing a direct reference to the baseline models it was compared against.'}\n\nThe answer is: Entity-GCN was compared to baseline models such as FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN.", "all_figures_names": ["8-Table4-1.png", "13-Figure3-1.png", "5-Table1-1.png", "1-Figure1-1.png", "11-Table5-1.png", "3-Figure2-1.png", "7-Table3-1.png", "6-Table2-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}, "858": {"question": "Did they use a relation extraction method to construct the edges in the graph?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Rationale': 'Illustrates the graph structure and the types of relations, including co-occurrence and coreference, used to connect nodes.'}\n\nThe answer is: Yes, they used a relation extraction method to construct the edges in the graph, as image 5 shows nodes connected by different types of relations including co-occurrence and coreference.", "all_figures_names": ["5-Table1-1.png", "1-Figure1-1.png", "13-Figure3-1.png", "8-Table4-1.png", "12-Table6-1.png", "3-Figure2-1.png", "6-Table2-1.png", "7-Table3-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "No"}, "859": {"question": "How did they get relations between mentions?", "referred_figures_indices": [7], "response": "{'Image': 0, 'Rationale': 'The image provides a direct example of multi-step reasoning with mentions from different documents to infer a relationship, which is relevant to understanding how relations between mentions are established.'}\n\nThe answer is: They got relations between mentions through multi-step reasoning and information combination from different documents.", "all_figures_names": ["1-Figure1-1.png", "11-Table5-1.png", "7-Table3-1.png", "6-Table2-1.png", "13-Figure3-1.png", "8-Table4-1.png", "12-Table6-1.png", "3-Figure2-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."}, "862": {"question": "What performance does the Entity-GCN get on WIKIHOP?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'This image directly shows the accuracy of different models on WIKIHOP, including the Entity-GCN.'}\n\nThe answer is: The Entity-GCN gets an accuracy of 67.6% without coreference (single model), 66.4% with coreference (single model), and 71.2% for an ensemble of 5 models on the WIKIHOP test set.", "all_figures_names": ["8-Table4-1.png", "6-Table2-1.png", "13-Figure3-1.png", "3-Figure2-1.png", "7-Table3-1.png", "11-Table5-1.png", "5-Table1-1.png", "12-Table6-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"}}