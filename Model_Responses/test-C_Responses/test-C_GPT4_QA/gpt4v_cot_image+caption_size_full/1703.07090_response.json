{"801": {"question": "how small of a dataset did they train on?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'The table explicitly lists the number of samples in the training, validation, and test sets for two datasets, which is directly relevant to the question about the size of the dataset they trained on.'}\n\nThe answer is: They trained on datasets of sizes 16,150 for the Shenma Voice Search and 6,935 for the Amap.", "all_figures_names": ["6-Table4-1.png", "6-Table3-1.png", "6-Table2-1.png", "5-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "23085 hours of data"}, "802": {"question": "what was their character error rate?", "referred_figures_indices": [2, 0, 1], "response": "{'Image': 0, 'Rationale': 'Image 0 provides a detailed table of character error rates (CER) for varying layers and training criteria, which is directly related to the question.'}\n\nThe answer is: The character error rates (CER) vary depending on the layer and training criteria, ranging from 2.49% to 3.93% as detailed in the table.", "all_figures_names": ["6-Table2-1.png", "6-Table4-1.png", "6-Table3-1.png", "5-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png", "6-Table4-1.png"], "answer": "2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning."}, "803": {"question": "what was their character error rate?", "referred_figures_indices": [4, 1], "response": "{'Image': 1, 'Rationale': 'Image 1 provides a table that directly lists the Character Error Rates (CER) for different training methods and layers of models.'}\n\nThe answer is: The character error rate (CER) varied depending on the layer and training method. The CER for the 6 to 9-layers models trained with regular Xavier Initialization ranged from 3.72% to 3.93%, with layer-wise training with the CE criterion it ranged from 2.82% to 3.68%, and for CE + sMBR criteria it ranged from 2.49%", "all_figures_names": ["5-Figure1-1.png", "6-Table2-1.png", "6-Table4-1.png", "4-Table1-1.png", "6-Table3-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Their best model achieved a 2.49% Character Error Rate."}, "804": {"question": "which lstm models did they compare with?", "referred_figures_indices": [0, 4], "response": "{'Image': 0, 'Rationale': 'This image provides a direct comparison of different LSTM models, which is necessary to answer the question about which models they compared.'}\n\nThe answer is: They compared the 9-layers LSTM, 2-layers regular-trained LSTM, and 2-layers distilled LSTM.", "all_figures_names": ["6-Table3-1.png", "4-Table1-1.png", "5-Figure1-1.png", "6-Table4-1.png", "6-Table2-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."}}