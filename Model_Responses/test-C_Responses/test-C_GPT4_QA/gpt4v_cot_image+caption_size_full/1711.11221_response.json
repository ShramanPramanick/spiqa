{"585": {"question": "What evaluations did the authors use on their system?", "referred_figures_indices": [4, 3, 1], "response": "{'Image': 4, 'Rationale': 'This image provides quantifiable results from the experiments conducted by the authors, showing the effectiveness of their system using BLEU scores, which is a common metric for evaluating machine translation systems.'}\n\nThe answer is: The authors used BLEU scores for evaluating their system on the NIST Chinese-English translation tasks, comparing the proposed model with both dynamic cache (+Cd) and topic cache (+Ct) against other models.", "all_figures_names": ["5-Figure3-1.png", "10-Table6-1.png", "8-Table2-1.png", "9-Table3-1.png", "8-Table1-1.png", "9-Table4-1.png", "4-Figure1-1.png", "9-Table5-1.png"], "referred_figures_names": ["8-Table1-1.png", "9-Table3-1.png", "10-Table6-1.png"], "answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."}}