{"248": {"question": "How were the human judgements assembled?", "referred_figures_indices": [5, 1], "response": "{'Image': 5, 'Rationale': 'Image 5 shows a table titled RNNMorph Intra-Annotator Agreement and has judging tasks, which likely involve human judgments.'}\n\nThe answer is: Human judgements were assembled by having annotators perform tasks related to adequacy, fluency, and ranking, with intra-annotator agreements measured by the probability of agreement P(A), the probability of agreement by chance P(E), and Kappa statistic K, as indicated in the Intra-Annotator Agreement table.", "all_figures_names": ["16-Figure10-1.png", "12-Table3-1.png", "11-Figure5-1.png", "12-Figure6-1.png", "14-Figure8-1.png", "11-Table2-1.png", "13-Figure7-1.png", "9-Figure4-1.png"], "referred_figures_names": ["11-Table2-1.png", "12-Table3-1.png"], "answer": "50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale."}}