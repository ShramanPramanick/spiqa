{"989": {"question": "How much better peformance is achieved in human evaluation when model is trained considering proposed metric?", "referred_figures_indices": [3, 2], "response": "{'Image': 3, 'Rationale': 'This image shows model-level correlation between metrics and human judgments, which directly relates to human evaluation performance comparison.'}\n\nThe answer is: The performance in human evaluation when the model is trained considering the proposed metric (\"our approach\" with skill rating) achieves a Spearman correlation of 0.764 and a Pearson correlation of 0.783 in dialogue tasks according to the model-level correlation data provided in Image 3.", "all_figures_names": ["7-Table4-1.png", "7-Table3-1.png", "6-Table1-1.png", "6-Table2-1.png", "7-Table5-1.png", "4-Figure1-1.png"], "referred_figures_names": ["6-Table2-1.png", "6-Table1-1.png"], "answer": "Pearson correlation to human judgement - proposed vs next best metric\nSample level comparison:\n- Story generation: 0.387 vs 0.148\n- Dialogue: 0.472 vs 0.341\nModel level comparison:\n- Story generation:  0.631 vs 0.302\n- Dialogue: 0.783 vs 0.553"}}