{"483": {"question": "Do they conduct any human evaluation?", "referred_figures_indices": [1], "response": "{'Image': 3, 'Rationale': 'Image 3 contains a table with model performances that include evaluation metrics, suggesting some form of human evaluation.'}\n\nThe answer is: Yes, they conduct human evaluation.", "all_figures_names": ["7-Table7-1.png", "2-Figure1-1.png", "4-Table1-1.png", "5-Table3-1.png", "6-Table5-1.png", "6-Table4-1.png", "4-Figure3-1.png", "2-Figure2-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "Yes"}, "485": {"question": "How do they enrich the positional embedding with length information", "referred_figures_indices": [7], "response": "{'Image': 1, 'Rationale': 'Image 1 shows a categorization of training data after assigning length tokens, which is directly related to how length information is used in the enrichment of positional embeddings.'}\n\nThe answer is: They enrich the positional embedding with length information by classifying the training data into different length categories (short, normal, long) and assigning appropriate length tokens to the data, as shown in Image 1. These length tokens provide the model with explicit information about the length of the sequences it is processing.", "all_figures_names": ["4-Table1-1.png", "4-Table2-1.png", "6-Table4-1.png", "2-Figure2-1.png", "2-Figure1-1.png", "7-Table7-1.png", "6-Table5-1.png", "4-Figure3-1.png"], "referred_figures_names": ["4-Figure3-1.png"], "answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative)."}, "486": {"question": "How do they condition the output to a given target-source class?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Rationale': 'This image directly demonstrates the use of different length classes during training to produce outputs of varied lengths during inference, which directly addresses controlling output length.'}\n\nThe answer is: They condition the output to a given target-source class by training the neural machine translation (NMT) system with length ratio classes, allowing the generation of outputs of different lengths at inference time.", "all_figures_names": ["7-Table7-1.png", "6-Table5-1.png", "2-Figure1-1.png", "4-Table2-1.png", "2-Figure2-1.png", "4-Figure3-1.png", "4-Table1-1.png", "6-Table4-1.png"], "referred_figures_names": ["2-Figure2-1.png"], "answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group."}}