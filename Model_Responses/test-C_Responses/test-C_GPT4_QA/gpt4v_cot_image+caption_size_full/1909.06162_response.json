{"909": {"question": "What is best performing model among author's submissions, what performance it had?", "referred_figures_indices": [4], "response": "{'Image': 1, 'Rationale': 'This image contains a table with detailed performance metrics (F1, Precision (P), Recall (R)) for various models and ensembles the author submitted, which directly answers the question about the best performing model and its performance.'}\n\nThe answer is: r22 (Ensemble of (r4, r7, r12) within Fold1 majority-voting |M| > 50%) with an F1 score of 0.669 on Dev (internal).", "all_figures_names": ["3-Figure1-1.png", "4-Table3-1.png", "2-Table1-1.png", "4-Table4-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."}, "910": {"question": "What extracted features were most influencial on performance?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Rationale': 'This table shows different classifiers and feature combinations with corresponding performance metrics, which is directly related to the question about influential features on performance.'}\n\nThe answer is: The most influential features on performance were the ensemble of BERT with Fine-tuning and linguistic, layout, and topical features, which achieved the highest scores across F1, Precision, and Recall in the ensemble categories, as shown in rows r22 and r23 for the Dev (external) data.", "all_figures_names": ["4-Table3-1.png", "2-Table1-1.png", "4-Table4-1.png", "3-Table2-1.png", "3-Figure1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "Linguistic"}, "912": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'This image provides direct comparison of F1 scores for models with and without ensemble approaches on the same task and data fold.'}\n\nThe answer is: Yes, ensemble schemes helped in boosting performance; the best ensemble variation (Ensemble+ with relax-voting, |M| >= 30%) improved F1 score from 0.614 for the best single model to 0.679 for Dev (internal) and from 0.638 to 0.747 for Dev (external).", "all_figures_names": ["4-Table4-1.png", "3-Table2-1.png", "4-Table3-1.png", "3-Figure1-1.png", "2-Table1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "The best ensemble topped the best single model by 0.029 in F1 score on dev (external)."}, "913": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [4, 3, 0], "response": "{'Image': 0, 'Rationale': 'This image directly compares the system in question (MIC-CIS) with other participants\u2019 results, showing performance metrics that indicate if ensemble schemes helped in boosting performance.'}\n\nThe answer is: Yes, ensemble schemes did help in boosting performance. If we look at the results for the MIC-CIS system in Image 0, we see that for the SLC task it achieved an F1 score of .6231, which is an improvement over its solo components listed in Image 3 (.618 for an ensemble of II and IV within Fold3). For the FLC task, there is", "all_figures_names": ["3-Table2-1.png", "2-Table1-1.png", "3-Figure1-1.png", "4-Table4-1.png", "4-Table3-1.png"], "referred_figures_names": ["4-Table3-1.png", "4-Table4-1.png", "3-Table2-1.png"], "answer": "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"}, "914": {"question": "Which basic neural architecture perform best by itself?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Rationale': 'The table provides a direct comparison of various basic neural architectures and their performance metrics, which is precisely what is needed to answer the question.'}\n\nThe answer is: BERT with Fine-tuning.", "all_figures_names": ["4-Table3-1.png", "3-Table2-1.png", "2-Table1-1.png", "4-Table4-1.png", "3-Figure1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "BERT"}, "915": {"question": "What participating systems had better results than ones authors submitted?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'This image provides a comparison of the scores for the SLC task between different participating teams, which is directly relevant to assessing which systems had better results.'}\n\nThe answer is: The systems with better results than the one the authors submitted for the SLC task are Ituorp, ProperGander, and YMJ, as shown by their F1 scores in Image 1.", "all_figures_names": ["4-Table3-1.png", "3-Table2-1.png", "3-Figure1-1.png", "4-Table4-1.png", "2-Table1-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."}, "916": {"question": "What is specific to multi-granularity and multi-tasking neural arhiteture design?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'This image provides a system description with multi-granularity through the SLC layer and multi-tasking with the FLC layer in a neural architecture, which is relevant to the question.'}\n\nThe answer is: Multi-granularity and multi-tasking neural architecture design involve creating systems that can operate at different levels of detail (granularity) and perform multiple tasks simultaneously. Specifically, this design can integrate various features and classifiers for single-label classification (SLC) and use a multi-tasking LSTM-CRF architecture for fine-grained label classification (FLC), allowing the system to handle", "all_figures_names": ["4-Table3-1.png", "2-Table1-1.png", "3-Figure1-1.png", "3-Table2-1.png", "4-Table4-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "An output layer for each task"}}