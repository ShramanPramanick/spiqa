{"182": {"question": "Do they inspect their model to see if their model learned to associate image parts with words related to entities?", "referred_figures_indices": [5], "response": "{'Image': 1, 'Rationale': 'This image shows an example of modality attention in action, with attention distribution over words, characters, and visual cues, which directly relates to the question about associating image parts with words.'}\n\nThe answer is: Yes, they inspect their model to see if it learned to associate image parts with words related to entities, as demonstrated by the modality attention mechanism which shows how the model focuses on different modalities (words, characters, visual) when making predictions.", "all_figures_names": ["8-Table3-1.png", "1-Figure1-1.png", "3-Figure2-1.png", "6-Table1-1.png", "7-Table2-1.png", "8-Figure3-1.png"], "referred_figures_names": ["8-Figure3-1.png"], "answer": "Yes"}, "183": {"question": "Do they inspect their model to see if their model learned to associate image parts with words related to entities?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'Provides examples of how image information contributes to the model\u2019s entity recognition, thus showing an inspection of association between image parts and words.'}\n\nThe answer is: Yes, they inspect their model to see if their model learned to associate image parts with words related to entities. The error analysis in the provided image demonstrates how visual tags contribute to the prediction of named entities in captions, showing an inspection of the association between image parts and related words.", "all_figures_names": ["3-Figure2-1.png", "6-Table1-1.png", "7-Table2-1.png", "8-Figure3-1.png", "8-Table3-1.png", "1-Figure1-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}}