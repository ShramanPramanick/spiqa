{"204": {"question": "Which baselines did they compare against?", "referred_figures_indices": [5, 0], "response": "{'Image': 0, 'Rationale': 'The table contains a comprehensive list of models, indicating it provides direct information about the baselines the authors compared against.'}\n\nThe answer is: They compared against several models, including: RNTN, AdaMC-RNTN, TE-RNTN, TBCNN, Tree-LSTM, AdaHT-LSTM-CM, DC-TreeLSTM, TE-LSTM, BiConTree, Gumbel Tree-LSTM, TreeNet, CNN, AdaSent, LSTM-CNN, byte-mLSTM, BCN + Char + CoVe, and BCN + Char", "all_figures_names": ["6-Table1-1.png", "7-Figure4-1.png", "3-Figure1-1.png", "7-Figure3-1.png", "4-Figure2-1.png", "6-Table2-1.png"], "referred_figures_names": ["6-Table2-1.png", "6-Table1-1.png"], "answer": "Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks"}, "205": {"question": "Which baselines did they compare against?", "referred_figures_indices": [0, 2], "response": "{'Image': 0, 'Rationale': 'This table lists the different models that were used as baselines for comparison in the context of the Stanford Natural Language Inference, which directly answers the question.'}\n\nThe answer is: They compared against \"100D Latent Syntax Tree-LSTM\", \"300D Tree-based CNN\", \"300D SPINN-PI\", \"300D Gumbel Tree-LSTM\", \"300D Neural Self-Attention Network (NSE)\", \"300D Reinforced Self-Attention Network\", \"600D Residual stacked encoders\", and \"600D BiLSTM with generalized pooling\".", "all_figures_names": ["6-Table2-1.png", "4-Figure2-1.png", "6-Table1-1.png", "7-Figure4-1.png", "7-Figure3-1.png", "3-Figure1-1.png"], "referred_figures_names": ["6-Table2-1.png", "6-Table1-1.png"], "answer": "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."}}