{"1259": {"question": "By how much does their method outperform the multi-head attention model?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Rationale': 'The table in Image 1 shows the comparative performance results between different methods, including the multi-head attention model (MHA) variants and their method (termed as HMI D). This allows for direct numerical comparison.'}\n\nThe answer is: Based on the results from Task 2, which is where their method (HMHID) performed best relative to the MHA variants, their method outperformed the MHA-Dot by 0.8%, the MHA-Add by 0.5%, and the MHA-Loc by 0.9%.", "all_figures_names": ["3-Figure2-1.png", "4-Table2-1.png", "2-Figure1-1.png", "4-Table1-1.png", "4-Figure3-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points."}, "1260": {"question": "How large is the corpus they use?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Rationale': 'This image contains explicit numerical data regarding the training corpus size which is necessary to answer the question about the corpus size.'}\n\nThe answer is: 445,068 utterances (581 hours).", "all_figures_names": ["3-Figure2-1.png", "4-Table2-1.png", "4-Table1-1.png", "4-Figure3-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "449050"}}