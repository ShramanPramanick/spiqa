{"585": {"question": "What evaluations did the authors use on their system?", "referred_figures_indices": [1, 0, 5], "response": "{'Answer': 'The authors used evaluations such as BLEU scores on the NIST Chinese-English translation tasks, average cosine similarity of adjacent sentences (coherence) on all test sets, and the average number of words in translations generated by the model that are also in the dynamic and topic cache.'}", "all_figures_names": ["8-Table1-1.png", "10-Table6-1.png", "4-Figure1-1.png", "9-Table4-1.png", "8-Table2-1.png", "9-Table3-1.png", "5-Figure2-1.png", "5-Figure3-1.png"], "referred_figures_names": ["10-Table6-1.png", "8-Table1-1.png", "9-Table3-1.png"], "answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."}}