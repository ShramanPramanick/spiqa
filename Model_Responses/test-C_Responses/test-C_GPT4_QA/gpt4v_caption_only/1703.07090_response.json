{"801": {"question": "how small of a dataset did they train on?", "referred_figures_indices": [4], "response": "{'Answer': 'The input captions do not provide information on the size of the dataset used for training.'}", "all_figures_names": ["6-Table3-1.png", "6-Table4-1.png", "5-Figure1-1.png", "6-Table2-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "23085 hours of data"}, "802": {"question": "what was their character error rate?", "referred_figures_indices": [4, 2, 1], "response": "{'Answer': 'The character error rate (CER) varied depending on the model and training criteria used, as shown in Tables 1, 2, 3, and 4.'}", "all_figures_names": ["4-Table1-1.png", "6-Table4-1.png", "6-Table2-1.png", "5-Figure1-1.png", "6-Table3-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png", "6-Table4-1.png"], "answer": "2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning."}, "803": {"question": "what was their character error rate?", "referred_figures_indices": [0, 1], "response": "{'Answer': 'The character error rate (CER) varied among different models and training methods. Specific CER values are not provided in the input captions.'}", "all_figures_names": ["6-Table3-1.png", "6-Table2-1.png", "4-Table1-1.png", "6-Table4-1.png", "5-Figure1-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Their best model achieved a 2.49% Character Error Rate."}, "804": {"question": "which lstm models did they compare with?", "referred_figures_indices": [0, 2], "response": "{'Answer': '9-layers, 2-layers regular-trained, 2-layers distilled, Shenma distilled, Amap further trained, Shenma trained with sMBR, 6 to 9-layers trained by regular Xavier Initialization, layer-wise training with CE criterion, CE + sMBR criteria'}", "all_figures_names": ["6-Table3-1.png", "6-Table4-1.png", "6-Table2-1.png", "4-Table1-1.png", "5-Figure1-1.png"], "referred_figures_names": ["6-Table3-1.png", "6-Table2-1.png"], "answer": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."}}