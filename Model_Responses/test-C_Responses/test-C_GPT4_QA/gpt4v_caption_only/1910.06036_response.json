{"654": {"question": "How they extract \"structured answer-relevant relation\"?", "referred_figures_indices": [6], "response": "{'Answer': 'They extract \"structured answer-relevant relation\" by using a model that compares sentences with the target question to identify non-stop tokens co-occurring in both the sentence/relation and the question, and calculates the proportion of source tokens used in the question. They also use OpenIE for n-ary extractions from sentences and select relations based on specific criteria.'}", "all_figures_names": ["4-Figure3-1.png", "3-Table2-1.png", "7-Table5-1.png", "5-Table3-1.png", "6-Table4-1.png", "2-Table1-1.png", "1-Figure1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["1-Figure1-1.png"], "answer": "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."}, "656": {"question": "How big are significant improvements?", "referred_figures_indices": [3], "response": "{'Answer': 'The captions provided do not contain information about the size of significant improvements.'}", "all_figures_names": ["8-Figure5-1.png", "4-Figure3-1.png", "2-Table1-1.png", "6-Table4-1.png", "5-Table3-1.png", "7-Figure4-1.png", "1-Figure1-1.png", "3-Figure2-1.png"], "referred_figures_names": ["6-Table4-1.png"], "answer": "Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"}}