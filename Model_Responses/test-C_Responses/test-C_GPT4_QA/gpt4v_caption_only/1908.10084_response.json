{"434": {"question": "What other sentence embeddings methods are evaluated?", "referred_figures_indices": [5, 7], "response": "{'Answer': 'SBERT-NLI, concatenation methods with MEAN pooling strategy, and SentEval toolkit evaluation'}", "all_figures_names": ["3-Figure2-1.png", "5-Table2-1.png", "6-Table4-1.png", "7-Table6-1.png", "3-Figure1-1.png", "6-Table3-1.png", "7-Table5-1.png", "4-Table1-1.png"], "referred_figures_names": ["6-Table3-1.png", "4-Table1-1.png"], "answer": "GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent"}, "435": {"question": "What other sentence embeddings methods are evaluated?", "referred_figures_indices": [6], "response": "{'Answer': 'SBERT trained with classification objective function, SBERT trained with regression objective function, SBERT-NLI pretrained on NLI datasets and fine-tuned on STSb, and SBERT trained with triplet loss.'}", "all_figures_names": ["7-Table6-1.png", "3-Figure1-1.png", "8-Table7-1.png", "3-Figure2-1.png", "5-Table2-1.png", "6-Table3-1.png", "7-Table5-1.png", "6-Table4-1.png"], "referred_figures_names": ["7-Table5-1.png"], "answer": "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."}}