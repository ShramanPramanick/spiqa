{"856": {"question": "What baseline did they compare Entity-GCN to?", "referred_figures_indices": [5], "response": "{'Answer': 'GloVe with and without R-GCN'}", "all_figures_names": ["1-Figure1-1.png", "13-Figure3-1.png", "8-Table4-1.png", "7-Table3-1.png", "12-Table6-1.png", "6-Table2-1.png", "3-Figure2-1.png", "11-Table5-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}, "858": {"question": "Did they use a relation extraction method to construct the edges in the graph?", "referred_figures_indices": [6], "response": "Based on Caption 6, it describes nodes being connected by three specific relations: co-occurrence in the same document, exact match, and coreference. These are types of relations, hence relation extraction methods were used to construct the edges in the graph.\n\n{'Answer': 'Yes'}", "all_figures_names": ["5-Table1-1.png", "1-Figure1-1.png", "13-Figure3-1.png", "11-Table5-1.png", "12-Table6-1.png", "6-Table2-1.png", "3-Figure2-1.png", "8-Table4-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "No"}, "859": {"question": "How did they get relations between mentions?", "referred_figures_indices": [2], "response": "{'Answer': 'Nodes are connected by three simple relations: one indicating co-occurrence in the same document, another connecting mentions that exactly match, and a third one indicating a coreference.'}", "all_figures_names": ["8-Table4-1.png", "6-Table2-1.png", "3-Figure2-1.png", "11-Table5-1.png", "12-Table6-1.png", "13-Figure3-1.png", "1-Figure1-1.png", "7-Table3-1.png"], "referred_figures_names": ["3-Figure2-1.png"], "answer": "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."}, "862": {"question": "What performance does the Entity-GCN get on WIKIHOP?", "referred_figures_indices": [3], "response": "Based on the provided captions, the performance of Entity-GCN on WIKIHOP can be found in Caption 3, which mentions the accuracy of different models on the WIKIHOP closed test set and public validation set. However, the exact numerical performance is not given in the provided captions.\n\nTherefore, the answer is inferred from the context:\n\n{'Answer': 'Outperforms recent prior work'}", "all_figures_names": ["8-Table4-1.png", "11-Table5-1.png", "12-Table6-1.png", "6-Table2-1.png", "3-Figure2-1.png", "5-Table1-1.png", "13-Figure3-1.png", "1-Figure1-1.png"], "referred_figures_names": ["6-Table2-1.png"], "answer": "During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"}}