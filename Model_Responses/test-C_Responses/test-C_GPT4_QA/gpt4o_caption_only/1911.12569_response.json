{"362": {"question": "What was their result on Stance Sentiment Emotion Corpus?", "referred_figures_indices": [5], "response": "{'Answer': 'The provided captions do not offer direct information about the results on the Stance Sentiment Emotion Corpus (SSEC). Captions primarily discuss various datasets and models, including comparisons and metrics, but specific results on SSEC are not mentioned.'}", "all_figures_names": ["5-Figure2-1.png", "5-TableIII-1.png", "7-TableXI-1.png", "5-TableI-1.png", "6-TableIV-1.png", "5-TableII-1.png", "3-Figure1-1.png"], "referred_figures_names": ["5-TableII-1.png"], "answer": "F1 score of 66.66%"}, "363": {"question": "What performance did they obtain on the SemEval dataset?", "referred_figures_indices": [5], "response": "Based on the input captions, there is no direct information provided about the performance results obtained on the SemEval dataset. Input captions only mention dataset statistics and comparisons with other systems, but do not disclose the actual performance metrics.\n\n```json\n{'Answer': 'Not provided in the input captions'}\n```", "all_figures_names": ["6-TableIV-1.png", "5-TableI-1.png", "5-TableIII-1.png", "5-Figure2-1.png", "3-Figure1-1.png", "5-TableII-1.png", "7-TableXI-1.png"], "referred_figures_names": ["5-TableII-1.png"], "answer": "F1 score of 82.10%"}, "364": {"question": "What are the state-of-the-art systems?", "referred_figures_indices": [1, 6], "response": "{'Answer': 'The state-of-the-art systems are those compared in TABLE III for the Semeval 2016 Task 6 sentiment dataset and those proposed by [16] in TABLE IV for the emotion dataset.'}", "all_figures_names": ["5-TableI-1.png", "5-TableIII-1.png", "3-Figure1-1.png", "7-TableXI-1.png", "5-Figure2-1.png", "5-TableII-1.png", "6-TableIV-1.png"], "referred_figures_names": ["5-TableIII-1.png", "6-TableIV-1.png"], "answer": "For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN"}}