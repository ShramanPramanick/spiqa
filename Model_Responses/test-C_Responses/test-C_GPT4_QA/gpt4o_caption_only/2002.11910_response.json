{"180": {"question": "What are previous state of the art results?", "referred_figures_indices": [0], "response": "{'Answer': 'The previous state of the art results are the ones produced by the two previous models mentioned in Table 1, which this study outperforms in precision, recall, and F1 scores for named entity and nominal mention tasks.'}", "all_figures_names": ["4-Table1-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "Overall F1 score:\n- He and Sun (2017) 58.23\n- Peng and Dredze (2017) 58.99\n- Xu et al. (2018) 59.11"}, "181": {"question": "What are previous state of the art results?", "referred_figures_indices": [0], "response": "```json\n{'Answer': 'The results of two previous models are provided in Table 1, showing their precision, recall, and F1 scores for both named entity and nominal mention tasks.'}\n```", "all_figures_names": ["4-Table1-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%"}}