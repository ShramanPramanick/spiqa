{"2": {"question": "What are the results?", "referred_figures_indices": [4, 1], "response": "```json\n{'Answer': 'Results are provided in Table 4 for small labeled training data, and in Table 3 for the performance of various models on the ACP test set.'}\n```", "all_figures_names": ["4-Table2-1.png", "5-Table4-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["5-Table3-1.png", "5-Table4-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [1], "response": "{'Answer': 'Relations are used to propagate polarity by linking events with specific discourse relations, such as CAUSE and CONCESSION. The latter event\u2019s polarity is automatically identified and then propagated to the former event. For the CAUSE relation, the same polarity is assigned to the former event, whereas for the CONCESSION relation, the polarity is reversed.'}", "all_figures_names": ["5-Table4-1.png", "2-Figure1-1.png", "4-Table2-1.png", "4-Table1-1.png", "5-Table5-1.png", "5-Table3-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [1], "response": "{'Answer': 'Relations are used to propagate polarity by focusing on pairs of events connected with discourse relations such as CAUSE or CONCESSION. In the AL dataset, the latter event\u2019s polarity is propagated to the former event based on a seed lexicon. For the CAUSE relation, the same polarity is used, while for the CONCESSION relation, the polarity is reversed. For CA and CO datasets, the events\u2019 polarities are encouraged to be the same (CA) or reversed (CO), depending on the discourse relation.'}", "all_figures_names": ["5-Table5-1.png", "2-Figure1-1.png", "4-Table2-1.png", "5-Table4-1.png", "5-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [4, 5], "response": "```json\n{'Answer': 'The size of the Japanese data is given in Table 1.'}\n```", "all_figures_names": ["2-Figure1-1.png", "5-Table5-1.png", "5-Table3-1.png", "5-Table4-1.png", "4-Table2-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png", "4-Table1-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [3], "response": "```json\n{\"Answer\": \"The input captions do not provide information about the size of the Japanese data.\"}\n```", "all_figures_names": ["5-Table3-1.png", "4-Table1-1.png", "5-Table5-1.png", "4-Table2-1.png", "5-Table4-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [3], "response": "{'Answer': 'The improvements of supervised learning results trained on smaller labeled data enhanced with the proposed approach compared to the basic approach can be found in Table 4.'}", "all_figures_names": ["4-Table2-1.png", "4-Table1-1.png", "5-Table3-1.png", "5-Table4-1.png", "2-Figure1-1.png", "5-Table5-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [2], "response": "{'Answer': 'Their model learns using mostly raw data by propagating the polarity of latter events to former events based on discourse relations, with polarity being automatically identified via a seed lexicon.'}", "all_figures_names": ["5-Table4-1.png", "4-Table1-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table3-1.png", "4-Table2-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [5], "response": "```json\n{\n  \"Answer\": \"The seed lexicon size is not specified in the given captions.\"\n}\n```", "all_figures_names": ["2-Figure1-1.png", "5-Table3-1.png", "5-Table5-1.png", "5-Table4-1.png", "4-Table2-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}