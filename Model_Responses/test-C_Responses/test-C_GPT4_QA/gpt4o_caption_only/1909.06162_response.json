{"909": {"question": "What is best performing model among author's submissions, what performance it had?", "referred_figures_indices": [1], "response": "Based on the provided captions, it appears that details about the performance and comparisons of various models are given in tables related to both SLC (Sentence-Level Classification) and FLC (Fragment-Level Classification). However, specific values indicating the best performance among the author's submissions are not provided in the captions.\n\nTherefore, I conclude:\n\n{'Answer': 'Insufficient information provided in captions to determine the best performing model and its performance.'}", "all_figures_names": ["3-Figure1-1.png", "3-Table2-1.png", "4-Table4-1.png", "2-Table1-1.png", "4-Table3-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."}, "910": {"question": "What extracted features were most influencial on performance?", "referred_figures_indices": [3], "response": "{'Answer': 'Pre-trained word embeddings from FastText and BERT were among the most influential features on performance, as indicated by their usage in the system descriptions for single label classification (SLC) and multi-grained propaganda detection (FLC) tasks.'}", "all_figures_names": ["4-Table4-1.png", "3-Table2-1.png", "3-Figure1-1.png", "4-Table3-1.png", "2-Table1-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "Linguistic"}, "912": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [0], "response": "```json\n{\n  \"Answer\": \"Yes, Table 4 shows the scores comparing different models, features, and ensembles, indicating that ensemble schemes did help in boosting performance. However, the exact boost in performance isn't specified in the provided captions.\"\n}\n```", "all_figures_names": ["4-Table3-1.png", "3-Figure1-1.png", "4-Table4-1.png", "2-Table1-1.png", "3-Table2-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "The best ensemble topped the best single model by 0.029 in F1 score on dev (external)."}, "913": {"question": "Did ensemble schemes help in boosting peformance, by how much?", "referred_figures_indices": [2, 3, 0], "response": "{'Answer': 'The captions provide information on different models, features, classifiers, and scores but do not provide specific information on the performance boost due to ensemble schemes.'}", "all_figures_names": ["3-Table2-1.png", "2-Table1-1.png", "4-Table4-1.png", "4-Table3-1.png", "3-Figure1-1.png"], "referred_figures_names": ["4-Table4-1.png", "4-Table3-1.png", "3-Table2-1.png"], "answer": "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"}, "914": {"question": "Which basic neural architecture perform best by itself?", "referred_figures_indices": [3], "response": "{'Answer': 'CNN'}", "all_figures_names": ["2-Table1-1.png", "3-Table2-1.png", "3-Figure1-1.png", "4-Table3-1.png", "4-Table4-1.png"], "referred_figures_names": ["4-Table3-1.png"], "answer": "BERT"}, "915": {"question": "What participating systems had better results than ones authors submitted?", "referred_figures_indices": [2], "response": "Based on the input captions, specifically Caption 2 which provides a comparison of systems, the answer to the question is not explicitly given. However, since the table compares the MIC-CIS system with the top-5 participants on test scores for SLC and FLC, we can infer that the comparison results might show which systems performed better than the author-submitted ones.\n\nTo get a definitive answer, you would need to refer directly to the scores within the table in Caption 2. Since this table is not provided, I can't give the exact participating systems that had better results. If we assume that the table provided information indicating superior performance by", "all_figures_names": ["4-Table4-1.png", "3-Figure1-1.png", "3-Table2-1.png", "4-Table3-1.png", "2-Table1-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."}, "916": {"question": "What is specific to multi-granularity and multi-tasking neural arhiteture design?", "referred_figures_indices": [4], "response": "{'Answer': 'The multi-granularity and multi-tasking neural architecture design is specific in its use of a multi-tasking LSTM-CRF architecture that jointly performs Propaganda Fragment Detection (PFD), Fragment-level Classification (FLC), and Sentence-level Classification (SLC) tasks.'}", "all_figures_names": ["2-Table1-1.png", "4-Table4-1.png", "3-Table2-1.png", "4-Table3-1.png", "3-Figure1-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "An output layer for each task"}}