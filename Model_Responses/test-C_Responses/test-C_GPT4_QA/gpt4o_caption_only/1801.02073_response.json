{"1000": {"question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "referred_figures_indices": [1], "response": "{'Answer': 'No'}", "all_figures_names": ["2-Table1-1.png", "3-Table2-1.png", "3-Figure1-1.png", "3-Figure2-1.png", "4-Table3-1.png"], "referred_figures_names": ["3-Table2-1.png"], "answer": "Yes"}, "1003": {"question": "How many question types do they find in the datasets analyzed?", "referred_figures_indices": [0], "response": "Based on the given captions, it appears that there is no direct mention of the specific number of question types found in the analyzed datasets. However, the focus on the distributions of question types and the information about different corpora suggests that the underlying datasets might involve several question types. Unfortunately, the exact number of question types is not provided in the captions.\n\n{'Answer': 'Not specified'}", "all_figures_names": ["3-Figure1-1.png", "2-Table1-1.png", "3-Figure2-1.png", "4-Table3-1.png", "3-Table2-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "7"}, "1004": {"question": "How do they analyze contextual similaries across datasets?", "referred_figures_indices": [1], "response": "{'Answer': 'They analyze contextual similarities across datasets by comparing results for answer selection and triggering across different corpora splits, as shown in the tables and figures provided.'}", "all_figures_names": ["3-Figure1-1.png", "2-Table1-1.png", "3-Figure2-1.png", "4-Table3-1.png", "3-Table2-1.png"], "referred_figures_names": ["2-Table1-1.png"], "answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."}}