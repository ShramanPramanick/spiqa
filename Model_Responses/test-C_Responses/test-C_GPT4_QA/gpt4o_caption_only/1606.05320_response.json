{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "referred_figures_indices": [1, 2], "response": "{'Answer': 'The HMM models use features related to spaces and comments in the text. The interpretability of those features is indicated by their ability to visualize the states, as seen in the decision tree predicting hidden state dimensions and the hybrid model visualizations.'}", "all_figures_names": ["3-Table1-1.png", "4-Figure3-1.png", "4-Figure2-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "referred_figures_indices": [1, 2], "response": "{'Answer': 'The HMM components learn some complementary features in the text related to spaces and comments.'}", "all_figures_names": ["3-Table1-1.png", "4-Figure3-1.png", "4-Figure2-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Figure3-1.png", "4-Figure2-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "referred_figures_indices": [1], "response": "{'Answer': 'The captions provided do not contain direct numerical or qualitative information regarding the performance gap between HMMs and LSTMs.'}", "all_figures_names": ["2-Figure1-1.png", "3-Table1-1.png", "4-Figure3-1.png", "4-Figure2-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}