{"845": {"question": "By how much does their best model outperform the state-of-the-art?", "referred_figures_indices": [4], "response": "{'Answer': 'Their best model outperforms the state-of-the-art by 0.5 F1 points.'}", "all_figures_names": ["5-Table2-1.png", "4-Table1-1.png", "3-Figure2-1.png", "3-Figure1-1.png", "5-Table3-1.png"], "referred_figures_names": ["5-Table3-1.png"], "answer": "0.8% F1 better than the best state-of-the-art"}, "846": {"question": "By how much does their best model outperform the state-of-the-art?", "referred_figures_indices": [4], "response": "{'Answer': 'Their best model (ER-CNN + R-RNN) outperforms the state-of-the-art (CR-CNN) by 0.8.'}", "all_figures_names": ["3-Figure2-1.png", "5-Table2-1.png", "4-Table1-1.png", "3-Figure1-1.png", "5-Table3-1.png"], "referred_figures_names": ["5-Table3-1.png"], "answer": "Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1."}, "852": {"question": "How do they obtain the new context represetation?", "referred_figures_indices": [1], "response": "{\n  \"Answer\": \"The new context representation is obtained by concatenating the outputs of the pooling layers after the convolution layers, as shown in Image 1.\"\n}", "all_figures_names": ["4-Table1-1.png", "3-Figure1-1.png", "5-Table2-1.png", "5-Table3-1.png", "3-Figure2-1.png"], "referred_figures_names": ["3-Figure1-1.png"], "answer": "They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."}}