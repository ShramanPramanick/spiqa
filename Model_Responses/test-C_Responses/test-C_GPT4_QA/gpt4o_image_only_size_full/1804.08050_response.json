{"1259": {"question": "By how much does their method outperform the multi-head attention model?", "referred_figures_indices": [2], "response": "{'Answer': 'Their method (HMHD 2xLoc+2xCov) outperforms the multi-head attention model (MHA-Add) by 0.3% in Task 1, 0.5% in Task 2, and 0.2% in Task 3.'}", "all_figures_names": ["3-Figure2-1.png", "2-Figure1-1.png", "4-Table2-1.png", "4-Figure3-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points."}, "1260": {"question": "How large is the corpus they use?", "referred_figures_indices": [4], "response": "{'Answer': '445,068 utterances (581 hours)'}", "all_figures_names": ["4-Table2-1.png", "4-Figure3-1.png", "3-Figure2-1.png", "2-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "449050"}}