{"747": {"question": "How do the backoff strategies work?", "referred_figures_indices": [0], "response": "{'Answer': 'The backoff strategies in the given models involve utilizing semi-character representations and hierarchical modeling to enhance performance on specific tasks. Image 0 illustrates the interplay between the Background Model and the Foreground Model, transitioning to semi-character representations for improved recognition. Image 2 and Image 4 provide empirical results showing that the \"Background\" strategy offers superior performance over \"Pass-Through\" and \"Neutral\" across different criteria, such as word recognition error rates and sensitivity to alterations. Additionally, Image 5 and Image 6 demonstrate that backoff strategies improve resilience to various types of attacks in sentiment analysis models, where the \"Background\"", "all_figures_names": ["4-Figure1-1.png", "8-Figure2-1.png", "5-Table2-1.png", "1-Table1-1.png", "8-Table5-1.png", "7-Table3-1.png", "7-Table4-1.png"], "referred_figures_names": ["4-Figure1-1.png"], "answer": "In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus."}, "748": {"question": "How do the backoff strategies work?", "referred_figures_indices": [4], "response": "{'Answer': 'Backoff strategies, including Pass-Through, Background, and Neutral methods, allow models to handle attacks on text by using different fallback mechanisms. Pass-Through tends to allow the original malformed inputs to pass through, Background incorporates a secondary model to mitigate the impact, and Neutral adds a neutralizing mechanism. These strategies generally improve model robustness against character-level attacks, as indicated by performance metrics across various models and tasks such as sentiment analysis and word recognition.'}", "all_figures_names": ["7-Table4-1.png", "7-Table3-1.png", "5-Table2-1.png", "1-Table1-1.png", "4-Figure1-1.png", "8-Table5-1.png", "8-Figure2-1.png"], "referred_figures_names": ["4-Figure1-1.png"], "answer": "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus."}, "749": {"question": "How do the backoff strategies work?", "referred_figures_indices": [6], "response": "{'Answer': 'Backoff strategies refer to different mechanisms used to handle OOV (Out-Of-Vocabulary) tokens in models. Common backoff strategies include: Pass-Through, Background, and Neutral. Pass-Through retains OOV tokens while processing. Background employs a secondary model to handle such tokens. Neutral attempts to replace them with neutral words. These strategies are vital for improving model robustness, particularly in handling adversarial attacks, as evidenced by their performance metrics across various models and scenarios.'}", "all_figures_names": ["7-Table3-1.png", "1-Table1-1.png", "8-Table5-1.png", "5-Table2-1.png", "7-Table4-1.png", "8-Figure2-1.png", "4-Figure1-1.png"], "referred_figures_names": ["4-Figure1-1.png"], "answer": "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"}}