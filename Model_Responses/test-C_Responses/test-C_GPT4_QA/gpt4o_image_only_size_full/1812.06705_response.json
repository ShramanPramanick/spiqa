{"247": {"question": "How do the authors measure performance?", "referred_figures_indices": [4], "response": "{'Answer': 'The authors measure performance using various datasets (SST5, SST2, Subj, TREC, MPQA, RT) and evaluating metrics such as accuracy across different models (CNN and RNN) and setups (with synonyms, context, context+label, BERT, and C-BERT). The results are then compared and averaged.'}", "all_figures_names": ["5-Table1-1.png", "7-Table3-1.png", "5-Figure1-1.png", "8-Table4-1.png", "7-Table2-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "Accuracy across six datasets"}, "248": {"question": "Does the new objective perform better than the original objective bert is trained on?", "referred_figures_indices": [4], "response": "{'Answer': 'Yes, the new objective (C-BERT) performs better than the original BERT, as indicated by the higher average scores across various benchmarks.'}", "all_figures_names": ["5-Figure1-1.png", "7-Table3-1.png", "5-Table1-1.png", "8-Table4-1.png", "7-Table2-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}, "249": {"question": "Are other pretrained language models also evaluated for contextual augmentation? ", "referred_figures_indices": [4], "response": "{'Answer': 'Yes'}", "all_figures_names": ["7-Table3-1.png", "8-Table4-1.png", "5-Table1-1.png", "5-Figure1-1.png", "7-Table2-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "No"}, "250": {"question": "Do the authors report performance of conditional bert on tasks without data augmentation?", "referred_figures_indices": [1], "response": "{'Answer': 'No'}", "all_figures_names": ["7-Table3-1.png", "7-Table2-1.png", "5-Figure1-1.png", "8-Table4-1.png", "5-Table1-1.png"], "referred_figures_names": ["7-Table2-1.png"], "answer": "Yes"}}