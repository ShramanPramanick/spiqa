{"654": {"question": "How they extract \"structured answer-relevant relation\"?", "referred_figures_indices": [5], "response": "{'Answer': 'They extract \"structured answer-relevant relation\" by parsing sentences into key components and generating relations that are focused on the answer-relevant parts of the text, using techniques like N-ary relations and dual copy mechanisms.'}", "all_figures_names": ["6-Table4-1.png", "3-Table2-1.png", "8-Figure5-1.png", "3-Figure2-1.png", "4-Figure3-1.png", "1-Figure1-1.png", "7-Figure4-1.png", "2-Table1-1.png"], "referred_figures_names": ["1-Figure1-1.png"], "answer": "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."}, "656": {"question": "How big are significant improvements?", "referred_figures_indices": [5], "response": "{\n    'Answer': 'Significant improvements in BLEU, MET, and R-L scores are shown. For >20 word sentences, Our Model's BLEU improved by 7.09% and MET by 4.30%.'\n}", "all_figures_names": ["7-Figure4-1.png", "7-Table5-1.png", "3-Table2-1.png", "2-Table1-1.png", "8-Figure5-1.png", "6-Table4-1.png", "1-Figure1-1.png", "4-Figure3-1.png"], "referred_figures_names": ["6-Table4-1.png"], "answer": "Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"}}