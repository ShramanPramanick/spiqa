{"31": {"question": "What experiments are proposed to test that upper layers produce context-specific embeddings?", "referred_figures_indices": [2, 0, 3], "response": "{'Answer': 'The experiments proposed to test that upper layers produce context-specific embeddings include measuring average intra-sentence similarity, average explainable variance, average cosine similarity between randomly sampled words, and average self-similarity across different layers of language models such as ELMo, BERT, and GPT-2.'}", "all_figures_names": ["7-Figure3-1.png", "8-Figure4-1.png", "5-Figure1-1.png", "6-Figure2-1.png", "8-Table1-1.png"], "referred_figures_names": ["5-Figure1-1.png", "7-Figure3-1.png", "6-Figure2-1.png"], "answer": "They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  \nThey plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.\nThey plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another."}, "32": {"question": "How do they calculate a static embedding for each word?", "referred_figures_indices": [4], "response": "{'Answer': 'They calculate a static embedding for each word by utilizing different layers of language models such as ELMo, BERT, and GPT-2. For instance, ELMo uses Layer 1 and Layer 2, while BERT and GPT-2 use layers like 1, 2, 11, and 12. The performance of these embeddings is then evaluated on various benchmarks like SimLex999, MEN, WS353, RW, Google, MSR, SemEval2012(2), BLESS, and AP as shown in the performance table.'}", "all_figures_names": ["5-Figure1-1.png", "7-Figure3-1.png", "6-Figure2-1.png", "8-Figure4-1.png", "8-Table1-1.png"], "referred_figures_names": ["8-Table1-1.png"], "answer": "They use the first principal component of a word's contextualized representation in a given layer as its static embedding."}}