{"382": {"question": "What kind of features are used by the HMM models, and how interpretable are those?", "referred_figures_indices": [0, 1], "response": "{'Answer': 'The HMM models use both discretized and continuous features. The interpretability of these models can vary, with discretized features potentially offering more straightforward interpretability, while continuous features might provide more detailed information.'}", "all_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png", "2-Figure1-1.png", "3-Table1-1.png"], "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "}, "383": {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "referred_figures_indices": [2, 0], "response": "{'Answer': 'HMMs learn sequence-level patterns and state transitions, capturing the probabilistic dependencies between different states explicitly. LSTMs, on the other hand, learn to capture long-term dependencies in sequences through memory cells and gates without explicitly modeling state transitions.'}", "all_figures_names": ["4-Figure3-1.png", "2-Figure1-1.png", "4-Figure2-1.png", "3-Table1-1.png"], "referred_figures_names": ["4-Figure2-1.png", "4-Figure3-1.png"], "answer": "The HMM can identify punctuation or pick up on vowels."}, "385": {"question": "How large is the gap in performance between the HMMs and the LSTMs?", "referred_figures_indices": [2], "response": "{'Answer': 'The exact performance difference varies based on the task and specific configuration, but generally, LSTMs outperform HMMs. For instance, with Linux text data using LSTM dimension of 20, the LL (Log-Likelihood) for LSTM is -1.88, whereas for HMM with 20 states it is -2.55.'}", "all_figures_names": ["4-Figure2-1.png", "2-Figure1-1.png", "3-Table1-1.png", "4-Figure3-1.png"], "referred_figures_names": ["3-Table1-1.png"], "answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."}}