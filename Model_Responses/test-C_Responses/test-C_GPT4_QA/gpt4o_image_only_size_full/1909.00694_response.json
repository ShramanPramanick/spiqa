{"2": {"question": "What are the results?", "referred_figures_indices": [3, 5], "response": "{\n  'Answer': 'The highest accuracy is achieved by the BERT encoder trained on the ACP dataset, with an accuracy of 0.933.'\n}", "all_figures_names": ["2-Figure1-1.png", "4-Table2-1.png", "4-Table1-1.png", "5-Table4-1.png", "5-Table5-1.png", "5-Table3-1.png"], "referred_figures_names": ["5-Table4-1.png", "5-Table3-1.png"], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."}, "3": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [1], "response": "{'Answer': 'Relations propagate polarity in different ways depending on their type. CAUSE relations typically propagate the same polarity from the former event to the latter event. CONTRAST relations encourage the propagation of the reverse polarity between events.'}", "all_figures_names": ["5-Table3-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table4-1.png", "4-Table1-1.png", "4-Table2-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "}, "4": {"question": "How are relations used to propagate polarity?", "referred_figures_indices": [3], "response": "{'Answer': 'Relations such as CAUSE and CONCESSION are used to propagate polarity between events by encouraging them to have either the same polarity or the reverse polarity. For instance, CAUSE relations tend to propagate the same polarity between events, while CONCESSION relations encourage the events to have opposite polarities.'}", "all_figures_names": ["4-Table1-1.png", "4-Table2-1.png", "5-Table4-1.png", "2-Figure1-1.png", "5-Table5-1.png", "5-Table3-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"}, "5": {"question": "How big is the Japanese data?", "referred_figures_indices": [1, 5], "response": "{'Answer': '6,000,000 pairs'}", "all_figures_names": ["5-Table4-1.png", "4-Table2-1.png", "5-Table3-1.png", "5-Table5-1.png", "2-Figure1-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table2-1.png", "4-Table1-1.png"], "answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus"}, "6": {"question": "How big is the Japanese data?", "referred_figures_indices": [3], "response": "{'Answer': 'The Japanese dataset consists of 1,000,000 Automatically Labeled Pairs (AL), 5,000,000 CAUSE Pairs (CA), and 5,000,000 CONCESSION Pairs (CO), for a total of 11,000,000 pairs.'}", "all_figures_names": ["5-Table5-1.png", "5-Table3-1.png", "4-Table1-1.png", "4-Table2-1.png", "5-Table4-1.png", "2-Figure1-1.png"], "referred_figures_names": ["4-Table2-1.png"], "answer": "The ACP corpus has around 700k events split into positive and negative polarity "}, "8": {"question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "referred_figures_indices": [3], "response": "{'Answer': 'The improvements of supervised learning results trained on smaller labeled data enhanced with the proposed approach are significant. For example, using BERT with the ACP (6K) dataset, the accuracy increases from 0.876 to 0.886 when enhanced with AL data. Similarly, using BiGRU with the ACP (6K) dataset, the accuracy improves from 0.830 to 0.879 when enhanced with AL, CA, and CO data.'}", "all_figures_names": ["2-Figure1-1.png", "4-Table2-1.png", "5-Table5-1.png", "5-Table4-1.png", "4-Table1-1.png", "5-Table3-1.png"], "referred_figures_names": ["5-Table4-1.png"], "answer": "3%"}, "9": {"question": "How does their model learn using mostly raw data?", "referred_figures_indices": [3], "response": "{\n    \"Answer\": \"The model learns using mostly raw data through the utilization of Automatically Labeled Pairs (AL), CAUSE Pairs (CA), and CONCESSION Pairs (CO) datasets. The model achieves high accuracy by training on these extensive datasets which include pairs of events with annotated polarities and relationships (cause or concession). The integration of these pairs helps the model propagate polarity and learn relationships between events, enhancing its performance.\"\n}", "all_figures_names": ["4-Table2-1.png", "5-Table5-1.png", "5-Table3-1.png", "2-Figure1-1.png", "4-Table1-1.png", "5-Table4-1.png"], "referred_figures_names": ["2-Figure1-1.png"], "answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"}, "10": {"question": "How big is seed lexicon used for training?", "referred_figures_indices": [5], "response": "{'Answer': '6,000'}", "all_figures_names": ["5-Table5-1.png", "5-Table4-1.png", "2-Figure1-1.png", "4-Table2-1.png", "5-Table3-1.png", "4-Table1-1.png"], "referred_figures_names": ["4-Table1-1.png"], "answer": "30 words"}}