{"48": {"question": "Do they report results only on English datasets?", "referred_figures_indices": [3, 5], "response": "{'Answer': 'No'}", "all_figures_names": ["9-Table3-1.png", "12-Table5-1.png", "16-Table7-1.png", "9-Table2-1.png", "14-Table6-1.png", "10-Table4-1.png", "8-Table1-1.png", "17-Figure4-1.png"], "referred_figures_names": ["9-Table2-1.png", "10-Table4-1.png"], "answer": "Yes"}, "51": {"question": "Do they test their approach on a dataset without incomplete data?", "referred_figures_indices": [1], "response": "{'Answer': 'Yes'}", "all_figures_names": ["14-Table6-1.png", "11-Figure2-1.png", "12-Table5-1.png", "17-Figure5-1.png", "5-Figure1-1.png", "16-Table7-1.png", "9-Table2-1.png", "10-Table4-1.png"], "referred_figures_names": ["11-Figure2-1.png"], "answer": "No"}, "52": {"question": "Do they test their approach on a dataset without incomplete data?", "referred_figures_indices": [7], "response": "{'Answer': 'Yes'}", "all_figures_names": ["17-Figure5-1.png", "15-Figure3-1.png", "9-Table2-1.png", "12-Table5-1.png", "5-Figure1-1.png", "9-Table3-1.png", "17-Figure4-1.png", "8-Table1-1.png"], "referred_figures_names": ["8-Table1-1.png"], "answer": "No"}, "53": {"question": "Should their approach be applied only when dealing with incomplete data?", "referred_figures_indices": [3], "response": "{'Answer': 'No'}", "all_figures_names": ["17-Figure5-1.png", "12-Table5-1.png", "9-Table3-1.png", "14-Table6-1.png", "17-Figure4-1.png", "8-Table1-1.png", "11-Figure2-1.png", "10-Table4-1.png"], "referred_figures_names": ["14-Table6-1.png"], "answer": "No"}, "54": {"question": "Should their approach be applied only when dealing with incomplete data?", "referred_figures_indices": [5], "response": "{'Answer': 'No'}", "all_figures_names": ["14-Table6-1.png", "15-Figure3-1.png", "11-Figure2-1.png", "9-Table3-1.png", "10-Table4-1.png", "5-Figure1-1.png", "17-Figure5-1.png", "12-Table5-1.png"], "referred_figures_names": ["5-Figure1-1.png"], "answer": "No"}, "55": {"question": "By how much do they outperform other models in the sentiment in intent classification tasks?", "referred_figures_indices": [5, 6], "response": "{'Answer': 'The Stacked DeBERT model outperforms most other models in the sentiment in intent classification tasks. It achieves an F1-score of 99.06 for complete data, 97.17 for gtts-witai data, and 96.23 for macsay-witai data. Comparing it to BERT, which has F1-scores of 98.11 (complete), 96.23 (gtts-witai), and 94.34 (macsay-witai), it shows improvements of 0.95%, 0.94%, and 1.89% respectively for these datasets", "all_figures_names": ["10-Table4-1.png", "17-Figure4-1.png", "12-Table5-1.png", "11-Figure2-1.png", "9-Table2-1.png", "14-Table6-1.png", "16-Table7-1.png", "17-Figure5-1.png"], "referred_figures_names": ["14-Table6-1.png", "16-Table7-1.png"], "answer": "In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average"}}